2.0
ELIZA was an early natural language processing system that used pattern matching to recognize phrases. ELIZA is an example of a chatbot. Regular expressions are the most important tool for text pattern characterization, and they can be used to specify string patterns to extract from a text. Text normalization uses regular expressions to convert text into a more standard form. For example, text normalization uses tokenizing, which breaks a sentence into a sequence of words and other tokens like punctuation marks. Lemmatization is also a part of text normalization. Lemmatization is the process of finding root words of inflected words. A simpler version of lemmatization is stemming, which removes suffixes from words. Edit distance is a measure of similarity between two strings.

2.1.1
Regular expressions are case sensitive. Square braces([ ]) specify a disjunction of characters that can be matched in a position in a longer regular expression. For example, the regular expression /[012]/ matches to any digit among 0, 1, and 2. A dash(-) used within square braces indicates a range of characters. When a caret(^) is used within square braces as the first symbol, it indicates that the pattern behind it is negated. Optional elements are indicated by the question mark(?) which means the character preceding the question mark can be included or not. The Kleene star(*) means 0 or more occurrences of the preceding character or regular expression. The Kleene + means 1 or more occurrences of the preceding character or regular expression. The period(.) is a wildcard expression that matches any single character. The wildcard does not match with a carriage return. Paired with the Kleene star, the wildcard can match any string of characters. Anchors are symbols that anchor regular expressions to certain positions. The caret(^) when used outside of square braces matches the start of a line. The dollar sign($) matches the end of a line. Backslash-b is an anchor that matches with a boundary, and backslash-B is an anchor that matches with a non-boundary.

2.1.2
Disjunction uses the pipe symbol (|) and represents the expression “A or B”. Parentheses are used to indicate precedence for matching. For example, if parentheses surround a disjunction regular expression that comes after a string, then we can match to string and A and string and B. Parentheses can also define strings to match with for symbols that apply to single characters by default. The operator precedence hierarchy prefers parentheses, counters, sequences and anchors, and disjunction in that order. Regular expressions always match the largest possible string, so patterns are greedy.

2.1.3
Reducing error increases precision and recall, which are respectively achieved by fixing false positives and false negatives.

2.1.4
You can specify the number of an instance in a pattern like this: (/{4}/). It means there are four instances of the previous character or expression. You can also specify ranges like (/{n,m}/) or (/{n,}/). You can refer to special characters with a backslash (\), for example: (\*), (\.), (\?), (
), (	).

2.1.6
Substitutions is an important use of regular expressions. We use number operators to match a specific expression twice or more in a text. The number operator backslash-number refers to the nth instance of a certain phrase or pattern in the text, where the number is denoted by n. We use parentheses to store a pattern in memory, which is called a capture group. We can use the command(?:) after an opening parenthesis to indicate a non-capturing group. A non-capturing group is not placed in the register, and therefore cannot be indicated by a number operator.

2.1.7
Lookahead assertions searches text ahead for patterns, using the question mark paired with an equal sign or with an exclamation point. The question-mark-equal-sign searches for a zero-width match, and the question-mark-exclamation is a negative lookahead which searches for a non-matching zero-width pattern.

2.2
A corpus is a computer-readable collection of text or dialogue. Speech can be organized into utterances, which are spoken sentences. Utterances may have disfluencies, which can be fragments or fillers. Fragments are broken words, while fillers are pauses such as um. Disfluencies can be both hindrances or useful signals. A lemma is the set of words that share the same major word, like hint, hints, hinted, or more. We can differentiate the number of words by counting tokens or types. Tokens are the total number of words (N). Types are the number of unique words (|V|). The relationship between tokens and types is illustrated by Herdan’s Law or Heap’s Law, which states that the type equals k times token to the power of beta, where k and beta are both positive, and beta is a number between 0 and 1 ($$|V| = k*N^{eta}$$). We can also measure a corpus by counting the number of lemmas. Dictionary entries or boldface forms is the rough upper limit for the number of possible lemmas.

2.3
There are many possible variations in languages, such as dialect, code switching, genre, and more. Corpus creators can build datasheets to organize the properties of a corpus to help address such complexity.

2.4.1
All natural languages must be normalized in order to be processed. Normalization involves tokenizing words, normalizing formats, and segmenting words. A basic method of tokenizing words can be done using Unix. Using commands such as tr, Unix can collapse, sort, and build statistics for the words in a corpus. The most common words in a corpus are function words such as articles, pronouns, and prepositions.

2.4.2
Actual tokenization involves segmenting text into words using more sophisticated algorithms. We must account for punctuation and special characters according to where and how they are used. A tokenizer may solve these complications, expand clitic contractions, recognize named entities, and more. Named entity recognition is the identification of names, dates, organizations, etc. An example of clitic contraction would be ‘I’m’, which can be expanded into I and am. The Penn Treebank tokenization standard is used for parsed corpora released by the LDC. Tokenization must occur before any other natural language processing can take place, so speed is important. Of course, there are ambiguities that algorithms must deal with. Tokenization is more complex for languages with no spaces, such as Chinese. Since word barriers are not marked, and each character is a morpheme, identifying where a word begins and ends can be difficult. For languages like Japanese, not all characters carry individual meaning, so word segmentation is required. Neural sequence models are used for segmentation in such languages with more ambiguities.

2.4.3
We can also automatically identify the type of token needed instead of pre-defining tokens as words or characters. Automatic identification of tokens can solve the unknown word problem which occurs when a training corpus does not contain the words that appear in other corpora. Tokenizers often induce subwords in order to do this. Most tokenization schemes are made out of a token learner and a token segmenter. The token learner induces a set of tokens from raw data. The token segmenter takes raw sentences and segments it based on the tokens produced by the learner. There are three widely-used algorithms that employ this method: byte-pair encoding, unigram language modeling, and WordPiece. The SentencePiece library implements byte-pair encoding and unigram language modeling. Byte-pair encoding, or BPE, begins with a vocabulary that is the set of all individual characters. BPE then identifies pairs of symbols that are most often adjacent, adds the merged pair to the vocabulary, replaces every adjacent pair with the merged pair, and repeats the process until k merges have happened. K is a parameter of the BPE and the number of tokens in the vocabulary is also k.

2.4.4
Word Normalization standardizes words and tokens by selecting single normal forms for words with multiple forms such as US and USA. Case Folding maps all characters to one type of casing. However, case folding is not used in some cases because generalization provided by case folding may be disadvantageous. Lemmatization is the process of determining shared roots among words. Lemmatization involves complete morphological parsing of a word. Morphemes are the smaller units that make up words, and can be divided into two broad classes: stems, and affixes. Stems are the central morpheme and supply the main meaning. Affixes are the add-ons that alter meanings or add more on. Stemming is a simpler version of lemmatization. The Porter algorithm is a widely used stemming algorithm based on a cascade rewriting process governed by a set of rules.

2.4.5
The most successful cues for sentence segmentation are punctuation. However, there are ambiguities with certain characters such as the period- a period can be used at the end of a sentence, but it can also be used as part of abbreviations. Thus, sentence tokenization usually identifies the role of a period before segmentation.

2.5.0
Natural language processing often involves identifying similarities between strings. When deciding whether two strings refer to the same entity, or coreference, identifying how similar two strings are is important. Edit distance is a way to quantify this measure of similarity. Minimum edit distance is the minimum number of editing operations such as insertion, deletion, and substitution required to make two strings equal. Given two strings, their alignment is a correspondence between their substrings. An operation list is positioned between this alignment, indicating the editing operations needed to equalize the two strings. Weight can be assigned to these operations, most simply done by the Levenshtein distance. Levenshtein proposed two weighting methods, one by giving each operation an equal weight of 1, another by allowing only insertion and deletion and giving them equal weights of 1.

2.5.1
To find the minimum edit distance we can use dynamic programming. Dynamic programming refers to a class of algorithms that attempts to solve problems by combining solutions to sub-problems. Examples of dynamic programming include the Viterbi algorithm and the CKY algorithm. The minimum edit distance algorithm was named by Wagner and Fischer, and works by computing the edit distance between a pair of strings by beginning with a pair of characters and working up from them by calculating D[i, j] where i and j are the target substrings of each original string. Assuming Levenshtein distance where insertion and deletion have cost 1 and substitution has cost 2, D[i, j] is the minimum between <<D[i-1, j] + 1>>, <<D[i, j-1] + 1>>, and <<D[i-1, j-1] + 2>> which respectively calculates the cost of deleting a character, inserting a character, and substituting a character. Alignments can be formed by extending the edit distance algorithm. Using an edit distance matrix, we backtrace from the last cell, and each complete path from the last to first cell is a minimum distance alignment. The Viterbi algorithm is a probabilistic extension of the minimum edit distance.

3.0
Models can assign probabilities to word sequences to predict the next word in a sentence. Probabilities can be used for speech recognition, spelling correction, grammatical error correction, machine translation, and augmentative and alternative communication. For example, we may have three different candidates for a translation of a sentence, but one may be more often used and thus more probable than the others. We call models that assign probabilities to word sequences language models (LM). An n-gram can refer to both a sequence of n words and the predictive model that assigns it a probability.

3.1
We define <<P(w|h)>> as the probability of a word w given history h. We can find <<P(w|h)>> based on relative frequency counts, but language is creative and makes it difficult to count the number of entire sentences in large corpora, even if it is the web. Joint probabilities are also difficult to calculate because there are simply too many possible sentences of a certain length. One way we can compute the probability of a sequence is by using the chain rule of probability. The chain rule demonstrates how we could calculate the joint probability of a sequence by multiplying together several conditional probabilities. The n-gram model approximates the history of a sequence using the last few words and uses that approximated history to estimate the probability of a word. The Markov assumption assumes that the probability of a word only depends on the previous word. Markov models therefore are models that try to predict future words without looking far into the past. Maximum likelihood estimation (MLE) is an intuitive way of estimating n-gram probabilities by getting counts from a corpus and normalizing them so they range from 0 to 1. For example, to calculate the bigram probability of x given y, we calculate count of the corpus C(xy) and normalize by the sum of all bigrams that share the first word x such that <<$$P(w_{n}|w_{n-1}) = rac{C(w_{n-1} w_{n})}{C(w_{n-1})}$$>>. Generally, the MLE n-gram parameter estimation would be <<$$P(w_{n}|w_{n-N+1 : n-1}) = rac{C(w_{ n-N+1 : n-1} w_{n})}{C(w_{ n-N+1 : n-1})}$$>>. This relative frequency is the ratio of dividing the observed frequency of a particular sequence by the observed frequency of a prefix. Some bigram probabilities encode syntactic rules. All language model probabilities are represented in log format as log probabilities. Log probabilities prevent storing too-small numbers.

3.2.0
Extrinsic evaluation embeds a model in an application and measures how much the application improves in an end to end evaluation. Intrinsic evaluation measures the quality of a model independent of any application and requires a test set of unseen data. The model that assigns a higher probability to the test set is the model with higher accuracy. If we accidentally trained the model on the test set, it would introduce a bias that makes the probabilities inaccurately higher, and incorrectly determines perplexity. A development set is a test set that has been used so often that we need a fresher test set. When dividing our data, we want the smallest test set that can still measure a statistically significant difference between models.

3.2.1
We use perplexity to measure model accuracy. Perplexity is the inverse probability of the test set normalized by the number of words. We want to minimize perplexity in a model. We can also think of perplexity as the weighted average branching factor of a language. The branching factor is the number of possible words that can follow any word. An intrinsic improvement in perplexity does not necessarily mean an extrinsic improvement in the performance of a model.

3.3.0
Many statistical models including the n-gram is dependent on their training corpus. Thus, probabilities may encode specific facts about a training corpus, and an n-gram model may increase in performance as the value of N is increased. Naturally, the longer the context on which we train an n-gram, the better the coherence. When we build an n-gram, we want it to answer and predict sentences that make sense in context as well. To do this, we may use a training corpus of a similar genre to our goal task. For example, a model for answering scientific questions would be trained with scientific text. It is also important to consider dialect and variety when training a model. However, even after considering dialect and genre we must also consider sparsity. If the training corpus fails to contain certain sequences, it is possible that the model incorrectly estimates that those sequences have a probability of 0. Such 0 probabilities are a problem because we are underestimating the probability for various words, and that this causes the entire probability of the test set containing a 0 probability to equate 0, making it impossible to compute perplexity.

3.3.1
In closed vocabulary systems, test sets contains only known words, such as for speech recognition tasks or machine translation where we have a dictionary or table that limits the variety of possible words in the test set. Otherwise, we may have to deal with unknown words, which are words that we have never seen before, also called out of vocabulary (OOV) words. The percentage of OOV in a test set is called the OOV rate. We model potential OOVs in the test set by adding a pseudo-word <UNK>, and we call this an open vocabulary system. There are two common ways to train probabilities of <UNK>. The first method turns the problem into a closed vocabulary situation by choosing a vocabulary fixed in advance, converting any training set word not in the vocabulary into <UNK>, and estimating probabilities for <UNK> by treating it like other words. The second method does not have a prior vocabulary, so we create it implicitly by replacing words in the training data with <UNK> based on their frequency. The <UNK> model effects metrics such as perplexity, which is why perplexities should be compared between language models with the same vocabularies.

3.4.0
We remove some probability mass for more frequent events and reassign it to unseen events with known words, and this is called smoothing or discounting. We study four 4 main methods of smoothing: Laplace smoothing, add-k smoothing, stupid backoff, and Kneser-Ney smoothing.

3.4.1
Laplace smoothing, or add-one smoothing, works by adding 1 to all bigram counts and then normalizing them. Given unsmoothed maximum estimate of unigram probability of word w_i is the count c_i normalized by word token count N, Laplace smoothing adds one to each count, which means we add the number of words V to N and 1 to c_i, resulting in P(w_i) = $$rac{c_i + 1}{N + V}$$. We can define an adjusted count c* which is easier to compare with MLE counts. Defining c* is done by adding 1 to count c, and multiplying by normalization factor of $$rac{N}{N+V}$$. Smoothing discounts some non-zero counts to assign probability to zero counts, which means we can describe a smoothing algorithm as a relative discount d_c, the ratio of discounted counts to original counts which is $$rac{c*}{c}$$.

3.4.2
Add-k smoothing moves less probability mass from the seen to the unseen than Laplace smoothing. It requires a method for choosing k, the fractional addition count, which can be done by optimizing on a devset. Add-k doesn’t work well for language modeling.

3.4.3
In backoff, we use a less-context n-gram if evidence is not sufficient. In a backoff model we have to discount higher order n-grams to save probability mass for lower order n-grams by using a function $$lpha$$. This backoff with discounting is called Katz backoff. Katz backoff is also often used with a Good-Turing smoothing method. In interpolation, we mix the probability estimates from all the n-gram estimators. Interpolators are weighted by $$\lambda$$ values which are set by a held-out corpus. A held-out corpus is an additional training corpus for setting hyperparameters such as these lambdas. Using the EM algorithm can converge on locally optimal lambda values.

3.5
Kneser-Ney Smoothing is one of the most used and best performing n-gram smoothing methods. Kneser-Ney is based on absolute discounting which subtracts an absolute discount d from each count. Kneser-Ney then bases the probability estimation on the contexts of a word w, which is the number of bigram types it completes. Finally, Interpolated Kneser-Ney smoothing uses an equation involving normalizing constant $$\lambda$$ to distribute probability mass. The best performing Kneser-Ney smoothing is modified Kneser-Ney smoothing, by Chen and Goodman (1998).

3.6
It is possible to build huge language models by using text from enormous collections such as the web. One example is the Web 1 Trillion 5-gram corpus released by Google, which includes large sets of n-grams from 1 to 5 created from 1,024,908,267,229 words of text from public Web pages in English. When building such large language models it is important to consider efficiency, which is done by storing words as 64-bit hash numbers in memory and words themselves stored on disk, by having probabilities quantized by using 4-8 bits, and storing n-grams in reverse tries. Additionally, n-grams can be shrunk by pruning. We can also build approximate language models through methods such as the Bloom filters. Efficient language model toolkits like KenLM use sorted arrays, efficient probability and backoff single value combinations, and merge sorts to build efficient probability tables. Although it is possible to build huge language models using Kneser-Ney smoothing, Brans et al. (2007) show that the simpler stupid backoff can suffice. Stupid backoff does not attempt to make the language model a true probability distribution. It does not discount higher-order probabilities- instead, if a higher-order n-gram has a zero count we backoff to a lower order n-gram weighed by a fixed weight. Stupid backoff does not produce a probability distribution, so we will refer to it as S. Brants et al. (2007) found that a value of 0.4 works well for lambda in a stupid backoff algorithm.

3.7
The concept of perplexity comes from the information-theoretic concept of cross-entropy. Entropy is a measure of information such as that, given a random variable X ranging over $$\chi$$ and a particular probability function p(x), the entropy of random variable X is $$H(x)=-\sum_{x\in\chi}p(x)log_{2}p(x)$$. Using log base 2 to compute entropy, we measure entropy in bits. Entropy can also be considered a lower bound on the number of bits required to encode something in the optimal coding scheme. The entropy rate, or per-word entropy, is the entropy of a sequence divided by the number of words. The Shannon-McMillan-Breiman theorem states that if the language is stationary and ergodic, we can take a long-enough single sequence of words instead of summing over all possible sequences. A stochastic process is stationary if the probabilities it assigns to a sequence are not affected by shifts in the time index. Cross-entropy is an upper bound on entropy and can be found using the Shannon-McMillan-Breiman theorem for a stationary ergodic process. Perplexity of model P on sequence W can now be defined as the exp of cross-entropy H, or $$P(W) = 2^{H(W)}$$.

4.0
Classification is a core part of sentient intelligence. Text categorization is a task that classifies and assigns labels to a text or document. Sentiment analysis, which identifies the positive or negative attitude of a writer towards an object, is a common text categorization task. Spam detection is a classification class that assigns an email to one of two classes spam or not-spam. Language id recognizes which language a task is written in. Authorship attribution is the task of identifying a text’s author. Assigning a library subject category or topic to a text is one of the oldest text classification tasks. Classification is also needed at levels smaller than the document. Period disambiguation, word tokenization, language modeling, and so forth can be considered a form of classification as well. The goal of classification is to take a single observation, extract features, and classify the observation into a specific class. Although many areas of language processing use rule-based classifiers, most cases of classification instead use supervised machine learning. A probabilistic classifier outputs the probability of an object belonging to a certain class. Generative classifiers, like naïve Bayes, build a model of how a class could generate some input. Discriminative classifiers, like logistic regression, learn which features from an observation are most useful to differentiate classes.

4.1
The multinomial naïve Bayes classifier is a probabilistic Bayesian classifier that makes simplifying assumptions about how features interact. The naïve Bayes classifier considers a document as a bag-of-words, and uses Bayesian inference to return a class c-hat to indicate an estimate of the correct class. Naïve Bayes is a generative model because its equation states an implicit assumption about how a document is generated: a class is sampled based on the probability P(c) and words are generated based on sampling from conditional probability P(d|c). We compute most probable class c-hat given document d by choosing the class with the highest product of the prior probability P(c) and the likelihood P(d|c). To directly compute c-hat, naïve Bayes classifiers make two simplifying assumptions: the bag-of-words assumption (position does not matter), and the naïve Bayes assumption (the conditional independence assumption that the conditional probability of each feature given class c are independent, and thus can be naively multiplied). Classifiers like naïve Bayes that use a linear combination of inputs are called linear classifiers.

4.2
Assuming a feature f_i is the existence of a word in the document’s bag-of-words, to find conditional probability of a feature according to class we want to compute the fraction of times word w_i appears among all words in all documents of topic c. Vocabulary V must consist of the union of all word types in all classes involved. Because naïve Bayes naively multiplies all feature likelihoods, we cannot have a zero probability in the likelihood for any class. The simplest solution for preventing zero probability classes is Laplace smoothing (equation 4.14). The naïve Bayes ignores unknown words. Some systems choose completely ignore stop words as well.

4.4
To optimize the naïve Bayes text classification for sentiment analysis, it is more important whether a word occurs or not than its frequency. This variation with clipped word counts in each document is the binary multinomial naïve Bayes (binary NB). Another issue to deal with is negation. A common solution to which prepends prefix NOT_ to every word after a token of logical negation until the next punctuation mark. Additionally, if we lack labeled training data to train accurate naïve Bayes classifiers, we can instead derive positive and negative features from sentiment lexicons. Popular sentiment lexicons include the General Inquirer, LIWC, opinion lexicon of Hu and Liu, and MPQA Subjectivity Lexicon.

4.5
Naïve Bayes can change features to express any property of input text we want. In spam detection, the features can include non-linguistic features such as HTML details. For language ID, the most effective naïve Bayes features are character n-grams rather than words.

4.6
A naïve Bayes model can be viewed as a set of class-specific unigram language models.

4.7.0
Gold labels are human-defined labels for documents. To evaluate text classifications, we use a confusion matrix, a table visualizing how an algorithm performs with respect to human gold labels based on system output and gold labels. Each cell in a confusion matrix contains possible outcomes. Accuracy is not a good measurement for classifiers, so we instead generally turn to precision and recall. Precision is the percentage of items detected by the system that are in fact positive according to gold labels (true positives / true positives + false positives). Recall is the percentage of items actually present in input that were correctly identified (true positives / true positives + false negatives). F-measure is a single metric incorporating precision P and recall R and is defined as $$F_{eta}=rac{(eta^{2}+1)PR}{eta^{2}P+R}$$ where $$eta$$ differentially weights the importance of recall and precision. F-measure is based on a weighted harmonic mean of P and R, and harmonic mean is the reciprocal of the arithmetic mean of reciprocals.

4.7.1
Many classification involves multiple classes. To this end, we must modify definitions of precision and recall. We can combine precision and recall by macroaveraging, where we compute performance for each class and average it, or by microaveraging where we collect decisions for all classes into a single confusion matrix before computing performance.

4.8
Training and testing for text classification is similar to that with language modeling. We use the development test set (devset) to tune parameters and decide on the best model, after which we run it on the test set. Cross-validation randomly chooses a training and test set division of our data, trains the classifier, and computes error rate on the test set, then repeats with different randomly selected training test sets. The 10-fold cross-validation gives an average error rate. The only problem with cross-validation is that because all data is used for testing, the whole corpus must be blind.

4.9.0
Statistical significance testing is used to compare the performance of two systems. Defining M(A,x) as the score system A gets on test set x and M(B,x) the score system B gets on test set x, we define delta(x) the performance difference between M(A,x) and M(A,b). We call performance difference delta(x) the effect size. The larger the effect size the better system A appears to be than system B. To check if system A is actually better than system B we must check its superiority over other test sets as well, which is formalized into two hypotheses: $$ H_{0} : \delta(x) \leq 0 $$ and $$ H_{1} : \delta(x) > 0 $$. Hypithesis H_0 is the null hypothesis that supposes that delta is negative or zero, indicating that A is not better than B. To find if we can rule out H_0 we create random variable X over all test sets and find the likelihood p-value of delta(x) if H_0 is correct. A system difference is statistically significant is the delta we saw has a probability under the threshold and therefore rejects H_0. There are two common non-parametric tests used in NLP: approximate randomization and the bootstrap test. Paired tests are tests where we compare two sets of aligned observations.

4.9.1
The bootstrap test can apply to any metric. It refers to repeatedly drawing large numbers of smaller samples with replacement from an original larger sample. It creates many virtual tests from an observed test set.

4.10
There exists a variety of harms that can result from classifiers. Representational harms are a class of harms caused by a system that demeans a social group. Toxicity detection aim to detect hate speech abuse, harassment, etc, but may act as censors instead. These issues can be caused by biases in training data. Thus it is important to study and clarify such factors, and one method is by releasing a model card for each version of a model that documents information such as algorithms, data sources, etc.


A Summary of Chapters 2-4 of “Speech and Language Processing” by Jurafsky & Martin (12/30/2020 Draft)

Generated by facebook/bart-large-cnn

2.0
The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian  psychotherapist. Eliza's mimicry of human conversation was remarkably successful. Modern  conversational agents are much more than a diversion;  they can answer questions, book flights, or find restaurants, functions for which they rely on a much more sophisticated understanding of the user's intent. English words are often  separated  from each other by whitespace. For processing tweets or texts we'll need to tokenize emoticons like :) or hashtags like #nlproc. Some languages, like Japanese, don't have spaces between words, so word tokenization becomes more difficult.

2.1.0
A regular expression (RE) is an algebraic  notation for characterizing a set of  strings. It's used in every computer language, word processor, and text processing tools like the Unix tools  grep or Emacs. We'll show regular expressions that come in many variants of regular expressions.

2.1.1
To search for woodchuck, we type /woodchuck/. The expression  /Buttercup/  matches any string containing the substring Buttercup. The search  string can consist  of  a single character (like /urgl/) or a sequence of characters. The pattern  /[wW]/ matches patterns containing either w or W. If the caret  ^ is the first symbol after the open square brace, the resulting pattern is negated. We can't use the square brackets,  because while they allow  us to say "s or S",  they don't allow us to  say  s or nothing. For this we use the question mark /?/. which means "the preceding character or nothing", as shown in Fig. 2.5. /a*/  means "any string of  zero or more as" /aa/ means one a followed by zero or  more as. For specifying multiple digits (useful for finding prices) we can extend /[0-9//. for  a single digit. /baaa*!/ or /baa+!/. are two ways to specify  "at least one" of some character. The most common anchors are the caret  ^ and the dollar sign $. The pattern  /^The/  matches the word "The" only at the start of a line. There are also two other anchors:  \b  matches a word boundary, and  \B matches a non-boundary.

2.1.2
The disjunction operator, also called the pipe symbol  |, matches either the string cat or the string dog. Sometimes we need to use this operator in the midst of a larger sequence. For example, suppose  I  want  to search  for information  about pet fish for my cousin David. The expression  /Column [0-9] */ will not match any  number  of columns; instead,  it will match a single column followed  by a number of spaces! The idea that one operator may take precedence over another is formalized by the operator precedence hierarchy for regular expressions. The operator *? is a Kleene star that matches as little text as possible.

2.1.3
A simple (but incorrect) pattern might be: /the/. One problem is that this pattern will miss the word when it begins a sentence and hence is capitalized (i.e., The). This might lead us to the following pattern: /[tT]he/. But we need to specify that we want instances with a word boundary on both sides. We might want this since /\b/ won't treat underscores and numbers as word boundaries. The process we just went through was based on fixing two kinds of errors: false positives and false negatives. Reducing error rate involves increasing precision (minimizing false positives) and increasing recall. We'll come back to precision and recall with more precise definitions in Chapter 4.

2.1.4
The regular expression  /{3/ means "exactly  3 occurrences of  the  previous  character  or expression" A range of  numbers can also be specified by enclosing them in curly brackets. Certain special characters are referred to by special notation based on the backslash (\)

2.1.5
A simple regular expression for a dollar sign followed by a string of digits: /$[0-9+/. The $ character has a different function  here than the end-of-line function we discussed earlier. Let's try out a more significant example of the power of REs. We'll need to allow for optional fractions again (5.5 GB) and disk space (5 GB)'s optional fractions. This pattern  only allows  $199.99  but  not  $/99.99. We need to limit the dollars: /(^|\W)$[0-9]{0,3}(\.[0/9])? *(GB|[Gg]igabytes?)\b/.

2.1.6
An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ is used in Python and in Unix commands like vim or sed. It is often useful to be able to refer to a particular subpart of the string matching the first pattern. ELIZA simulates a Rogerian psychologist by carrying on conversations with regular expressions. Substitutions and capture groups are very  useful in implementing simple chatbots. The first substitutions  then change all instances  of MY to YOUR, and I'M to YOU ARE, and so on. The next set of substitutions matches and replaces other patterns in the input.

2.1.7
Negative  lookahead  is commonly used  when  we are parsing complex patterns but want to rule out a special case. The operator  (?=  pattern) is true if pattern occurs, but  is zero-width, i.e. the match pointer doesn't  advance.

2.2
The Brown corpus is a million-word collection of samples from 500  written  English texts from different genres. Punctuation is critical for finding boundaries of things (commas, periods, colons) and  colons. For some tasks,  like part-of-speech tagging or parsing or speech synthesis,  we sometimes treat punctuation marks as separate words. Words like uh and um are called fillers or filled pauses. Disfluencies are helpful in speech recognition in predicting the upcoming word. For part-of-speech or named-entity tagging, capitalization is a useful feature and is retained. For morphologically complex languages like Arabic, we often need to deal with lemmatization. The larger  the corpora  we look at, the more word  types we find. The relationship is called Herdan's Law or Heaps' Law. The 1989 edition of the Oxford English Dictionary had 615,000 entries. For many tasks in English, however, wordforms are sufficient.

2.3
NLP algorithms are most useful when they apply across many languages. The world has 7097 languages, according to the online  Ethnologue catalog (Simons and Fennig, 2018) It is important to test algorithms on more than one language,  particularly  on languages with different properties. When developing computational models for language processing from a corpus, it's important to consider who produced the language, in what context, for what purpose. Text also reflects the demographic characteristics of the writer (or speaker) Their age, gender, race, socioeconomic  class can all influence the linguistic properties of the text we are processing. Language changes over time, and for some languages we have good corpora of texts from different historical periods.

2.4.0
Before almost any natural language processing  of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: Tokenizing  (segmenting) words, Normalizing word formats, Segmenting sentences. In the next sections we walk through each of these tasks.

2.4.1
We'll make use of some Unix commands:  tr, sort, uniq, sort and uniq. For example let's begin with the 'complete words' of Shakespeare in one textfile, sh.txt. We'll begin with an easy, if  somewhat  naive version of  word tokenization. Unix tools of this sort can be very handy in building quick word count statistics for any corpus. We can use tr to tokenize the words by changing every sequence of non-alphabetic characters to a newline. The results show that the most frequent words in Shakespeare, as in any other corpus, are the short function words like articles, pronouns, prepositions.

2.4.2
For most NLP applications, we'll need to keep punctuation, numbers and punctuation. We often want to break off punctuation as a separate token; punctuation is a useful piece of information for parsers. A tokenizer can also expand  clitic contractions that are marked by apostrophes. One commonly used tokenization  standard is known as the Penn Treebank tokenization. This standard separates out clitics (doesn't  becomes  does  plus  n't), keeps hyphenated words together, and separates out all punctuation. The standard method for tokenization is therefore to use deterministic algorithms based on regular expressions compiled into very efficient finite state automata. Words are about 2.4 characters long on average, but deciding what counts as a word in Chinese is complex. Each character generally represents a single unit of meaning (called a morpheme) and is pronounceable as a single syllable. For most Chinese NLP tasks it works better to take characters rather than words as input. For Japanese and Thai NLP algorithms, algorithms for word segmentation are required.

2.4.3
NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use these facts to make decisions about a separate  test corpus. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the morphemes  -est or -er. This is especially useful in dealing with  unknown  words, an important problem in language processing. Three algorithms are widely used: byte-pair encoding (Sennrich et al., 2016), unigram language modeling (Kudo, 2018), WordPiece (Schuster  and Nakajima, 2012) In this section we introduce the simplest of the three, the BPE algorithm. The BPE token learner begins with a vocabulary that is just the set of all individual characters. It then examines the training corpus, chooses the two symbols that are most frequently  adjacent (say 'A', 'B'), adds a new merged symbol 'AB' to the vocabulary. The BPE algorithm first counts all pairs of  adjacent symbols:  the most frequent is the pair e  r. We then merge  these symbols, treating  er  as one symbol, and treat er as one. The token parser just runs on the test data the merges we have learned from the training data, greedily, in order we learned them.

2.4.4
Word normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. Mapping  everything  to  lower case means  Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many tasks, such as information  retrieval or speech recognition. For sentiment analysis  analysis  and other text classification tasks,  information extraction,  and machine translation,  by contrast,  case folding is generally not done. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. The most sophisticated methods for lemmatization involve complete morphological  parsing of the word. The Porter stemmer algorithm is based on series of rewrite rules run in series, as a cascade.

2.4.5
Sentence segmentation is another important step in text processing. The most useful cues for segmenting  a text into sentences are punctuation, like periods, question marks, and exclamation points. Periods, on the other hand, are more ambiguous. For this reason, sentence tokenization and word tokenization may be addressed jointly.

2.5.0
Much of natural language processing is concerned with measuring how similar two strings are. The minimum edit distance is defined as the minimum number of editing operations needed to transform one string into another. It's much easier to see this by looking at the most important visualization for string distances, an alignment between the two strings. Given two sequences, an alignment is a correspondence between substrings of  the two sequences. Beneath the aligned  strings is another representation; a series of symbols expressing an operation  list for converting  the top string into the bottom string: d for deletion, s for substitution, i for insertion. We can assign a particular  cost or weight to each of these operations.

2.5.1
Dynamic programming is the name for a class of algorithms, first introduced by Bellman (1957), that apply a table-driven method to solve problems by combining solutions to sub-problems. Some of the most commonly used algorithms in natural language processing make use of dynamic programming. The minimum edit distance algorithm was named by Wagner and Fischer (1974) but independently discovered by many people. The algorithm is summarized in Fig.2.17; Fig. 2.18 shows the results of applying the algorithm to the distance  between  intention  and execution with the version of Levenshtein in Eq. Aligning two strings is useful throughout speech and language processing. In speech recognition,  minimum  edit distance  alignment is used to compute the word error rate. Alignment  plays a role in machine translation, in which sentences in a parallel corpus need to be matched to each other. Exercise  2.7 asks  you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment. The backpointer  from a cell points to the previous cell  (or cells) that we came from in entering the current cell. Some cells have multiple backpointers because  the minimum extension could have come from multiple previous cells.

3.0
Predictions are essential in any task in which we have to identify words in noisy, ambiguous input, like speech recognition. Assigning probabilities to sequences of words is also essential in machine translation. For writing tools like spelling correction or grammatical error correction,  we need to find and correct errors in writing like "Their are two midterms" A probabilistic model of word sequences could suggest that briefed reporters on is a more probable English phrase than briefed to reporters (which has an awkward "to" after "briefed") or  introduced reporters  to (which  uses  a verb that  is  less fluent English in this context) Probabilities are also important  for augmentative  and alternative  communication systems.

3.1
P(w|h), the  probability  of a word  w given some history h, is the probability that the next word is the word w. With a large enough  corpus, such as the web, we can compute these counts and estimate  the probability  from Eq. 2.2. While this method of estimating  probabilities from counts works fine in many cases, it turns out that even the web isn't big enough to give us good estimates in most cases. We'll represent a sequence of N words either as w1...wn or w1:n. We'll use P(W1, w2,...,wn) for the joint probability of each word in a sequence having a particular value P(X=w1, Y=w2, Z=w3..., W=wn) We don't know any way to estimate the exact probability of a word given a long sequence of preceding words. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. We use a bigram model to predict the conditional probability of the next word. We can generalize the bigram to the trigram and thus to the n-gram. Eq.11 estimates  the n-gram  probability by dividing  the observed frequency  of a particular sequence by the frequency of a prefix. For example, suppose  the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus. In MLE,  the resulting  parameter set maximizes  the likelihood of  the training  set T given the model. We'll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California. Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the Berkeley  Restaurant  Project. Note that the majority  of  the values are zero. In fact, we have chosen the sample words to cohere with each other; a random set of seven words would be even more sparse. Bigram probabilities encode some facts that we think of as strictly syntactic in nature. In practice it's more common to use trigram models, which condition on the previous word. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., P(I|)

3.2.0
The best way to evaluate the performance of a language model is to embed it in an application and measure how much it improves. Such end-to-end evaluation is called extrinsic evaluation. The probabilities of an n-gram model come from the corpus it is trained on, the training set or test corpus. We can then measure the quality of an  model by its performance on some unseen data. Given a large corpus that we want to divide into training and test, test data can either be taken from some continuous sequence of text inside the corpus, or we can remove smaller "stripes" of text from randomly selected parts of our corpus and combine them into a test set. We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative.

3.2.1
The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. We can use the chain rule to expand the probability of W: (3.15) The higher the conditional probability of  the word sequence, the lower the perplexity. The branching factor of a language is the number of possible next words that can follow any word. Consider the task of recognizing the digits in English (zero, one, two..., nine) given that (both in some training set and in some test set) each of the 10 digits occurs with equal probability P 0.1. The perplexity of this mini-language is in fact 10. We see in Section 3.7 that perplexity is closely  related  to the information-theoretic notion of entropy. The more information the n-gram  gives us about the word sequence, the lower the perplexity. The perplexity of two language models is only comparable  if they use identical vocabularies.

3.3.0
The n-gram model, like many statistical models, is dependent on the training corpus. The longer the context on which we train the model, the more coherent sentences, we can generate sentences. We can use the same technique to generate sentences from random bigrams, trigrams, and 4-grams. In the unigram sentences, there is no coherent relation between words or any sentence-final punctuation. The bigram  sentences have some local word-to-word coherence. The tri-gram and 4-gram sentences are beginning to look a lot like Shakespeare. The words "It cannot be but so" are directly from "King John" Statistical models are likely to be useless as predictors if the training sets and the test sets are as different as Shakespeare and WSJ. One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. It is equally important  to get training data in the appropriate dialect or variety, especially when processing social media posts or spoken transcripts. For any n-gram that occurred a sufficient number of times, we might have a good estimate of its probability. But because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it. Matching genres  and  dialects is still not sufficient. Our models may still be subject to the problem of sparsity.

3.3.1
The previous section discussed the problem of words whose bigram  probability is zero. But what about words we simply have never seen before? Sometimes we have a language task in which we know all the words  that can occur. In other cases we have to deal with words we haven't seen before, or out of vocabulary (OOV) words. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. The exact choice of <UNK> model does have an effect on metrics like perplexity. For this reason, perplexities should only be compared across language models with the same vocabularies (Buck et al., 2014)

3.4.0
What do we do with words that are in our vocabulary (they are not unknown words) but appear in a test set in an unseen context? To keep a language model from assigning zero probability  to these unseen  events,  we'll have to shave off  a bit of  probability mass from some  more frequent events. This modification is called smoothing or discounting.

3.4.1
Laplace smoothing does not perform  well enough to be used in modern n-gram models, but it usefully introduces  many  concepts that  we see in other smoothing  algorithms. This  algorithm  is called add-one smoothing. It merely adds one to each count (hence its alternate name add one smoothing) A related way to view smoothing  is as discounting  (lowering)  some  non-zero counts in order to get the probability  mass that will be assigned to the zero counts. We can now turn c*  into a probability  P* by normalizing by N. We need to augment the unigram count by the number of total word types. Add-one smoothing has made a very big change to the counts. C(want to) changed from 609 to 238! We can see this in probability  space as well:  P(to|want) decreases from.66 in the unsmoothed case to.26 in the smoothed case.

3.4.2
Instead of adding  I  to each count, we add a fractional count k (.5?.05?.01?). This algorithm is therefore called add-k smoothing. Add-k  smoothing is useful for some tasks (including text classification) But it still doesn't  work  well  for language modeling.

3.4.3
Discounting can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute P but we have no examples of a particular trigram we can instead estimate  its  probability by using the bigram probability. Similarly, if we don't have counts to compute the bigrams probability, we can look to the unigrams. We estimate the trigram probability by mixing together  the unigram, bigram, and trigram probabilities, each weighted by a lambdas. In a slightly more sophisticated  version of linear interpolation, each weight is computed by conditioning on the context. Equation  3.28 shows the equation for interpolation  with context-conditioned weights. The EM algorithm converges on locally optimal lambdas (Jelinek and Mercer, 1980) In a backoff  n-gram model, we approximate it by backing off to the (N-1)-gram. In Katz backoff, we rely on a discounted  probability  P* if we've seen this n-ram before (i.e., if we have non-zero counts)

3.5
Kneser-Ney has roots in a method called absolute discounting. It is necessary to save some probability mass for the smoothing algorithm to distribute to unseen n-grams. To see this, we can use a clever idea from  Church  and  Gale (1991) to see this. Absolute discounting formalizes this intuition by subtracting a fixed (absolute) discount d from each count. The intuition is that since we have good estimates already for the very high counts, a small discount d won't affect them much. Kneser-Ney discounting augments  absolute discounting with a more sophisticated way to handle the lower-order unigram distribution. The Kneser-Ney intuition is to base our estimate of P on the number of different contexts  word  w has appeared  in. We hypothesize  that words  that have appeared in more contexts in the past are more likely to appear in some new context. A frequent word (Kong) occurring in only one context (Hong) will  have a low continuation probability. The final equation for Interpolated Kneser-Ney smoothing for bigrams is then: (3.35) The  lambda is a normalizing  constant that is used to distribute the  probability mass we've discounted. The best performing version of smoothing is called modified Kneer Ney. Modified N-gram smoothing uses three different discounts for n-grams with counts of 1, 2 and three or more.

3.6
The Web  1 Trillion  5-gram  corpus  released  by Google includes various  large sets of  n-grams. Google has also released  Google  Books  Ngrams corpora with n-rams drawn from their book collections. The corpus includes 1,024,908,267,229 words  of text from  publicly accessible Web pages in English. Language model toolkits like KenLM use sorted arrays, efficiently  combine probabilities  and backoffs  in a single value. With very large language models a much simpler algorithm is called stupid  backoff. This algorithm does not produce a probability distribution. The backoff terminates in the unigram, which has probability S(w).

3.7
Entropy is a measure of information; entropy is a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme. A better n-gram  model  is one that assigns a higher  probability to the test data, and perplexity is a normalized version of the probability of the  test data. The entropy  of  the random  variable X ranges over horses gives us a lower bound on the number of bits. A code that averages 2 bits per race can be built with short encodings for more probable  horses, and  longer encodesings  for less  probable horses. The entropy of the choice of horses is then 3 bits. To measure the true entropy of a language, we need to consider sequences of infinite length. The Shannon-McMillan-Breiman theorem states that if the language is regular in certain ways (to be exact, if it is both stationary  and ergodic), (3.47) A stochastic process is said to be stationary if the probabilities  it assigns  to a sequence are invariant with respect  to shifts in the time index. But natural language is not stationary, since as we show in Chapter 12, the probability  of upcoming words can be dependent  on events that were arbitrarily  distant and time dependent. The cross-entropy is useful when we don't know the actual probability distribution that generated some data. It allows us to use some m, which is a model  of p (i.e., an approximation  to p) The more accurate m is, the closer the cross-enterropy  H(p, m) will be to the true entropy.

4.0
Classification lies at the heart of  both human  and machine intelligence. The Bayes algorithm is applied to text categorization, the task of assigning a label or category to an entire text or document. In this chapter, we focus on one common task, sentiment analysis, the extraction of sentiment. The task  of  language id is the first step in most language processing pipelines. Spam detection is another  important  commercial  application, the binary  classification task. Related text classification tasks like authorship attribution are also relevant to the digital humanities, social sciences, and forensic linguistics. A part-of-speech tagger classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. The goal  of  classification is to  take  a  single  observation,  extract  some useful features,  and  classify  the observation into one of a set of discrete classes. Classification  is essential  for  tasks  below the  level of the document as well. The task of supervised  classification is to take an input x and a fixed set of  output classes Y=y1,y2...,yM. For text classification, we'll sometimes talk about c (for "class") instead  of y as our output variable. In the supervised situation we have a training  set of N documents that have each been hand-labeled with a class: (d1, c2),..., (dN, cN)

4.1
In this section we introduce the multinomial  naive Bayes classifier. Naive Bayes is a Bayesian  classifier that makes a simplifying assumption about how the features interact. The intuition of the classifier is shown in Fig. 4.1. We represent a text document as if it were a bag-of-words. Naive Bayes a generative model. We can read it as stating a kind of implicit assumption about how a document is generated. We'll say more about this intuition of generative models in Chapter 5. We could imagine generating artificial documents, or at least their word counts, by following this process. Naive Bayes calculations are done in log space to avoid underflow and increase speed. To apply  the naive Bayes classifier  to text, we need to consider  word positions,  by simply walking an index through every word position in the document. Eq.10 computes the predicted class as a linear function of input features.

4.2
For the class prior P(c) we ask what percentage of  documents  in our training set are in each class c. To learn the probability P(fi|c, we'll assume a feature is just the existence of a word in the document's bag of words. We compute P(w, c) as the fraction of times the word wi appears among all words in all documents of topic c. Bayes naively multiplies all the feature  likelihoods together, resulting in zero. The simplest solution is the add-one smoothing (Laplace) smoothing introduced in Chapter 3. Laplace smoothing is usually replaced by more sophisticated smoothing algorithms. It is commonly used in naive Bayes text  categorization.

4.4
While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. For sentiment classification and a number of other text classification tasks, whether  a word  occurs  or not seems  to matter  more  than its frequency. A second important addition commonly  made when doing text classification for sentiment is to deal with negation. Negation can modify a negative word to produce a positive review (don't dismiss this film, doesn't let us get bored) Newly formed 'words' like NOT_like, NOT_recommend  will thus occur more often in negative documents and act as cues for negative sentiment. In some situations we might have insufficient labeled training data to train accurate Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive and  negative word features  from sentiment lexicons. Four popular lexicons are the General Inquirer, LIWC, the opinion lexicon of Hu and Liu and the MPQA Subjectivity  Lexicon.

4.5
SpamAssassin predefines features like the phrase "one hundred percent guaranteed", or the feature "mentions millions of dollars", which is a regular expression that matches suspiciously large sums of money. In fact features in naive Bayes can express any property of the input text we want. The most effective Bayes features are not words at all, but character n-grams, or byte n-rams, where we pretend everything is a string of raw bytes. Language ID systems are trained on multilingual text, such as Wikipedia text in 68 different languages. But it also includes features like " HTML has a low ratio of text to image area"

4.6
A naive Bayes Bayes model can be viewed as  a set of  class-specific unigram language models. The model for each class instantiates a  language model. Since the likelihood features assign a probability to each word P(word|c), the model also assigns a probability  to each sentence.

4.7.0
A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimensions (system output and gold labels), and each cell labeling a set of  possible outcomes. In spam detection,  our goal is to label every text as being  in the  spam  category ("positive") or not in the spam category ("negative") Or imagine you're the CEO of the Delicious Pie Company and you need to know what people are saying about your pies on social media. A simple classifier would have 999,900 true negatives and only 100 false negatives for accuracy of 99.99%. But accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely  balanced in frequency. Instead of accuracy, we generally turn to two other metrics: precision and recall. Recall  measures the percentage of items actually present in the input that were correctly identified by the system. Precision  and recall  emphasize true positives:  finding the things that we are supposed to be looking for. The "nothing is pie" classifier has a 99.99% accuracy, but has a terrible recall of 0.

4.7.1
Bayes algorithm is already a multi-class classification algorithm. For sentiment analysis we generally have 3 classes (positive,  negative,  neutral) and even more  classes are common for  tasks like part-of-speech tagging,  word sense disambiguation, semantic  role labeling, emotion detection, and so on. In microaveraging, we collect decisions for all classes into a single confusion matrix and then compute precision and recall from that table.

4.8
The training and testing procedure for text classification follows  what we saw with language modeling. We use the training set to train the model, then use the development test set to report its performance. Having a fixed training  set,  devset,  and test  set creates another problem:  in order  to save lots of  data for training,  the test set might not be large enough to be representative.

4.9.0
In building systems we often need to compare the performance of two systems. How can we know if the new system we just built is better than our old one? Or better than the some other system described in the literature? This is the domain  of statistical hypothesis testing. In this section we introduce tests for statistical significance for NLP classifiers. In statistical hypothesis testing, we test this by formalizing two hypotheses. The hypothesis  H0 supposes that d(x) is actually negative or zero, meaning  A is not better than B. We want to know if A's superiority over B is likely to hold again if we checked another test set x', or under some other set of circumstances. We formalize this likelihood as the p-value: the probability, assuming  the null  hypothesis H0 is true, of seeing the d(X) that we saw. In NLP we generally don't  use simple parametric  tests like t-tests or ANOVAs. We say that a result (e.g., "A is better than B") is statistically significant if the d  we saw has a probability that is below the threshold. We usually use non-parametric tests based on sampling:  we artificially create many versions of the experimental setup.

4.9.1
The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision,  recall,  or F1. The word bootstrapping  refers to drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample. Consider a tiny text classification example with a test set x of 10 documents. To create each virtual test set x(i), we repeatedly (n=10 times) select a cell from row x with replacement. Now  we create a large  number b (perhaps  10^5) of  virtual  test sets x, each of size n=10. Now that  we have the b test sets,  providing  a sampling distribution, we can do statistics on how often  A has an accidental advantage. The full algorithm for the bootstrap is shown in Fig.9. It is given a test set x, a number of samples b, and counts the percentage of the b bootstrap test sets in which d(x*(i) ) > 2d(x) This percentage then acts as a one-sided empirical  p-value.

4.10
One class of harms is representational harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. In other tasks  classifiers may lead  to  both representational  harms  and  other harms, such  censorship. Some widely used toxicity classifiers incorrectly flag as being toxic sentences that are non-toxic sentences. Such  false positive errors, if employed by toxicity detection systems without human oversight, could lead to the censoring of discourse by or about these groups. These model problems can be caused by biases or other problems in the training data.


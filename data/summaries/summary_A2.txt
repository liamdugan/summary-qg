A Summary of Chapters 2-4 of “Speech and Language Processing” by Jurafsky & Martin (12/30/2020 Draft)

Written by Annotator #2

2.0
ELIZA is an early natural language processing system that mimiced a Rogerian psychotherapist to carry on conversation. Although the responses were limited, ELIZA was remarkably successful. ELIZA and other chatbots still play a crucial role in natural language processing today. Our process begins with extracting specific strings using regular expression, before converting the text into a more convenient, standard form using text normalization. Text normalization also includes lemmatization, which identifies the implicit shared roots between words. This could be done by stripping the suffixes. Finally, sentence segmentation breaks up a text into individual sentences using punctuation. We use edit distance to evaluate the degree of resemblance between two strings.

2.1.1
The regular expression is an algebraic notation for characterizing a set of strings that is useful for searching for a pattern or through a corpus. Only the first matching result will be shown. The search string is case sensitive, but it can consist of a single character or a sequence of characters. While square braces specify a disjunction of characters, the dash can more easily look for characters over a range. The caret specifies the non-existence of a character. Additionally, the (?) is used to for flexibility on a single character, meaning "the preceding character or nothing". More importantly, the Kleene star means zero or more occurrences of the immediately previous character or regular expression in square braces. A special case is the Kleene plus, which is equivalent to the Kleene star with one more more occurrences. A wildcard is an expression that matches any single character, in the form of a period.  Anchors are special characters that fix regular expressions to particular places in a string, including the caret(^) for start of the line and the dollar sign($) for the end of the line. While  matches a word boundary, \B matches a non-word boundary.

2.1.2
The pipe symbol signifies disjunction, matching words on either side of the symbol. Additionally, the parenthesis operator helps to match a whole sequence and is often used in combination with counters. Overall, the regular expression follows the operator precedence hierarchy, in the order of parenthesis, counters, sequences and anchors, and disjunction, from highest to lowest precedence. While the regular expression is greedy for having a tendency to match the largest possible string, non-greedy matching can be enforced by the question mark qualifier.

2.1.3
We can avoid false positives by increasing precision. We can avoid false negatives by increasing recall.

2.1.4
We can use explicit numbers as counters by enclosing them in curly brackets. Modified curly brackets expression can be used to indicate occurrences within a range. Certain characters, such as the newline character and tab character, are referred to by special notation based on the backslash. Characters that are special themselves are preceded with a backslash.

2.1.6
The substitution operator (s/regexpl/pattern/) is used in Python and in Unix commands allows a string characterized by a regular expression to be replaced by another string. The number operator  refers back to the first pattern described by the parenthesis operator, allowing for modifications without repetitions. The capture group is the use of parentheses to store a pattern in a numbered register. The second capture group is matched with . A non-capturing group is used to group but not capture the resulting pattern in a register. Substitutions and capture groups are crucial in implementing simple chatbots like ELIZA.

2.1.7
The lookahead assertion looks for future matchings without advancing the match cursor. Regardless of the matching-result, the operator is zero-width, meaning that the match pointer doesn’t advance. Negative lookahead is used to rule out special cases while parsing complex pattern.

2.2
A corpus is a computer-readable collection of text or speech, sometimes including punctuations for finding boundaries and identifying aspects of meaning. An utterance is the spoken correlate of a sentence, which never contains punctuations. Examples of disfluencies include fragments and fillers or filled pauses. A lemma is a set of lexical forms having the same stem, major part-of-speech, and word sense. The word-form is the derived form of the word. Types |V| are the numbers of distinct words in a corpus; tokens are the total number N of running words. The relationship between the number of types |V| and number of tokens N is called Herdan's Law or Heaps' Law. The dictionary entires or boldface forms are a rough upper bound on the number of lemmas.

2.3
While there are 7098 languages in the world, most NLP algorithms have been developed or tested just for the official languages of large industrialized nations. Code switching is the phenomenon where speakers and writers use multiple languages in a single communicative act. Other dimensions of variation are genre, demographic characteristics of the writer or speaker, and time. A datasheet or data statement specifies properties of a dataset used in the development of computational models. The datasheet outlines motivation, situation, language variety, speaker demographics, collection process, annotation process, and distribution.

2.4.1
Tokenizing words, normalizing word formats, and segmenting sentences are the minimal tasks applied as part of any normalization process. The tr command in Unix changes particular characters in the input, the sort command sorts input lines in alphabetical order, and the uniq command collapse and counts adjacent identical lines. Function words are articles, pronouns, prepositions.

2.4.2
Tokenization is the task of segmenting running text into words. Punctuations and numbers are intentionally included in most NLP applications because they represent word boundaries. A tokenizer can also expand clitic contractions in alphabetic languages and recognize named entities. The Penn Treebank tokenization standard separates out clitics, keeps hyphenated words together, and separates out all punctuations. Under the Penn Treebank tokenization standard, the clitic doesn't becomes does plus n't. Given the speed requirement for tokenization, the standard method is to use deterministic algorithms based on regular expressions compiles into efficient finite state automata, while dealing with ambiguities. Word tokenization is more complex in languages that do not use spaces to mark potential word-boundaries. For example, in Chinese (hanzi), each character represents a morpheme, or a single unit of meaning. Therefore, most Chinese NLP tasks take characters as input. In comparison, Japanese and Thai characters are too small to be a unit, so word segmentation algorithms that use sequence models are required.

2.4.3
NLP algorithms learn facts about an unknown language from a training corpus, before using these facts to make decisions about a separate test corpus and its language. Subwords are tokens smaller than words, either as arbitrary substrings or meaning-bearing unit of language.  Most tokenization schemes have a token learner and a token segmenter. The two parts that most tokenization schemes have are the token learner and the token segmenter. The token learner induces a vocabulary from a raw training corpus, and the token segmenter segments a raw test sentence into the tokens in the vocabulary. Three widely used algorithms are byte-pair encoding, unigram language modeling, and WordPiece. a SentencePiece library implements both byte-pair encoding and unigram language modeling. The byte-pair encoding is also known as the BPE algorithm. The BPE algorithm runs inside words that are white-space-separated. The BPE token learner begins with all individual characters and combines most frequently adjacent symbols into longer strings until k mergers have been done, where k is a parameter of the algorithm. The BPE token parser then evaluate the test sentence using the given vocabulary. In real BPE algorithms, besides the unknown words, only the very rare words will be represented by their parts.

2.4.4
Word normalization is the task of putting words and tokens in a standard format. Case folding is a type of normalization that maps everything to its lowercase form, which is useful for information retrieval or speech recognition. In contrast, case folding is not done for sentiment analysis, information extraction, and machine translation, where case matters. Lemmatization is the task of determining that two words have the same root, despite their surface differences.Lemmatization is commonly done through the complete morphological parsing of the word.Morphology is the study of the way words are built up from smaller meaning-based units called morphemes. All morphemes can be grouped into stems that supply the main meaning and affixes that add additional meanings.

2.4.5
The most useful cues for sentence segmentation are punctuations. However, punctuations like the period character can be ambiguous between a sentence boundary marker and other meanings, so the sentence tokenization always first determines the purpose of a punctuation.

2.5.0
Coreference is the task of deciding whether two strings refer to the same entity. Edit distance quantifies string similarity, such that the minimum edit distance between two strings is the minimum number of editing operations needed to transform one string into another. An alignment is a correspondence between substrings of the two sequences. Beneath the aligned strings, the operation list contains a series of symbols expressing the editing operations needed, following d for deletion, s for substitution, and i for insertion. The Levenshtein distance between two sequences is the weighting factor in which each of the three editing operations has a cost of 1.

2.5.1
Dynamic programming is a class of algorithms that solve problems by combining solutions to sub-problems.  Some commonly used algorithms in natural language processing, including the Viterbi algorithm and the CKY algorithm use dynamic programming. The minimum edit distance algorithm was named by Wagner and Fischer in 1974 but independent discovered by many people. The initialization step of the algorithm calculates the distance for the zeroth row and column. The recurrence relation step then takes the minimum of three possible paths to determine the minimum edit distance for the rest of the matrix. One way to visualize this algorithm is to show the path with boldfaced cells. Each boldfaced cell represents an alignment of a pair of letters in the two strings. Two boldfaced cells in the same row present an insertion, while two boldfaced cell in the same column represent a deletion from the source to the target. The backtrace is a procedure in which we follow the pointers from the last cell, such that each complete path between the final cell and the initial cell is a minimum distance alignment.

3.0
Probability is used in speech recognition, spelling correction, and machine translation. Probability is also important for augmentative and alternative communication systems or AAC devices for the disabled. Models that assign probability to sequences of a word are called language models or LMs. The simplest example of LMs is the n-gram.

3.1
Estimating probabilities directly from counts doesn't give good estimates when new sentences are created. The chain rule of probability computes probability of a word given previous words. The n-gram model approximates the history by just the last few words. For example, the bigram model predicts the conditional probability of the last word given the first word. The Markov assumption states that the probability of a word depends only on the previous word, without looking too far into the past. The maximum likelihood estimation or MLE gives the parameters of an n-gram model by normalizing the counts from a corpus so that they lie between 0 and 1. The particular bigram probability of a word y given a previous word x is given by the bigram of xy over the unigram count for x, or the number of times x appears before y over the number of times x appears. The relative frequency is the ratio between the observed frequency of a particular sequence and the observed frequency of a prefix. The bigram probabilities are normalized by dividing each cell by the set of unigram probabilities. In practice, it is more common to use the trigram models, or the 4-gram and 5-gram models when there is sufficient training data. To avoid numerical underflow for large n-grams, language model probabilities are always represented and computed as log probabilities.

3.2.0
An extrinsic evaluation determines the performance of a language model by embedding it in an application and measuring how much the application improves. An intrinsic evaluation metric measures the quality of a model independent of any application. The quality of an n-gram model from its training set is measured by its performance on a test set in the held out corpora. Between two n-gram models, the one that better fits the test set assigns a higher probability to the test set and predicts its outcomes more accurately. The development test (devset) is the initial test set that has never been used before. Training on the same test set introduces a bias that gives disproportionally high probabilities and causes huge inaccuracies in perplexity.

3.2.1
The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. The smaller the perplexity, the greater the language model's test set probability is. Begin-sentence and end-sentence markers are included in the total count of word tokens N. Another definition of perplexity is the weighted average branching factor of a language, or the number of possible next words that can follow any word. In computing perplexities, the n-gram model P must be constructed without any knowledge of the test set or its vocabulary. Although an intrinsic improvement in perplexity doesn't guarantee an extrinsic improvement in the performance of a specific language processing task, it can still be used as a quick check on the algorithm.

3.3.0
The n-gram model is dependent on the training corpus because probabilities often encode specific facts about a given training corpus. The n-gram model also models the training corpus better as the word tokens N increases. It is important to use a training corpus that has a similar genre to the task and to get training data in the appropriate dialect or variety. Zeros are phrases that don't occur in the training set but do occur in the test set. Zeros cause the underestimation of all other phrases and give a false negative probability of the entire test set.

3.3.1
In a closed vocabulary system, the test set doesn't contain any words outside of the given lexicon. The closed vocabulary system is often used in speech recognition or machine translation, where a pronunciation dictionary or a phrase table is fixed in advanced. Unknown words are also called out of vocabulary (OOV) words. The OOV rate is the percentage of OOV words that appear in the test set. In an open vocabulary system, potential unknown words in the test set are modeled by adding a pseudo-word called (UNK). We can train the probabilities of the unknown words is to convert any unknown words to the word token (UNK) and add the word token (UNK) to the closed vocabulary. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability, so perplexities should only be compared across language models with the same vocabularies.

3.4.0
Smoothing or discounting is the procedure of transferring the probability mass of frequent events to other words that appear in the test set in an unseen context.

3.4.1
The Laplace smoothing algorithm adds one to all the bigram counts. Although the Laplace smoothing algorithm is not sophisticated enough to be used in modern n-gram models, it is used for tasks like text classification. Instead of adding one to both the numerator and denominator of the probability, it is convenient to define an adjusted count. The adjusted count is equal to count plus one multiplied by the normalization factor N over N plus V, where N is the total number of word tokens and V is the number of words in the vocabulary. The smoothing algorithm can also be viewed as discounting non-zero counts and reassigning probability mass to zero counts. The smoothing algorithm can also be described in terms of a relative discount d, instead of the discounted counts c*. The reconstructed count matrix shows both the discounted previously-nonzero counts and increased previously-zero counts.

3.4.2
The algorithm that adds a fractional count k to each count is called add-k smoothing. The fractional count k can be chosen by optimizing on a devset.

3.4.3
Using less context, such as using the bigram when we have no examples of a particular trigram, helps to generalize more for unlearned contexts. In backoff, we move along the n-gram hierarchy until there is sufficient evidence. In interpolation, we mix the probability estimates from all n-gram estimators. The different order n-grams are each weighted by a lambda value, and the lambda values sum to one. The lambda values or coefficients in both simple and conditional interpolation are learned from a held-out corpus. One way to find the optimal set of lambdas is by using the EM algorithm, an iterative learning algorithm that converges on locally optimal lambdas. For a backoff n-gram model to give the correct possibility distribution, the higher-order n-grams are discounted by a function alpha to save some probability mass for the lower-order n-grams. The backoff n-gram model with discounting is called Katz backoff, often combined with a smoothing method as the combined Good-Turning backoff algorithm.

3.5
The most commonly used and best performing n-gram smoothing method is the interpolated Knerser-Ney algorithm. The interpolated Knerser-Ney algorithm originated from absolute discounting, which subtracts a fixed discount d from each count. The Knerser-Ney algorithm modifies absolute discounting by creating a unigram model called PContunuation, which calculates the probability of a word to appear as a novel continuation. The best performing version of Knerser-Ney smoothing is called modified Knerser-Ney smoothing.

3.6
By using text from web or other enormous collections, it is possible to build extremely large language models. An example of a large corpus is the COCA, the Corpus of Contemporary American English, which contains one billion words with equal number of words from different genres. Due to efficiency considerations, large sets of n grams usually store each word as a sixtyfour bit hash number rather than a string, and probabilities are quantized using only four eight bits. N grams can also be shrunk by pruning, including setting a threshold for n-gram storage and using entropy to prune less-important n-grams. Bloom filters used to build appropriate language models. Efficient language model toolkits like KenLM use sorted arrays, combine probabilities and backoffs in a single value, and use merge sorts to build the probability tables. The stupid backoff algorithm is used when the full Knerser-ney smoothing is unnecessary. The stupid backoff simply gets rid of the zero-count higher-order n-grams, instead of discounting them.

3.7
Entropy is the measure of information, and entropy is calculated using the log function in any base. Using log base 2, the resulting entropy is measured as the lower bound on the number of bits it would take to encode a certain piece of information in the optimal coding scheme. The entropy rate or per-word entropy is the entropy of a sequence divided by the number of words. The Shannon-McMillan-Breiman theorem states that if the language is regular, the entropy of the given language would be the entropy of long-enough sequences from that language. The stochastic process is stationary if the probabilities it assigns to a sequence are independent of the shifts in the time index. Markov models and n-grams are stationary, but natural language is not stationary. We can compute the entropy of natural language by taking a sufficiently long sample of the output to determine its average log probability, but the assumptions that we make might be incorrect. Cross-entropy is useful when the actual probability distribution p used to generate some data is unknown. We would take advantage of some m model of the probability distribution p to calculate the cross-entropy of m on p, summing the log of probabilities according to m. The cross-entropy is essentially an upper bound on the entropy of p. The difference between the cross-entropy of m on p and the entropy of p measures how accurate the model m is. The more accurate model would give a lower cross-entropy. The perplexity of a model P on a sequence of words W can be formally defined as some exponential of the cross-entropy.

4.0
Text categorization is the task of assigning a label or category to an entire text or document. Sentiment analysis is the extraction of sentiment, the positive or negative orientation that a writer expresses towards some object. Spam detection is the binary classification task of assigning an email to either spam or not-spam. Language id is the task of determining the language of a text, before choosing the correct processing. Related text classification tasks like authorship attribution are relevant to digital humanities, social sciences, and forensic linguistics. The goal of classification is to take a single observation, extract some useful features, and classify the given observation into one of a set of discrete classes. The rules of classification determined by humans can become inaccurate as situations or data change over time. As such, most classifications in language processing are done through supervised machine learning. The goal of supervised machine learning is to learn a classifier that is capable of mapping from a new document to its correct class. A probabilistic classifier gives the probability of the observation being in the class. Generative classifiers like naive Bayes return the class most likely to have generated some given observation. Discriminative classifiers like logistic regression learn about the features of the input to discriminate between the different possible classes.

4.1
The multinomial naive Bayes classifier makes a simplifying assumption, known as bag-of-words, about how the features of the input interact, by representing a text document as an unordered set of words with only their frequency recorded. Naive Bayes is a probabilistic classifier, returning the class that has the maximum posterior probability given the document. The Bayesian inference was built upon Baye's rule and first applied to text classification by Mosteller and Wallace. Naive Bayes is a generative model because it generate words by sampling from the conditional probability.The prior probability is the probability of a class and the likelihood is the conditional probability of the document given the class. The class that has the greatest product of the prior probability and the likelihood is considered the most probable class. The naive Bayes assumption is the conditional independence assumption that the probabilities of features given a class are independent and can be multiplied directly. Following the assumption that word positions matter, Naive Bayes calculations are done in log space, to avoid udnerflow and increase speed. Naive Bayes and linear regression belong to the class of linear classifiers that use a lienar combination of inputs to make a classification decision.

4.2
The maximum likelihood estimate of the conditional probability of any feature given a class can be calculated as the fraction of times a word appears among all words in all documents of that class. In the case that a word gives a zero probability with maximum likelihood training, the probability of the entire class would evaluate to zero. The simplest solution to avoid a zero overall probability is the add-one (Laplace) smoothing, which is commonly used in naive Bayes text categorization. Unknown words do not occur in any training document in any class, and they are, therefore, removed from the test document and excluded from all probabilities. Stop words are frequent words like the and a. Some systems choose to ignore stop words by sorting the vocabulary by defining the top 10-100 vocabulary entries as stop words, which is a better solution than using a predefined stop word list.

4.4
The binary multinomial naive Bayes or binary NB removes all duplicate words before concatenating them into a single big document, thus clipping the word counts in each document at 1. Binary NB is more efficient when the existence of a word matters more than its frequency. In sentiment analysis, negation is dealt with by prepending the prefix NOT to every word after a token of logical negation until the next punctuation mark. When there's insufficient labeled training data to train accurate naive Bayes classifiers, positive and negative word features can be derived from sentiment lexicons, including the General Inquirer, LIWC, the opinion lexicon of Hu and Liu, and the MPQA subjective Lexicon.

4.5
Features in naive Bayes may express any property of the input text. For example, for spam detection, only a set of likely words or phrases are predefined as features. Language ID determines what language a given piece of information is written in . The most effective naive Bayes features for Language ID are character n-grams or byte n-grams, in which strings and spaces are represented as raw bytes. The naive Bayes system langid.py uses feature selection to determine the most informative final features. Language ID systems are trained on multilingual texts, including the 68 languages that Wikipedia operates in. The systems have added Twitter text, Bible and Quran translations, slang websites, and many more to make sure that the multilingual text accurately reflects different regions, dialects, and socioeconomic classes.

4.6
Naive Bayes models are similar to language modeling in that they can be viewed as a set of class-specific unigram language models. The probability of a sentence being positive is the total product of the individual probabilities that each word in the sentence is positive.

4.7.0
Gold labels are human-defined labels that verify system outputs. A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using both system output and gold labels as parameters. Accuracy is calculated as the percentage of correct system output, but accuracy is an inefficient matric for text classification tasks in which the classes are unbalanced. Precision measures the percentage of true gold-labeled positives out of all system-labeled positives. Recall measures the percentage of true gold-labeled positives out of true positives and true negatives. Incorporating both precision and recall, the F-measure is expressed as a Harmonic mean and weighs the lower of the two metrices more heavily.

4.7.1
The Naive Bayes algorithm is a multi-class classification algorithm that can perform sentiment analysis beyond the positive and negative class. Macroaveraging averages over the performance for each class and reflects the statistics of the smaller classes more precisely, while microaverage computes precision and recall from a single confusion matrix with decisions for all classes.

4.8
Cross-validation allows all data be used for both training and testing. 10-fold cross-validation entails training the classifier on a set of randomly selected data, computing the error rate on the test set, and repeating the same procedure for ten times. However, cross-validation prohibits previewing the data to determine possible features.

4.9.0
Statistical significance testing compares the performance of multiple systems. The effect size presents the degree to which system A is better than system B. The null hypothesis supposes that the effective size is negative or zero, meaning that system A is not better than system B. The statistical significance testing attempts to rule out the null hypothesis. The p-value is the probability that the actual effect size is higher than it was in the hypothesis, assuming that the null hypothesis is true. A p-value below the threshold of 0.05 or 0.1 suggests that the result is statistically significant and the null hypothesis can be rejected. NLP research usually use non-parametric tests based on sampling, creating many versions of the experimental setup. The two most common non-parametric tests in NLP are approximate randomization and the bootstrap test. The paired version of the bootstrap test compares two sets of observations that are aligned.

4.9.1
The bootstrap test refers to repeatedly drawing large numbers of smaller samples with replacement from an original large sample, and it can be applied to any metric.

4.10
It is crucial to avoid harms that exist both for naive Bayes classifiers and other classification algorithms. Representational harms are caused by a system that demeans a social group, by perpetuating negative stereotypes about them. Censorship is another class of harms that may exist in toxicity detection. While toxicity detection is supposed to rule out hate speech, abuse, harassment, or other toxic languages, toxicity classifiers can incorrectly flag non-toxic sentences that simply mention minority identifies and lead to the censoring discourses by or about minority groups. Machine learning systems could have replicated and amplified biases in their training data, but classification harms can also be caused by the human labelers, by the resources used, or by the model architecture. A model card is released with each version of a model and clarifies any flaws that the model might have.


Author,Section,Question,Answer
Human Summary (A1),2.0,What was an early natural language processing system that used pattern matching to recognize phrases?,ELIZA
Human Summary (A1),2.0,What is ELIZA an example of?,chatbot
Human Summary (A1),2.0,What is the most important tool for text pattern characterization?,Regular expressions
Human Summary (A1),2.0,What uses regular expressions to convert text into a more standard form?,Text normalization
Human Summary (A1),2.0,What breaks a sentence into a sequence of words and punctuation marks?,tokenizing
Human Summary (A1),2.0,What is the process of finding root words of inflected words called?,Lemmatization
Human Summary (A1),2.0,What is the process of finding root words of inflected words?,Lemmatization
Human Summary (A1),2.0,What removes suffixes from words?,stemming
Human Summary (A1),2.0,What is a measure of similarity between two strings?,Edit distance
Human Summary (A1),2.1.1,What are case sensitive?,Regular expressions
Human Summary (A1),2.1.1,What specifies a disjunction of characters that can be matched in a position in a longer regular expression?,Square braces
Human Summary (A1),2.1.1,What does the regular expression /[012]/ match to?,"any digit among 0, 1, and 2"
Human Summary (A1),2.1.1,What indicates a range of characters?,dash
Human Summary (A1),2.1.1,"When a caret() is used within square braces as the first symbol, what does it indicate?",it indicates that the pattern behind it is negated
Human Summary (A1),2.1.1,What is indicated by the question mark?,Optional elements
Human Summary (A1),2.1.1,Optional elements are indicated by the question mark(?) which means what can be included or not?,the character preceding the question mark
Human Summary (A1),2.1.1,What does the Kleene star mean?,0 or more occurrences
Human Summary (A1),2.1.1,What does the Kleene + mean?,1 or more occurrences
Human Summary (A1),2.1.1,What is a wildcard expression that matches any single character?,The period
Human Summary (A1),2.1.1,What is the period(.)?,a wildcard expression
Human Summary (A1),2.1.1,The wildcard does not match with what?,carriage return
Human Summary (A1),2.1.1,What can the wildcard match with the Kleene star?,any string of characters
Human Summary (A1),2.1.1,What are symbols that anchor regular expressions to certain positions?,Anchors
Human Summary (A1),2.1.1,What symbol matches the end of a line?,dollar sign
Human Summary (A1),2.1.1,What is an anchor that matches with a boundary?,Backslash-b
Human Summary (A1),2.1.2,What does disjunction use?,pipe symbol
Human Summary (A1),2.1.2,What is used to indicate precedence for matching?,Parentheses
Human Summary (A1),2.1.2,Parentheses can define strings to match with for symbols that apply to what by default?,single characters
Human Summary (A1),2.1.2,"What prefers parentheses, counters, sequences and anchors, and disjunction in that order?",operator precedence hierarchy
Human Summary (A1),2.1.2,Regular expressions always match what?,largest possible string
Human Summary (A1),2.1.3,What increases precision and recall?,Reducing error
Human Summary (A1),2.1.4,What can you specify?,the number of an instance in a pattern
Human Summary (A1),2.1.4,How many instances of the previous character or expression are there?,four
Human Summary (A1),2.1.4,What can you specify in a pattern?,ranges
Human Summary (A1),2.1.4,What can you refer to with a backslash?,special characters
Human Summary (A1),2.1.4,What are some special characters that you can refer to with a backslash?,"( ), ( )"
Human Summary (A1),2.1.6,What is an important use of regular expressions?,Substitutions
Human Summary (A1),2.1.6,What do we use number operators to do?,match a specific expression twice or more in a text
Human Summary (A1),2.1.6,What backslash operator refers to the nth instance of a certain phrase or pattern in the text?,n
Human Summary (A1),2.1.6,When does the command(? :) indicate a non-capturing group?,after an opening parenthesis
Human Summary (A1),2.1.6,What is not placed in the register?,A non-capturing group
Human Summary (A1),2.1.7,What searches text ahead for patterns?,Lookahead assertions
Human Summary (A1),2.1.7,What match does the question-mark-equal-sign search for?,zero-width
Human Summary (A1),2.2,What is a computer-readable collection of text or dialogue?,A corpus
Human Summary (A1),2.2,What are utterances?,spoken sentences
Human Summary (A1),2.2,What are disfluencies?,fragments or fillers
Human Summary (A1),2.2,Fragments are what?,broken words
Human Summary (A1),2.2,Disfluencies can be hindrances or what kind of signals?,useful
Human Summary (A1),2.2,What is the set of words that share the same major word?,lemma
Human Summary (A1),2.2,How can we differentiate the number of words?,by counting tokens or types
Human Summary (A1),2.2,What are tokens?,total number of words
Human Summary (A1),2.2,What are the number of unique words?,Types
Human Summary (A1),2.2,Herdan's Law or Heap's Law states that the type equals what to the power of beta?,k times token
Human Summary (A1),2.2,How can we measure a corpus?,counting the number of lemmas
Human Summary (A1),2.2,What is the rough upper limit for the number of possible lemmas?,Dictionary entries or boldface forms
Human Summary (A1),2.3,What are some variations in languages?,"dialect, code switching, genre"
Human Summary (A1),2.3,Who can build datasheets to organize the properties of a corpus?,Corpus creators
Human Summary (A1),2.4.1,What must be normalized in order to be processed?,natural languages
Human Summary (A1),2.4.1,What does normalization involve?,"tokenizing words, normalizing formats, and segmenting words"
Human Summary (A1),2.4.1,What is a basic method of tokenizing words?,Unix
Human Summary (A1),2.4.1,"Unix can collapse, sort, and build statistics for the words in a corpus using commands such as what?",tr
Human Summary (A1),2.4.1,What are the most common words in a corpus?,"articles, pronouns, and prepositions"
Human Summary (A1),2.4.2,What does actual tokenization involve?,segmenting text into words
Human Summary (A1),2.4.2,What must we account for according to where and how they are used?,punctuation and special characters
Human Summary (A1),2.4.2,What can a tokenizer do?,expand clitic contractions
Human Summary (A1),2.4.2,"What is the identification of names, dates, organizations, etc?",Named entity recognition
Human Summary (A1),2.4.2,What is an example of clitic contraction?,I'm
Human Summary (A1),2.4.2,What is used for parsed corpora released by the LDC?,Penn Treebank tokenization standard
Human Summary (A1),2.4.2,What is important to tokenization before any other natural language processing can take place?,speed
Human Summary (A1),2.4.2,What do algorithms have to deal with?,ambiguities
Human Summary (A1),2.4.2,What language has no spaces?,Chinese
Human Summary (A1),2.4.2,What is each character in a word barrier?,a morpheme
Human Summary (A1),2.4.2,What language requires word segmentation?,Japanese
Human Summary (A1),2.4.2,What is used for segmentation in languages with more ambiguities?,Neural sequence models
Human Summary (A1),2.4.3,What can be automatically identified instead of pre-defining tokens as words or characters?,type of token
Human Summary (A1),2.4.3,Automatic identification of tokens can solve what problem?,unknown word problem
Human Summary (A1),2.4.3,Tokenizers often do what in order to solve the unknown word problem?,induce subwords
Human Summary (A1),2.4.3,What are most tokenization schemes made out of?,token learner and a token segmenter
Human Summary (A1),2.4.3,What induces a set of tokens from raw data?,token learner
Human Summary (A1),2.4.3,What takes raw sentences and segments it based on the tokens produced by the learner?,token segmenter
Human Summary (A1),2.4.3,What are the three widely-used algorithms that employ this method?,"byte-pair encoding, unigram language modeling, and WordPiece"
Human Summary (A1),2.4.3,What library implements byte-pair encoding and unigram language modeling?,The SentencePiece library
Human Summary (A1),2.4.3,What is another name for byte-pair encoding?,BPE
Human Summary (A1),2.4.3,BPE repeats the process until what number of merges have happened?,k
Human Summary (A1),2.4.3,What is the number of ens in a vocabulary?,k
Human Summary (A1),2.4.4,What standardizes words and tokens?,Word Normalization
Human Summary (A1),2.4.4,What maps all characters to one type of casing?,Case Folding
Human Summary (A1),2.4.4,Why is case folding not used in some cases?,disadvantageous
Human Summary (A1),2.4.4,What is the process of determining shared roots among words?,Lemmatization
Human Summary (A1),2.4.4,What does lemmatization involve?,complete morphological parsing of a word
Human Summary (A1),2.4.4,What are the two broad classes of morphemes?,"stems, and affixes"
Human Summary (A1),2.4.4,Morphemes can be divided into two broad classes: what is the central morpheme and supply the main meaning?,stems
Human Summary (A1),2.4.4,What are the add-ons that alter meanings or add more on?,Affixes
Human Summary (A1),2.4.4,What is a simpler version of lemmatization?,Stemming
Human Summary (A1),2.4.4,What is a widely used stemming algorithm based on a cascade rewriting process governed by a set of rules?,The Porter algorithm
Human Summary (A1),2.4.5,What is one of the most successful cues for sentence segmentation?,punctuation
Human Summary (A1),2.4.5,A period can be used as part of what?,abbreviations
Human Summary (A1),2.4.5,What identifies the role of a period before segmentation?,sentence tokenization
Human Summary (A1),2.5.0,What often involves identifying similarities between strings?,Natural language processing
Human Summary (A1),2.5.0,What is another term for determining whether two strings refer to the same entity?,coreference
Human Summary (A1),2.5.0,What is a way to quantify this measure of similarity?,Edit distance
Human Summary (A1),2.5.0,What is the minimum number of editing operations required to make two strings equal?,Minimum edit distance
Human Summary (A1),2.5.0,What is a correspondence between two strings?,their alignment
Human Summary (A1),2.5.0,What indicates the editing operations needed to equalize two strings?,operation list
Human Summary (A1),2.5.0,What is the most simple way to assign weight to editing operations?,Levenshtein distance
Human Summary (A1),2.5.0,How many methods did Levenshtein propose?,two weighting methods
Human Summary (A1),2.5.1,What is used to find the minimum edit distance?,dynamic programming
Human Summary (A1),2.5.1,What is a class of algorithms that attempts to solve problems by combining solutions to sub-problems?,Dynamic programming
Human Summary (A1),2.5.1,What are two examples of dynamic programming?,Viterbi algorithm and the CKY algorithm
Human Summary (A1),2.5.1,Who named the minimum edit distance algorithm?,Wagner and Fischer
Human Summary (A1),2.5.1,What is the distance where insertion and deletion have cost 1 and substitution has cost 2?,Levenshtein
Human Summary (A1),2.5.1,What can be formed by extending the edit distance algorithm?,Alignments
Human Summary (A1),2.5.1,From what cell does an edit distance matrix backtrace?,last cell
Human Summary (A1),2.5.1,What is a probabilistic extension of the minimum edit distance?,Viterbi algorithm
Human Summary (A1),3.0,What can assign probabilities to word sequences to predict the next word in a sentence?,Models
Human Summary (A1),3.0,"Probabilities can be used for speech recognition, spelling correction, grammatical error correction, and what else?",machine translation
Human Summary (A1),3.0,How many different candidates may we have for a translation of a sentence?,three
Human Summary (A1),3.0,What are models that assign probabilities to word sequences called?,language models
Human Summary (A1),3.0,What can refer to both a sequence of n words and the predictive model that assigns it a probability?,n-gram
Human Summary (A1),3.1,What does P(w|h)> mean?,the probability of a word w given history h
Human Summary (A1),3.1,Why are joint probabilities difficult to calculate?,too many possible sentences of a certain length
Human Summary (A1),3.1,What is one way we can compute the probability of a sequence?,the chain rule of probability
Human Summary (A1),3.1,How can we calculate the joint probability of a sequence?,by multiplying together several conditional probabilities
Human Summary (A1),3.1,What model approximates the history of a sequence using the last few words and uses that approximated history to estimate the probability of a word?,The n-gram model
Human Summary (A1),3.1,What assumes that the probability of a word only depends on the previous word?,The Markov assumption
Human Summary (A1),3.1,What models try to predict future words without looking far into the past?,Markov models
Human Summary (A1),3.1,What is an intuitive way of estimating n-gram probabilities?,Maximum likelihood estimation
Human Summary (A1),3.1,"To calculate the bigram probability of x given y, we calculate count of what corpus?",C(xy)
Human Summary (A1),3.1,What is the MLE?,n-gram parameter estimation
Human Summary (A1),3.1,What is the ratio of dividing the observed frequency of a particular sequence by the observed frequency of a prefix?,relative frequency
Human Summary (A1),3.1,Some bigram probabilities encode what?,syntactic rules
Human Summary (A1),3.1,All language model probabilities are represented in log format as what?,log probabilities
Human Summary (A1),3.1,What prevents storing too-small numbers?,Log probabilities
Human Summary (A1),3.2.0,What embeds a model in an application and measures how much the application improves in an end to end evaluation?,Extrinsic evaluation
Human Summary (A1),3.2.0,What measures the quality of a model independent of any application and requires a test set of unseen data?,Intrinsic evaluation
Human Summary (A1),3.2.0,What would happen if we accidentally trained the model on the test set?,bias
Human Summary (A1),3.2.0,What is a test set that has been used so often that we need a fresher test set?,A development set
Human Summary (A1),3.2.0,What do we want when dividing our data?,smallest test set
Human Summary (A1),3.2.1,What is the inverse probability of the test set normalized by the number of words?,perplexity
Human Summary (A1),3.2.1,What is the inverse probability of the test set normalized by the number of words?,Perplexity
Human Summary (A1),3.2.1,What do we want to do with perplexity in a model?,minimize perplexity
Human Summary (A1),3.2.1,What is perplexity referred to as?,weighted average branching factor of a language
Human Summary (A1),3.2.1,What is the branching factor?,the number of possible words that can follow any word
Human Summary (A1),3.2.1,What does not necessarily mean an extrinsic improvement in the performance of a model?,intrinsic improvement in perplexity
Human Summary (A1),3.3.0,Many statistical models including the n-gram are dependent on what?,training corpus
Human Summary (A1),3.3.0,What can an n-gram model do as the value of N is increased?,increase
Human Summary (A1),3.3.0,What does the better the coherence of an n-gram model?,the longer the context on which we train an n-gram
Human Summary (A1),3.3.0,What do we want an n-gram to answer and predict?,sentences
Human Summary (A1),3.3.0,What can we use to build an n-gram to answer and predict sentences that make sense in context?,training corpus of a similar genre
Human Summary (A1),3.3.0,What type of questions would a model for answering be trained with?,scientific
Human Summary (A1),3.3.0,What is important to consider when training a model?,dialect and variety
Human Summary (A1),3.3.0,What must we consider even after considering dialect and genre when training a model?,sparsity
Human Summary (A1),3.3.0,"If a training corpus fails to contain certain sequences, it is possible that the model incorrectly estimates that those sequences have a probability of",0
Human Summary (A1),3.3.0,Why are 0 probabilities a problem?,we are underestimating the probability for various words
Human Summary (A1),3.3.1,"In closed vocabulary systems, test sets contain only what?",known words
Human Summary (A1),3.3.1,What is the term for unknown words?,out of vocabulary
Human Summary (A1),3.3.1,What is the percentage of OOV in a test set called?,the OOV rate
Human Summary (A1),3.3.1,What is the pseudo-word used to model potential OOVs in a test set?,UNK>
Human Summary (A1),3.3.1,How many ways are there to train probabilities of UNK>?,two
Human Summary (A1),3.3.1,How does the first method turn the problem into a closed vocabulary situation?,by choosing a vocabulary fixed in advance
Human Summary (A1),3.3.1,What metrics does the UNK> model effect?,perplexity
Human Summary (A1),3.4.0,What is it called when we remove some probability mass for more frequent events and reassign it to unseen events with known words?,smoothing or discounting
Human Summary (A1),3.4.0,How many main methods of smoothing are studied?,four
Human Summary (A1),3.4.1,What is another name for Laplace smoothing?,add-one smoothing
Human Summary (A1),3.4.1,What is the unigram probability of word w_i?,c_i
Human Summary (A1),3.4.1,What is the adjusted count c* easier to compare with?,MLE counts
Human Summary (A1),3.4.1,What is a smoothing algorithm described as?,relative discount d_c
Human Summary (A1),3.4.2,What moves less probability mass from the seen to the unseen than Laplace smoothing?,Add-k smoothing
Human Summary (A1),3.4.2,How can add-k smoothing be done?,by optimizing on a devset
Human Summary (A1),3.4.2,Add-k doesn't work well for what?,language modeling
Human Summary (A1),3.4.3,"In backoff, we use a less-context n-gram if what?",evidence is not sufficient
Human Summary (A1),3.4.3,What is the backoff with discounting called?,Katz backoff
Human Summary (A1),3.4.3,What smoothing method is Katz backoff often used with?,Good-Turing
Human Summary (A1),3.4.3,In what method do we mix the probability estimates from all the n-gram estimators?,interpolation
Human Summary (A1),3.4.3,What can converge on locally optimal lambda values?,EM algorithm
Human Summary (A1),3.5,What is one of the most used and best performing n-gram smoothing methods?,Kneser-Ney Smoothing
Human Summary (A1),3.5,What is Kneser-Ney based on?,absolute discounting
Human Summary (A1),3.5,What is the contexts of a word w?,the number of bigram types it completes
Human Summary (A1),3.5,What is the best performing Kneser-Ney smoothing?,modified Kneser-Ney smoothing
Human Summary (A1),3.6,What is it possible to build by using text from enormous collections such as the web?,huge language models
Human Summary (A1),3.6,What company released the Web 1 Trillion 5-gram corpus?,Google
Human Summary (A1),3.6,What type of hash numbers are stored in memory?,64-bit
Human Summary (A1),3.6,How can n-grams be shrunk?,pruning
Human Summary (A1),3.6,What is a method used to build approximate language models?,Bloom filters
Human Summary (A1),3.6,What toolkit uses sorted arrays to build efficient probability tables?,KenLM
Human Summary (A1),3.6,What is it possible to build huge language models using?,Kneser-Ney smoothing
Human Summary (A1),3.6,What does Brans et al. (2007) show can suffice?,simpler stupid backoff
Human Summary (A1),3.6,What does not attempt to make the language model a true probability distribution?,Stupid backoff
Human Summary (A1),3.6,Who found that a value of 0.4 works well for lambda in a stupid backoff algorithm?,S. Brants et al.
Human Summary (A1),3.6,What value works well for lambda in a stupid backoff algorithm?,0.4
Human Summary (A1),3.7,The concept of perplexity comes from the information-theoretic concept of what?,cross-entropy
Human Summary (A1),3.7,What is the entropy of random variable X?,p(x)
Human Summary (A1),3.7,What is entropy measured by using log base 2?,entropy in bits
Human Summary (A1),3.7,What can also be considered a lower bound on the number of bits required to encode something in the optimal coding scheme?,Entropy
Human Summary (A1),3.7,What is another name for the entropy rate?,per-word entropy
Human Summary (A1),3.7,What is the Shannon-McMillan-Breiman theorem?,if the language is stationary and ergodic
Human Summary (A1),3.7,What is stationary if the probabilities it assigns to a sequence are not affected by shifts in the time index?,A stochastic process
Human Summary (A1),3.7,What is an upper bound on entropy?,Cross-entropy
Human Summary (A1),3.7,What can now be defined as the exp of cross-entropy H?,Perplexity
Human Summary (A1),4.0,What is a core part of sentient intelligence?,Classification
Human Summary (A1),4.0,What is a task that classifies and assigns labels to a text or document?,Text categorization
Human Summary (A1),4.0,What identifies the positive or negative attitude of a writer towards an object?,Sentiment analysis
Human Summary (A1),4.0,What is a classification class that assigns an email to one of two classes spam or not-spam?,Spam detection
Human Summary (A1),4.0,What recognizes which language a task is written in?,Language id
Human Summary (A1),4.0,What is the task of identifying a text's author?,Authorship attribution
Human Summary (A1),4.0,What is one of the oldest text classification tasks?,Assigning a library subject category or topic to a text
Human Summary (A1),4.0,Where is classification needed?,levels smaller than the document
Human Summary (A1),4.0,What can be considered a form of classification?,"Period disambiguation, word tokenization, language modeling"
Human Summary (A1),4.0,What is the goal of classification?,"to take a single observation, extract features, and classify the observation into a specific class"
Human Summary (A1),4.0,What do many areas of language processing use?,rule-based classifiers
Human Summary (A1),4.0,What does a probabilistic classifier output?,probability of an object belonging to a certain class
Human Summary (A1),4.0,What type of classifier builds a model of how a class could generate input?,Generative classifiers
Human Summary (A1),4.0,What classifiers learn which features from an observation are most useful to differentiate classes?,Discriminative classifiers
Human Summary (A1),4.1,What is a probabilistic Bayesian classifier that makes simplifying assumptions about how features interact?,The multinomial naive Bayes classifier
Human Summary (A1),4.1,What does the naive Bayes classifier consider a document as?,a bag-of-words
Human Summary (A1),4.1,What type of model is naive Bayes?,generative
Human Summary (A1),4.1,How do we compute most probable class c-hat given document d?,choosing the class with the highest product of the prior probability P(c) and the likelihood P(d|c)
Human Summary (A1),4.1,What are the two assumptions that naive Bayes classifiers make?,"the bag-of-words assumption (position does not matter), and the naive Bayes assumption"
Human Summary (A1),4.1,What are classifiers like naive Bayes that use a linear combination of inputs called?,linear classifiers
Human Summary (A1),4.2,What must consist of the union of all word types in all classes involved?,Vocabulary V
Human Summary (A1),4.2,What naively multiplies all feature likelihoods?,naive Bayes
Human Summary (A1),4.2,What is the simplest solution for preventing zero probability classes?,Laplace smoothing
Human Summary (A1),4.2,What does the naive Bayes ignore?,unknown words
Human Summary (A1),4.2,What do some systems completely ignore?,stop words
Human Summary (A1),4.4,What is more important to optimize the naive Bayes text classification for sentiment analysis?,whether a word occurs or not
Human Summary (A1),4.4,What is another name for binary multinomial naive Bayes?,binary NB
Human Summary (A1),4.4,What is another issue to deal with?,negation
Human Summary (A1),4.4,What prefix is prepended to every word after a token of logical negation until the next punctuation mark?,NOT_
Human Summary (A1),4.4,What can we derive if we lack labeled training data to train accurate naive Bayes classifiers?,positive and negative features
Human Summary (A1),4.4,What are some popular sentiment lexicons?,"General Inquirer, LIWC, opinion lexicon of Hu and Liu, and MPQA Subjectivity Lexicon"
Human Summary (A1),4.5,What can change features to express any property of input text?,Naive Bayes
Human Summary (A1),4.5,What can non-linguistic features include in spam detection?,HTML details
Human Summary (A1),4.5,What are the most effective naive Bayes features?,character n-grams
Human Summary (A1),4.6,A naive Bayes model can be viewed as a set of what?,class-specific unigram language models
Human Summary (A1),4.7.0,Gold labels are human-defined labels for what?,documents
Human Summary (A1),4.7.0,What is a table visualizing how an algorithm performs with respect to human gold labels based on system output and gold labels?,confusion matrix
Human Summary (A1),4.7.0,What does each cell in a confusion matrix contain?,possible outcomes
Human Summary (A1),4.7.0,"Accuracy is not a good measurement for classifiers, so we turn to what?",precision and recall
Human Summary (A1),4.7.0,What is the percentage of items detected by the system that are in fact positive according to gold labels?,Precision
Human Summary (A1),4.7.0,What is the percentage of items actually present in input that were correctly identified?,Recall
Human Summary (A1),4.7.0,What is a single metric incorporating precision P and recall R?,F-measure
Human Summary (A1),4.7.0,What is the weighted mean of P and R?,harmonic mean
Human Summary (A1),4.7.1,Many classification involves what?,multiple classes
Human Summary (A1),4.7.1,What definitions must we modify to classify multiple classes?,precision and recall
Human Summary (A1),4.7.1,How do we combine precision and recall?,macroaveraging
Human Summary (A1),4.7.1,What is another way to combine precision and recall?,microaveraging
Human Summary (A1),4.8,Training and testing for text classification is similar to what?,language modeling
Human Summary (A1),4.8,What do we use to tune parameters and decide on the best model?,the development test set
Human Summary (A1),4.8,What randomly chooses a training and test set division of our data?,Cross-validation
Human Summary (A1),4.8,How many times does cross-validation give an average error rate?,10-fold
Human Summary (A1),4.8,What is the problem with cross-validation?,blind
Human Summary (A1),4.9.0,What is used to compare the performance of two systems?,Statistical significance testing
Human Summary (A1),4.9.0,"What is the performance difference between M(A,x) and M(A,b)?",delta(x)
Human Summary (A1),4.9.0,What is the effect size?,delta(x)
Human Summary (A1),4.9.0,What is the effect size?,larger
Human Summary (A1),4.9.0,"To check if system A is actually better than system B, we must check what over other test sets?",its superiority
Human Summary (A1),4.9.0,What is the null hypothesis that supposes that delta is negative or zero?,Hypothesis H_0
Human Summary (A1),4.9.0,What random variable is created over all test sets to find if we can rule out H_0?,X
Human Summary (A1),4.9.0,What determines a system difference is statistically significant?,the delta we saw has a probability under the threshold
Human Summary (A1),4.9.0,What are the two common non-parametric tests used in NLP?,approximate randomization and the bootstrap test
Human Summary (A1),4.9.0,What are tests where we compare two sets of aligned observations?,Paired tests
Human Summary (A1),4.9.1,What can apply to any metric?,bootstrap test
Human Summary (A1),4.9.1,The bootstrap test refers to repeatedly drawing what with replacement from an original larger sample?,large numbers of smaller samples
Human Summary (A1),4.9.1,What does the bootstrap test create from an observed test set?,virtual tests
Human Summary (A1),4.10,What can cause a variety of harms?,classifiers
Human Summary (A1),4.10,What class of harms are caused by a system that demeans a social group?,Representational
Human Summary (A1),4.10,"What aim to detect hate speech abuse, harassment, etc. but may act as censors instead?",Toxicity detection
Human Summary (A1),4.10,What can cause these issues?,biases in training data
Human Summary (A1),4.10,What is one way to study and clarify biases in training data?,releasing a model card
Human Summary (A2),2.0,What is an early natural language processing system that mimiced a Rogerian psychotherapist to carry on conversation?,ELIZA
Human Summary (A2),2.0,ELIZA's responses were what?,limited
Human Summary (A2),2.0,ELIZA and what other system still play a crucial role in natural language processing today?,chatbots
Human Summary (A2),2.0,"What does ELIZA use to convert text into a more convenient, standard form?",text normalization
Human Summary (A2),2.0,What identifies the implicit shared roots between words?,lemmatization
Human Summary (A2),2.0,How is lemmatization done?,stripping the suffixes
Human Summary (A2),2.0,What is used to break up a text into individual sentences?,punctuation
Human Summary (A2),2.0,What does edit distance evaluate?,degree of resemblance between two strings
Human Summary (A2),2.1.1,What is an algebraic notation for characterizing a set of strings that is useful for searching for a pattern or through a corpus?,The regular expression
Human Summary (A2),2.1.1,What will be shown in the regular expression?,first matching result
Human Summary (A2),2.1.1,Is the search string case sensitive or case sensitive?,case sensitive
Human Summary (A2),2.1.1,What can more easily look for characters over a range?,dash
Human Summary (A2),2.1.1,What specifies the non-existence of a character?,The caret
Human Summary (A2),2.1.1,What does the (?) mean?,the preceding character or nothing
Human Summary (A2),2.1.1,What does the (?) mean?,the preceding character or nothing
Human Summary (A2),2.1.1,What means zero or more occurrences of the immediately preceding character or regular expression in square braces?,the Kleene star
Human Summary (A2),2.1.1,What special case is equivalent to the Kleene star with one more occurrences?,Kleene plus
Human Summary (A2),2.1.1,What is an expression that matches any single character in the form of a period?,wildcard
Human Summary (A2),2.1.1,What are special characters that fix regular expressions to particular places in a string?,Anchors
Human Summary (A2),2.1.1,What matches a non-word boundary?,B
Human Summary (A2),2.1.2,The pipe symbol signifies what?,disjunction
Human Summary (A2),2.1.2,The parenthesis operator is often used in combination with what?,counters
Human Summary (A2),2.1.2,"What does the regular expression follow in the order of parenthesis, counters, sequences and anchors, and disjunction?",the operator precedence hierarchy
Human Summary (A2),2.1.2,Non-greedy matching can be enforced by what?,question mark qualifier
Human Summary (A2),2.1.3,How can we avoid false positives?,increasing precision
Human Summary (A2),2.1.3,How can we avoid false negatives?,increasing recall
Human Summary (A2),2.1.4,How can we use explicit numbers as counters?,curly brackets
Human Summary (A2),2.1.4,What expression can be used to indicate occurrences within a range?,Modified curly brackets
Human Summary (A2),2.1.4,The newline character and tab character are referred to by special notation based on what?,the backslash
Human Summary (A2),2.1.4,Characters that are special themselves are what?,preceded with a backslash
Human Summary (A2),2.1.6,What operator allows a string characterized by a regular expression to be replaced by another string?,substitution operator
Human Summary (A2),2.1.6,What refers back to the first pattern described by the parenthesis operator?,The number operator
Human Summary (A2),2.1.6,What is the use of parentheses to store a pattern in a numbered register?,The capture group
Human Summary (A2),2.1.6,What is matched with?,second capture group
Human Summary (A2),2.1.6,What is used to group but not capture the resulting pattern in a register?,non-capturing group
Human Summary (A2),2.1.6,What are crucial in implementing simple chatbots like ELIZA?,Substitutions and capture groups
Human Summary (A2),2.1.7,What assertion looks for future matchings without advancing the match cursor?,lookahead
Human Summary (A2),2.1.7,What is the operator of the lookahead assertion?,zero-width
Human Summary (A2),2.1.7,What is used to rule out special cases while parsing complex pattern?,Negative lookahead
Human Summary (A2),2.2,What is a computer-readable collection of text or speech?,A corpus
Human Summary (A2),2.2,What is an utterance?,spoken correlate of a sentence
Human Summary (A2),2.2,What are examples of disfluencies?,fragments and fillers or filled pauses
Human Summary (A2),2.2,"What is a set of lexical forms having the same stem, major part-of-speech, and word sense?",A lemma
Human Summary (A2),2.2,What is the derived form of a word?,The word-form
Human Summary (A2),2.2,What are Types |V|?,the numbers of distinct words in a corpus
Human Summary (A2),2.2,What is the relationship between the number of types |V| and number of tokens N called?,Herdan's Law or Heaps' Law
Human Summary (A2),2.2,What is a rough upper bound on the number of lemmas?,boldface forms
Human Summary (A2),2.3,How many languages are there in the world?,7098
Human Summary (A2),2.3,What is the phenomenon where speakers and writers use multiple languages in a single communicative act?,Code switching
Human Summary (A2),2.3,What are other dimensions of variation?,"genre, demographic characteristics of the writer or speaker, and time"
Human Summary (A2),2.3,What specifies properties of a dataset used in the development of computational models?,data statement
Human Summary (A2),2.3,What does the datasheet outline?,"motivation, situation, language variety, speaker demographics, collection process, annotation process, and distribution"
Human Summary (A2),2.4.1,What are the minimal tasks applied as part of any normalization process?,"Tokenizing words, normalizing word formats, and segmenting sentences"
Human Summary (A2),2.4.1,What command in Unix changes particular characters in the input?,The tr command
Human Summary (A2),2.4.1,The sort command sorts input lines in what order?,alphabetical order
Human Summary (A2),2.4.1,What are function words?,"articles, pronouns, prepositions"
Human Summary (A2),2.4.2,What is the task of segmenting running text into words?,Tokenization
Human Summary (A2),2.4.2,What are intentionally included in most NLP applications because they represent word boundaries?,Punctuations and numbers
Human Summary (A2),2.4.2,What can a tokenizer do in alphabetic languages?,expand clitic contractions
Human Summary (A2),2.4.2,What standard separates out clitics?,Penn Treebank tokenization
Human Summary (A2),2.4.2,What do deterministic algorithms compile into?,finite state automata
Human Summary (A2),2.4.2,What is more complex in languages that do not use spaces to mark potential word-boundaries?,Word tokenization
Human Summary (A2),2.4.2,What does each character represent in Chinese?,a morpheme
Human Summary (A2),2.4.2,What do most Chinese NLP tasks take as input?,characters
Human Summary (A2),2.4.2,What are too small to be a unit?,Japanese and Thai characters
Human Summary (A2),2.4.3,What learns facts about an unknown language from a training corpus?,NLP algorithms
Human Summary (A2),2.4.3,What are tokens smaller than words?,Subwords
Human Summary (A2),2.4.3,Most tokenization schemes have a token learner and what?,token segmenter
Human Summary (A2),2.4.3,What is the second part of most tokenization schemes?,token segmenter
Human Summary (A2),2.4.3,What segments a raw test sentence into the tokens in the vocabulary?,token segmenter
Human Summary (A2),2.4.3,What is one of the three widely used algorithms?,WordPiece
Human Summary (A2),2.4.3,What implements both byte-pair encoding and unigram language modeling?,a SentencePiece library
Human Summary (A2),2.4.3,What is also known as the BPE algorithm?,byte-pair encoding
Human Summary (A2),2.4.3,What does the BPE algorithm run?,inside words that are white-space-separated
Human Summary (A2),2.4.3,What is a parameter of the BPE algorithm?,k
Human Summary (A2),2.4.3,The BPE token parser evaluates the test sentence using what?,the given vocabulary
Human Summary (A2),2.4.3,"In real BPE algorithms, only what words will be represented by their parts?",very rare
Human Summary (A2),2.4.4,What is the task of putting words and tokens in a standard format?,Word normalization
Human Summary (A2),2.4.4,What is case folding not used for?,"sentiment analysis, information extraction, and machine translation"
Human Summary (A2),2.4.4,What is the task of determining that two words have the same root?,Lemmatization
Human Summary (A2),2.4.4,What can all morphemes be grouped into?,stems
Human Summary (A2),2.4.5,What are the most useful cues for sentence segmentation?,punctuations
Human Summary (A2),2.4.5,What determines the purpose of a punctuation?,the sentence tokenization
Human Summary (A2),2.5.0,What is the task of deciding whether two strings refer to the same entity?,Coreference
Human Summary (A2),2.5.0,What quantifies string similarity?,Edit distance
Human Summary (A2),2.5.0,What is a correspondence between substrings of the two sequences?,An alignment
Human Summary (A2),2.5.0,"The operation list contains symbols expressing the editing operations needed, following what?","d for deletion, s for substitution, and i for insertion"
Human Summary (A2),2.5.0,What is the weighting factor in which each of the three editing operations has a cost of 1?,The Levenshtein distance between two sequences
Human Summary (A2),2.5.1,What is a class of algorithms that solve problems by combining solutions to sub-problems?,Dynamic programming
Human Summary (A2),2.5.1,What are two examples of algorithms that use dynamic programming?,Viterbi algorithm and the CKY algorithm
Human Summary (A2),2.5.1,Who named the minimum edit distance algorithm?,Wagner and Fischer
Human Summary (A2),2.5.1,What step of the minimum edit distance algorithm calculates the distance for the zeroth row and column?,The initialization step
Human Summary (A2),2.5.1,What step takes the minimum of three possible paths to determine the minimum edit distance for the rest of the matrix?,The recurrence relation
Human Summary (A2),2.5.1,What is one way to visualize the minimum edit distance algorithm?,boldfaced cells
Human Summary (A2),2.5.1,What does each boldfaced cell represent?,an alignment of a pair of letters in the two strings
Human Summary (A2),2.5.1,What do two boldfaced cells present in the same row?,insertion
Human Summary (A2),2.5.1,What is a procedure in which we follow the pointers from the last cell?,The backtrace
Human Summary (A2),3.0,"What is used in speech recognition, spelling correction, and machine translation?",Probability
Human Summary (A2),3.0,Probability is important for what type of devices for the disabled?,AAC devices
Human Summary (A2),3.0,What are models that assign probability to sequences of a word called?,language models
Human Summary (A2),3.0,What is the simplest example of LMs?,n-gram
Human Summary (A2),3.1,What doesn't give good estimates when new sentences are created?,Estimating probabilities directly from counts
Human Summary (A2),3.1,What computes probability of a word given previous words?,chain rule of probability
Human Summary (A2),3.1,What model approximates the history by just the last few words?,The n-gram model
Human Summary (A2),3.1,What does the bigram model predict given the first word?,the conditional probability of the last word
Human Summary (A2),3.1,What assumption states that the probability of a word depends only on the previous word?,Markov
Human Summary (A2),3.1,What gives the parameters of an n-gram model by normalizing the counts from a corpus so that they lie between 0 and 1?,MLE
Human Summary (A2),3.1,What is given by the bigram of xy over the unigram count for x?,x
Human Summary (A2),3.1,What is the ratio between the observed frequency of a particular sequence and the observed frequency of a prefix?,relative frequency
Human Summary (A2),3.1,How are bigram probabilities normalized?,by dividing each cell by the set of unigram probabilities
Human Summary (A2),3.1,What model is used when there is sufficient training data?,trigram models
Human Summary (A2),3.1,"To avoid numerical underflow, language model probabilities are always represented and computed as what?",log probabilities
Human Summary (A2),3.2.0,How does an extrinsic evaluation determine the performance of a language model?,by embedding it in an application
Human Summary (A2),3.2.0,What measures the quality of a model independent of any application?,intrinsic evaluation metric
Human Summary (A2),3.2.0,What is the quality of an n-gram model from its training set measured by?,performance on a test set in the held out corpora
Human Summary (A2),3.2.0,Which n-gram model assigns a higher probability to the test set?,the one that better fits the test set
Human Summary (A2),3.2.0,What is the initial test set that has never been used before?,The development test
Human Summary (A2),3.2.0,Training on the same test set introduces a bias that gives disproportionally high probabilities and causes what?,huge inaccuracies in perplexity
Human Summary (A2),3.2.1,What is the perplexity of a language model on a test set?,the inverse probability of the test set
Human Summary (A2),3.2.1,What is the greater the language model's test set probability?,The smaller the perplexity
Human Summary (A2),3.2.1,What is another definition of perplexity?,the weighted average branching factor of a language
Human Summary (A2),3.2.1,How must the n-gram model P be constructed?,without any knowledge of the test set or its vocabulary
Human Summary (A2),3.2.1,What does not guarantee an extrinsic improvement in the performance of a specific language processing task?,an intrinsic improvement in perplexity
Human Summary (A2),3.3.0,The n-gram model is dependent on what?,training corpus
Human Summary (A2),3.3.0,What increases in the n-gram model?,the word tokens N
Human Summary (A2),3.3.0,What is important to get training data in?,dialect or variety
Human Summary (A2),3.3.0,What are phrases that don't occur in the training set but do occur in the test set?,Zeros
Human Summary (A2),3.3.0,Zeros cause the underestimation of all other phrases and give what probability of the entire test set?,false negative probability
Human Summary (A2),3.3.1,What is a closed vocabulary system?,the test set doesn't contain any words outside of the given lexicon
Human Summary (A2),3.3.1,The closed vocabulary system is often used in speech recognition or what?,machine translation
Human Summary (A2),3.3.1,Unknown words are also called what?,out of vocabulary
Human Summary (A2),3.3.1,What is the percentage of OOV words that appear in the test set?,OOV
Human Summary (A2),3.3.1,What is a pseudo-word called in an open vocabulary system?,UNK
Human Summary (A2),3.3.1,How can we train the probabilities of unknown words in an open vocabulary system?,convert any unknown words to the word token
Human Summary (A2),3.3.1,What can a language model achieve by choosing a small vocabulary and assigning the unknown word a high probability?,low perplexity
Human Summary (A2),3.4.0,What is the procedure of transferring the probability mass of frequent events to other words that appear in the test set in an unseen context?,Smoothing or discounting
Human Summary (A2),3.4.1,How many bigram counts does the Laplace smoothing algorithm add?,one
Human Summary (A2),3.4.1,What is the Laplace smoothing algorithm used for?,text classification
Human Summary (A2),3.4.1,What does the Laplace smoothing algorithm define instead of adding one to both the numerator and denominator of the probability?,an adjusted count
Human Summary (A2),3.4.1,What is the normalization factor N over N plus V?,the total number of word tokens
Human Summary (A2),3.4.1,What can the smoothing algorithm be viewed as?,discounting non-zero counts and reassigning probability mass to zero counts
Human Summary (A2),3.4.1,The smoothing algorithm can be described in terms of what?,relative discount d
Human Summary (A2),3.4.1,What shows both the discounted previously-nonzero counts and increased previously-zero counts?,The reconstructed count matrix
Human Summary (A2),3.4.2,What is the algorithm that adds a fractional count k to each count called?,add-k smoothing
Human Summary (A2),3.4.2,How can the fractional count k be chosen?,by optimizing on a devset
Human Summary (A2),3.4.3,What helps to generalize more for unlearned contexts?,bigram
Human Summary (A2),3.4.3,When do we move along the n-gram hierarchy?,until there is sufficient evidence
Human Summary (A2),3.4.3,In what method do we mix the probability estimates from all n-gram estimators?,interpolation
Human Summary (A2),3.4.3,The different order n-grams are each weighted by what?,lambda value
Human Summary (A2),3.4.3,Where are lambda values or coefficients learned from?,held-out corpus
Human Summary (A2),3.4.3,What is an iterative learning algorithm that converges on locally optimal lambdas?,EM algorithm
Human Summary (A2),3.4.3,What are the higher-order n-grams discounted by a function alpha to save some probability mass for?,lower-order n-grams
Human Summary (A2),3.4.3,What is the backoff n-gram model with discounting called?,Katz backoff
Human Summary (A2),3.5,What is the most commonly used and best performing n-gram smoothing method?,interpolated Knerser-Ney algorithm
Human Summary (A2),3.5,What did the interpolated Knerser-Ney algorithm originate from?,absolute discounting
Human Summary (A2),3.5,What is the unigram model created by the Knerser-Ney algorithm called?,PContunuation
Human Summary (A2),3.5,What is the best performing version of Knerser-Ney smoothing called?,modified Knerser-Ney smoothing
Human Summary (A2),3.6,By using text from web or other what is it possible to build extremely large language models?,enormous collections
Human Summary (A2),3.6,What is the COCA?,Corpus of Contemporary American English
Human Summary (A2),3.6,How many bit hash numbers do large sets of n grams usually store each word as?,sixtyfour
Human Summary (A2),3.6,How can n grams be shrunk?,pruning
Human Summary (A2),3.6,What is used to build appropriate language models?,Bloom filters
Human Summary (A2),3.6,What do efficient language model toolkits use?,sorted arrays
Human Summary (A2),3.6,What is used when the full Knerser-ney smoothing is unnecessary?,The stupid backoff algorithm
Human Summary (A2),3.7,What is the measure of information?,Entropy
Human Summary (A2),3.7,What is the resulting entropy measured as?,lower bound on the number of bits it would take to encode a certain piece of information in the optimal coding scheme
Human Summary (A2),3.7,What is the entropy of a sequence divided by the number of words?,The entropy rate or per-word entropy
Human Summary (A2),3.7,"What theorem states that if the language is regular, the entropy of the given language would be the entropy of",Shannon-McMillan-Breiman theorem
Human Summary (A2),3.7,What is stationary if the probabilities it assigns to a sequence are independent of the shifts in the time index?,The stochastic process
Human Summary (A2),3.7,"What are stationary, but natural language is not stationary?",Markov models and n-grams
Human Summary (A2),3.7,How can we compute the entropy of natural language?,taking a sufficiently long sample of the output to determine its average log probability
Human Summary (A2),3.7,When is cross-entropy useful?,when the actual probability distribution p used to generate some data is unknown
Human Summary (A2),3.7,What model of the probability distribution p is used to calculate the cross-entropy of m on p?,m
Human Summary (A2),3.7,What would give a lower cross-entropy?,The more accurate model
Human Summary (A2),3.7,The perplexity of a model P on a sequence of words W can be formally defined as what?,some exponential of the cross-entropy
Human Summary (A2),4.0,What is the task of assigning a label or category to an entire text or document?,Text categorization
Human Summary (A2),4.0,"What is the extraction of sentiment, the positive or negative orientation that a writer expresses towards some object?",Sentiment analysis
Human Summary (A2),4.0,What is the binary classification task of assigning an email to either spam or not-spam?,Spam detection
Human Summary (A2),4.0,What is the task of determining the language of a text?,Language id
Human Summary (A2),4.0,"Related text classification tasks like what are relevant to digital humanities, social sciences, and forensic linguistics?",authorship attribution
Human Summary (A2),4.0,What is the goal of classification?,"to take a single observation, extract some useful features, and classify the given observation into one of a set of discrete classes"
Human Summary (A2),4.0,Who determines the rules of classification?,humans
Human Summary (A2),4.0,Most classifications in language processing are done through what?,supervised machine learning
Human Summary (A2),4.0,What is the goal of supervised machine learning?,learn a classifier that is capable of mapping from a new document to its correct class
Human Summary (A2),4.0,What does a probabilistic classifier give?,probability of the observation being in the class
Human Summary (A2),4.0,What type of classifiers return the class most likely to have generated some given observation?,Generative classifiers
Human Summary (A2),4.0,Discriminative classifiers like what learn about the features of the input to discriminate between the different possible classes?,logistic regression
Human Summary (A2),4.1,What is the simplifying assumption made by the multinomial naive Bayes classifier?,bag-of-words
Human Summary (A2),4.1,What is a probabilistic classifier?,Naive Bayes
Human Summary (A2),4.1,Who first applied the Bayesian inference to text classification?,Mosteller and Wallace
Human Summary (A2),4.1,Naive Bayes generates words by sampling from what?,conditional probability
Human Summary (A2),4.1,The class that has the greatest product of the prior probability and the likelihood is considered what?,the most probable class
Human Summary (A2),4.1,What is the conditional independence assumption?,the probabilities of features given a class are independent and can be multiplied directly
Human Summary (A2),4.1,Why are Naive Bayes calculations done in log space?,increase speed
Human Summary (A2),4.1,Naive Bayes and linear regression belong to the class of linear classifiers that use what to make a classification decision?,lienar combination of inputs
Human Summary (A2),4.2,What is calculated as the fraction of times a word appears among all words in all documents of a class?,The maximum likelihood estimate of the conditional probability
Human Summary (A2),4.2,What would happen if a word gave a zero probability with maximum likelihood training?,probability of the entire class would evaluate to zero
Human Summary (A2),4.2,What is the simplest solution to avoid a zero overall probability?,add-one (Laplace) smoothing
Human Summary (A2),4.2,What do not occur in any training document in any class?,Unknown words
Human Summary (A2),4.2,What are frequent words like the and a?,Stop words
Human Summary (A2),4.2,How do some systems ignore stop words?,by sorting the vocabulary
Human Summary (A2),4.4,What is another name for binary multinomial naive Bayes?,binary NB
Human Summary (A2),4.4,When is binary NB more efficient?,when the existence of a word matters more than its frequency
Human Summary (A2),4.4,How is negation dealt with in sentiment analysis?,prepending the prefix
Human Summary (A2),4.4,"When there's insufficient labeled training data to train accurate naive Bayes classifiers, positive and negative word features can",sentiment lexicons
Human Summary (A2),4.5,Features in naive Bayes may express what?,any property of the input text
Human Summary (A2),4.5,What are features in naive Bayes for spam detection?,a set of likely words or phrases
Human Summary (A2),4.5,What determines what language a given piece of information is written in?,Language ID
Human Summary (A2),4.5,What are the most effective naive Bayes features for Language ID?,character n-grams or byte n-grams
Human Summary (A2),4.5,What naive Bayes system uses feature selection to determine the most informative final features?,langid.py
Human Summary (A2),4.5,What type of texts are Language ID systems trained on?,multilingual
Human Summary (A2),4.5,What does the naive Bayes system add to Wikipedia?,slang websites
Human Summary (A2),4.6,What are similar to language modeling in that they can be viewed as a set of class-specific unigram language models?,Naive Bayes models
Human Summary (A2),4.6,What is the probability of a sentence being positive?,the total product of the individual probabilities that each word in the sentence is positive
Human Summary (A2),4.7.0,What do gold labels do?,verify system outputs
Human Summary (A2),4.7.0,What is a table for visualizing how an algorithm performs with respect to the human gold labels?,confusion matrix
Human Summary (A2),4.7.0,What is calculated as the percentage of correct system output?,Accuracy
Human Summary (A2),4.7.0,What measures the percentage of true gold-labeled positives out of all system-labeled positives?,Precision
Human Summary (A2),4.7.0,What measures the percentage of true gold-labeled positives out of true positives and true negatives?,Recall
Human Summary (A2),4.7.0,What is the F-measure expressed as?,Harmonic mean
Human Summary (A2),4.7.1,What is a multi-class classification algorithm that can perform sentiment analysis beyond the positive and negative class?,The Naive Bayes algorithm
Human Summary (A2),4.7.1,What can the Naive Bayes algorithm perform beyond the positive and negative classes?,sentiment analysis
Human Summary (A2),4.7.1,What averages over the performance for each class?,Macroaveraging
Human Summary (A2),4.8,What allows all data to be used for both training and testing?,Cross-validation
Human Summary (A2),4.8,What does 10-fold cross-validation involve?,training the classifier on a set of randomly selected data
Human Summary (A2),4.8,What does 10-fold cross-validation involve?,computing the error rate on the test set
Human Summary (A2),4.8,What does cross-validation prohibit?,previewing the data
Human Summary (A2),4.9.0,What compares the performance of multiple systems?,Statistical significance testing
Human Summary (A2),4.9.0,What presents the degree to which system A is better than system B?,The effect size
Human Summary (A2),4.9.0,The null hypothesis assumes that the effective size is what?,negative or zero
Human Summary (A2),4.9.0,What attempts to rule out the null hypothesis?,statistical significance testing
Human Summary (A2),4.9.0,What is the probability that the actual effect size is higher than it was in the hypothesis?,The p-value
Human Summary (A2),4.9.0,A p-value below what threshold indicates that the result is statistically significant and the null hypothesis can be rejected?,0.05 or 0.1
Human Summary (A2),4.9.0,NLP research usually use non-parametric tests based on what?,sampling
Human Summary (A2),4.9.0,What are the two most common non-parametric tests in NLP?,approximate randomization and the bootstrap test
Human Summary (A2),4.9.0,What compares two sets of observations that are aligned?,The paired version of the bootstrap test
Human Summary (A2),4.9.1,What refers to drawing large numbers of smaller samples with replacement from an original large sample?,bootstrap test
Human Summary (A2),4.10,What is crucial to avoid that exist both for naive Bayes classifiers and other classification algorithms?,harms
Human Summary (A2),4.10,What type of harms are caused by a system that demeans a social group?,Representational
Human Summary (A2),4.10,What is another class of harms that may exist in toxicity detection?,Censorship
Human Summary (A2),4.10,Non-toxic sentences that simply mention what identifies can lead to the censoring discourses by or about minority groups.,minority
Human Summary (A2),4.10,What could have replicated and amplified biases in their training data?,Machine learning systems
Human Summary (A2),4.10,What is released with each version of a model and clarifies any flaws that the model might have?,A model card
Human Summary (A3),2.0,What is an example of an early NLP processing system that recognizes patterns and matches them with appropriate responses?,ELIZA
Human Summary (A3),2.0,What is the essence of ELIZA?,regular expressions
Human Summary (A3),2.0,Regular expressions are what?,strings
Human Summary (A3),2.0,What is it called when we want to standardize a text?,text normalization
Human Summary (A3),2.0,What is another example of text normalization?,splitting sentences
Human Summary (A3),2.0,What algorithm determines how similar words are between each other?,edit distance
Human Summary (A3),2.1.1,What is a simple type of regular expressions?,a sequence of characters
Human Summary (A3),2.1.1,What defines a disjunction of characters when we are matching them to the regular expressions?,Square braces
Human Summary (A3),2.1.1,What is used when we know the range of characters to match?,Dash
Human Summary (A3),2.1.1,What is used when you don't want to use a certain character?,Caret
Human Summary (A3),2.1.1,Where should the symbol go in order to negate the pattern?,the beginning of the square brackets
Human Summary (A3),2.1.1,What lets us match the word if it has the character before the question mark?,Question mark
Human Summary (A3),2.1.1,What lets us match the word if it has the character before the question mark?,if it has the character before the question mark
Human Summary (A3),2.1.1,What lets us match the word if it has the character before the question mark?,if it has the character before the question mark
Human Summary (A3),2.1.1,What lets us match the word if it has zero or more occurrences of the character before it or a regular expression before it?,Kleene star
Human Summary (A3),2.1.1,What lets us match the character with one or more occurrences of the character or regular expression before the Kleene (+)?,Kleene
Human Summary (A3),2.1.1,What is Period (.)?,a wildcard expression
Human Summary (A3),2.1.1,What is Period (.)?,a wildcard expression
Human Summary (A3),2.1.1,What do anchors do?,hold regular expressions to certain locations of the string
Human Summary (A3),2.1.2,"What means ""or""?",Pipe
Human Summary (A3),2.1.2,"What does (( )) mean ""and""?",Parentheses
Human Summary (A3),2.1.2,What determines which operator has higher precedence than another?,precedence hierarchy
Human Summary (A3),2.1.2,What is the order from highest precedence to lowest precedence?,"parenthesis, counters, sequences and anchors, disjunction"
Human Summary (A3),2.1.2,"When regular expressions are not matched, we match them with what?",largest string they can
Human Summary (A3),2.1.3,How many types of errors are there?,two
Human Summary (A3),2.1.3,What are the two types of errors?,false positives
Human Summary (A3),2.1.3,What is the other type of error called?,false negatives
Human Summary (A3),2.1.3,What is when we incorrectly match strings?,False positive
Human Summary (A3),2.1.3,What is when we incorrectly miss strings?,False negative
Human Summary (A3),2.1.3,What do less false positives errors mean?,more precision
Human Summary (A3),2.1.3,Less false negatives errors mean more what?,recall
Human Summary (A3),2.1.4,What can you specify?,the number of an instance in a pattern
Human Summary (A3),2.1.4,How many instances of the previous character or expression are there?,four
Human Summary (A3),2.1.4,What can you specify in a pattern?,ranges
Human Summary (A3),2.1.4,What can you refer to with a backslash?,special characters
Human Summary (A3),2.1.4,What are some special characters that you can refer to with a backslash?,"( ), ( )"
Human Summary (A3),2.1.6,What means replacing one string characterized with a regular expression with another string?,Substitution
Human Summary (A3),2.1.6,Capture group means using what to store a pattern in memory?,parentheses
Human Summary (A3),2.1.7,What syntax do Lookahead assertions use?,non-capture groups' syntax
Human Summary (A3),2.2,What is a collection of text or speech that a computer can read?,Corpus
Human Summary (A3),2.2,"Punctuation, fillers, and disfluencies are treated as what?",words
Human Summary (A3),2.2,"What is a set of lexical forms that share the steam, major part-of-speech, and word sense?",A lemma
Human Summary (A3),2.3,"What should be tested with many languages, not just one?",NLP algorithms
Human Summary (A3),2.3,What is the phenomenon of people using multiple languages in a single communicative act called?,code switching
Human Summary (A3),2.3,What is the ideal way of creating a corpus creator?,build a datasheet
Human Summary (A3),2.3,What does a datasheet help understand?,context
Human Summary (A3),2.3,What are the properties of a datasheet?,"motivation, situation, language variety, speaker demographics, collection process, annotation process, and distribution"
Human Summary (A3),2.4.1,What does normalizing a text mean?,tokenizing words
Human Summary (A3),2.4.1,How many words is a normalized text divided into?,one word per line
Human Summary (A3),2.4.1,What are the most frequent words?,function words
Human Summary (A3),2.4.1,What are some examples of function words?,"articles, pronouns, and prepositions"
Human Summary (A3),2.4.2,What is the task of segmenting running text into words?,Tokenization
Human Summary (A3),2.4.2,What do we keep in our tokenizations?,punctuation and numbers
Human Summary (A3),2.4.2,What are clitic contractions marked by?,apostrophes
Human Summary (A3),2.4.2,Tokenization is tied to what?,named entity recognition
Human Summary (A3),2.4.2,What is a common tokenization standard called?,Penn Treebank tokenization standard
Human Summary (A3),2.4.2,What does the Penn Treebank tokenization standard do?,"separates out clitics, keeps hyphenated words and separates punctuation"
Human Summary (A3),2.4.2,Tokenization is a task that can differ between what?,languages
Human Summary (A3),2.4.2,Tokenization can be segmented into what?,characters or into words
Human Summary (A3),2.4.3,What are sets of tokens that include tokens smaller than words called?,Subwords
Human Summary (A3),2.4.3,What are the two parts of tokenizations?,token learner and a token segmenter
Human Summary (A3),2.4.3,What is BPE?,byte-pair encoding
Human Summary (A3),2.4.4,What is standardizing the format of words and tokens?,Word normalization
Human Summary (A3),2.4.4,What is another kind of normalization?,Case folding
Human Summary (A3),2.4.4,What is an example of case folding?,mapping everything to lowercase
Human Summary (A3),2.4.4,What determines if two words share the same root?,Lemmatization
Human Summary (A3),2.4.5,What is an important step in text processing?,Sentence segmentation
Human Summary (A3),2.4.5,What are some clues in sentence segmentation?,"periods, question marks, and exclamation points"
Human Summary (A3),2.4.5,What are some challenges in sentence segmentation?,knowing whether a period is a full-stop or it's the end of an abbreviation
Human Summary (A3),2.5.0,What can help to quantify the similarity between strings?,Minimum edit distance
Human Summary (A3),2.5.0,What does minimum edit distance return?,the minimum number of editing operations needed to transform one string into another
Human Summary (A3),2.5.1,What is a method to solve problems that combines solutions to sub-problems?,Dynamic programming
Human Summary (A3),2.5.1,What are two examples of dynamic programming?,Viterbi algorithm and the CKY algorithm
Human Summary (A3),2.5.1,What is an application of the minimum edit distance algorithm?,spelling errors
Human Summary (A3),2.5.1,What is the name of the imum edit distance algorithm?,n
Human Summary (A3),2.5.1,What is the name of the relation where costs of insertion and deletion are calculated?,recurrence relation
Human Summary (A3),2.5.1,What is a probabilistic extension of the minimum edit distance algorithm?,Viterbi's algorithm
Human Summary (A3),3.0,"What is essential for speech recognition, spelling or grammar correction, machine translation, and augmentative communication technologies?",Predicting upcoming words
Human Summary (A3),3.0,What are models that assign probabilities to sequences of words called?,Language models
Human Summary (A3),3.1,What is a model that estimates the probability of the last word given the previous words?,N-gram
Human Summary (A3),3.1,Why is the N-gram model not as effective?,the constant change in languages
Human Summary (A3),3.1,What shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words?,The chain rule of probability
Human Summary (A3),3.1,What model approximates this probability by using only the conditional of the previous words?,The bigram model
Human Summary (A3),3.1,The n-gram model looks at how many words into the past?,n-1
Human Summary (A3),3.1,What is the assumption that the probability of a word depends only on the previous word?,Markov assumption
Human Summary (A3),3.1,What is another name for the maximum likelihood estimation?,MLE
Human Summary (A3),3.1,What are the normalized counts of a corpus?,between 0 and 1
Human Summary (A3),3.1,How do we estimate the n-gram probability?,by dividing the observed frequency of a particular sequence by the observed frequency of a prefix
Human Summary (A3),3.1,What is dividing the observed frequency of a particular sequence by the observed frequency of a prefix called?,relative frequency
Human Summary (A3),3.1,What do we usually convert the relative frequency of a particular sequence by the observed frequency of a prefix into?,logarithmic space
Human Summary (A3),3.2.0,What is used to evaluate the performance of a language model?,extrinsic evaluation
Human Summary (A3),3.2.0,What is the downside of extrinsic evaluation?,very expensive
Human Summary (A3),3.2.0,What ensures the quality of a model independent of any application?,intrinsic evaluation
Human Summary (A3),3.2.0,What are the three types of data that we divide the data in?,"training set, test set, held out set"
Human Summary (A3),3.2.0,What is the better model?,Whichever model assigns a higher probability to the test set
Human Summary (A3),3.2.0,What is problematic because it introduces a bias in the probabilities and causes inaccuracies in perplexity?,Training on the test set
Human Summary (A3),3.2.1,What is the inverse probability of a test set?,Perplexity
Human Summary (A3),3.2.1,What is equal to maximizing the test set probability according to the language model?,Minimizing perplexity
Human Summary (A3),3.3.0,N-grams are dependent on what?,training corpus
Human Summary (A3),3.3.0,To build a better model we need to do what in the training corpus?,match genres and dialects
Human Summary (A3),3.3.1,What do we call the words that are not known in our dictionary?,unknown words
Human Summary (A3),3.4.0,What is another name for not assigning zero to the probability of an unseen word in a test set?,discounting
Human Summary (A3),3.4.0,What is the name of the smoothing method?,Kneser-Ney
Human Summary (A3),3.4.1,What algorithm adds one to all the bigram counts before normalizing them into probabilities?,Laplace smoothing
Human Summary (A3),3.4.2,What smoothing is to move a bit less of the probability mass from the seen to the unseen events?,Add-k
Human Summary (A3),3.4.2,What does add-k smoothing add?,fractional count
Human Summary (A3),3.4.3,What does backoff use if the evidence is sufficient?,bigram
Human Summary (A3),3.4.3,What does backoff use if the evidence is sufficient?,trigram
Human Summary (A3),3.4.3,In what method do we mix the probability estimates from all the n-gram estimators?,interpolation
Human Summary (A3),3.5,What company augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution?,Kneser-Ney
Human Summary (A3),3.5,"How many different discounts does Kneser-Ney use for n-grams with counts of 1,2,3 or more?",three
Human Summary (A3),3.6,Efficient language model toolkits use what to build probability tables in a minimal number of passes through a large corpus?,merge sorts
Human Summary (A3),3.6,What do efficient language model toolkits use to combine probabilities and backoffs in a single value?,sorted arrays
Human Summary (A3),3.7,What do we use to measure information?,entropy
Human Summary (A3),3.7,What is the entropy of a sequence divided by the number of words?,Entropy rate
Human Summary (A3),3.7,What is used when the actual probability distribution that generated some data is unknown?,cross entropy
Human Summary (A3),4.0,What is the task of assigning a label to a text?,Text categorization
Human Summary (A3),4.0,What is the positive or negative experience that the writer expresses towards an object?,Sentiment analysis
Human Summary (A3),4.0,What is another example of Sentiment analysis?,Spam detection
Human Summary (A3),4.0,What is the goal of supervised machine learning?,to learn how to mpa
Human Summary (A3),4.1,The Bayesian classifier makes a simplifying assumption of what?,how features interact
Human Summary (A3),4.1,What does the Bayesian classifier present the text as?,a bag of words
Human Summary (A3),4.1,How do we calculate the most probable class given some document?,by choosing the class which has the highest product of the prior probability of the class and the likelihood of the document
Human Summary (A3),4.1,What is the naive Bayes assumption?,conditional independence assumption
Human Summary (A3),4.1,What are the Bayesian classifiers called?,linear classifiers
Human Summary (A3),4.2,What do we use in the data to estimate the maximum likelihood?,frequencies
Human Summary (A3),4.2,What is used to avoid zeros?,Laplace smoothing
Human Summary (A3),4.2,What is the solution for unknown words?,ignore them and remove them from the document
Human Summary (A3),4.2,Some systems decide to ignore a class of what?,very frequent words
Human Summary (A3),4.4,What is the variant of binary NB called?,binary multinomial naive Bayes
Human Summary (A3),4.4,What can modify a negative word and produce a positive review?,Negation
Human Summary (A3),4.4,What is an example of a sentiment lexicon?,MPQA Subjectivity Lexicon
Human Summary (A3),4.4,What are lists of words that are pre annotated with a positive or negative sentiment?,sentiment lexicons
Human Summary (A3),4.5,What does Bayes not require to use all the words in training data as a feature?,the classifier
Human Summary (A3),4.5,Language iD systems are trained on multilingual texts such as what?,Wikipedia
Human Summary (A3),4.6,What can be viewed as a set of class-specific unigram language models?,naive Bayes model
Human Summary (A3),4.6,A naive Bayes model can be viewed as a set of what?,class-specific unigram language models
Human Summary (A3),4.7.0,What does a human define?,Gold labels
Human Summary (A3),4.7.0,What helps visualize how an algorithm performs with respect to the human gold labels?,confusion matrix
Human Summary (A3),4.7.0,"Instead of measuring accuracy, we measure what?",precision
Human Summary (A3),4.7.0,What measures the percentage of items actually present in the input that were correctly identified by the system?,Recall
Human Summary (A3),4.7.1,What is a multi-class classification algorithm?,naive Bayes algorithm
Human Summary (A3),4.7.1,What method computes the performance of each class and averages over classes?,macroaveraging
Human Summary (A3),4.7.1,In what class classification algorithm do we collect the decisions for all classes into a single confusion matrix?,microaveraging
Human Summary (A3),4.8,What allows you to randomly select a training set and a test set division of our data?,k fold cross validation
Human Summary (A3),4.8,How many times does k fold cross validation repeat?,k times
Human Summary (A3),4.9.0,What represents if a model is better than another model?,The effect size
Human Summary (A3),4.9.0,What are the two common non-parametric tests used in NLP?,"approximate randomization, and the bootstrap test"
Human Summary (A3),4.9.1,What test can be applied to any metric?,bootstrap
Human Summary (A3),4.9.1,Bootstrap refers to repeatedly drawing what with replacement?,large numbers of smaller samples
Human Summary (A3),4.10,What are representational harms caused by?,systems that demeans a social group
Human Summary (A3),4.10,What are some tasks that include toxicity detection?,"hate speech, abuse, harassment, or other kinds of toxic language"
Human Summary (A3),5.0,What can help discover the link between features and some outcome?,Logistic regression
Human Summary (A3),5.0,What is the algorithm used for classification in nlp?,Logistics regression
Human Summary (A3),5.0,How many classes can logistic regression classify?,2
Human Summary (A3),5.0,What type of classifier is naive Bayes?,generative
Human Summary (A3),5.0,What does a discriminative model try to directly compute?,P(c/d)
Human Summary (A3),5.0,How many components does a machine learning system have?,four
Human Summary (A3),5.0,What are the two phases of logistic regression?,training and testing
Human Summary (A3),5.1,What classifier can help make a binary decision about the class of a new input observation?,sigmoid
Auto Summary,2.0,What is the name of the early natural language processing system that could carry on a limited conversation with a user by imitating the responses of ,ELIZA
Auto Summary,2.0,ELIZA could carry on a limited conversation with a user by imitating the responses of what?,Rogerian psychotherapist
Auto Summary,2.0,Who's mimicry of human conversation was remarkably successful?,Eliza
Auto Summary,2.0,What do modern conversational agents rely on?,more sophisticated understanding of the user's intent
Auto Summary,2.0,English words are separated from each other by what?,whitespace
Auto Summary,2.0,What is an example of a hashtag?,#nlproc
Auto Summary,2.0,What language doesn't have spaces between words?,Japanese
Auto Summary,2.1.0,What is an algebraic notation for characterizing a set of strings?,regular expression
Auto Summary,2.1.0,What are two examples of regular expressions?,Unix tools grep or Emacs
Auto Summary,2.1.0,What types of regular expressions do regular expressions come in?,variants
Auto Summary,2.1.1,"To search for woodchuck, we type what?",/woodchuck/
Auto Summary,2.1.1,What does the expression /Buttercup/ match?,any string containing the substring Buttercup
Auto Summary,2.1.1,What can the search string consist of?,a single character
Auto Summary,2.1.1,The pattern /[wW]/ matches patterns containing what two characters?,w or W
Auto Summary,2.1.1,What do square brackets not allow us to say?,s or nothing
Auto Summary,2.1.1,What does the question mark /?/ mean?,the preceding character or nothing
Auto Summary,2.1.1,What does the question mark /?/ mean?,the preceding character or nothing
Auto Summary,2.1.1,"What means ""any string of zero or more as""?",/a*/
Auto Summary,2.1.1,"What means ""any string of zero or more as""?",/a*/
Auto Summary,2.1.1,What can we extend /[0-9//. for a single digit?,multiple digits
Auto Summary,2.1.1,"For specifying multiple digits, we can extend /[0-9//. for what?",single digit
Auto Summary,2.1.1,"What are two ways to specify ""at least one"" of some character?",/baaa*!/ or /baa+!/
Auto Summary,2.1.1,"How many ways to specify ""at least one"" of some character?",two
Auto Summary,2.1.1,"What pattern matches the word ""The"" only at the start of a line?",The
Auto Summary,2.1.2,What is another name for the disjunction operator?,pipe symbol
Auto Summary,2.1.2,When do we need to use the disjunction operator?,in the midst of a larger sequence
Auto Summary,2.1.2,What do I want to search for for my cousin David?,pet fish
Auto Summary,2.1.2,What will the expression /Column [0-9] */ match?,a single column followed by a number of spaces
Auto Summary,2.1.2,The idea that one operator may take precedence over another is formalized by what for regular expressions?,operator precedence hierarchy
Auto Summary,2.1.2,What is a Kleene star that matches as little text as possible?,*
Auto Summary,2.1.2,What is the operator *??,Kleene star
Auto Summary,2.1.3,What is an example of an incorrect pattern?,/the/
Auto Summary,2.1.3,What happens when a pattern begins a sentence?,miss the word
Auto Summary,2.1.3,What is the pattern that will miss the word when it begins a sentence?,/[tT]he/
Auto Summary,2.1.3,What do we want instances with on both sides?,word boundary
Auto Summary,2.1.3,What does /b/ not treat as word boundaries?,underscores and numbers
Auto Summary,2.1.3,What are two kinds of errors?,false positives and false negatives
Auto Summary,2.1.3,Reducing error rate involves increasing precision (minimizing false positives) and what?,increasing recall
Auto Summary,2.1.3,When will we come back to precision and recall with more precise definitions?,Chapter 4
Auto Summary,2.1.4,Where can a range of numbers be specified by enclosing them?,curly brackets
Auto Summary,2.1.4,Where can a range of numbers be specified by enclosing them?,curly brackets
Auto Summary,2.1.4,What are referred to by special notation based on the backslash?,special characters
Auto Summary,2.1.5,What is a simple regular expression for a dollar sign followed by a string of digits?,/$[0-9+/
Auto Summary,2.1.5,What character has a different function here than the end-of-line function?,$
Auto Summary,2.1.5,Let's try out a more significant example of the power of what?,REs
Auto Summary,2.1.5,What do we need to allow for?,optional fractions
Auto Summary,2.1.5,What does this pattern only allow?,$199.99
Auto Summary,2.1.5,What do we need to do to allow for optional fractions?,limit the dollars
Auto Summary,2.1.5,What is the limit to the dollars?,*(GB|[Gg]igabytes
Auto Summary,2.1.5,What is the limit of dollars?,b/
Auto Summary,2.1.6,What is an important use of regular expressions?,substitutions
Auto Summary,2.1.6,What is the substitution operator used in Python and in Unix commands like vim or sed?,s/regexp1/pattern/
Auto Summary,2.1.6,What is the substitution operator s/regexp1/pattern/ often useful to be able to refer to?,a particular subpart of the string matching the first pattern
Auto Summary,2.1.6,What simulates a Rogerian psychologist?,ELIZA
Auto Summary,2.1.6,Substitutions and what are very useful in implementing simple chatbots?,capture groups
Auto Summary,2.1.6,What do the first substitutions do?,"change all instances of MY to YOUR, and I'M to YOU ARE"
Auto Summary,2.1.6,What does the next set of substitutions do in the input?,matches and replaces other patterns
Auto Summary,2.1.7,What is commonly used when we are parsing complex patterns but want to rule out a special case?,Negative lookahead
Auto Summary,2.1.7,What is the operator true if pattern occurs?,pattern
Auto Summary,2.1.7,"The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. what doesn't advance",match pointer
Auto Summary,2.2,What is a million-word collection of samples from 500 written English texts from different genres called?,The Brown corpus
Auto Summary,2.2,What is critical for finding boundaries of things?,Punctuation
Auto Summary,2.2,"For some tasks, like part-of-speech tagging or parsing or speech synthesis, we treat punctuation marks as",separate words
Auto Summary,2.2,What are words like uh and um called fillers?,filled pauses
Auto Summary,2.2,What is helpful in speech recognition in predicting the upcoming word?,Disfluencies
Auto Summary,2.2,What is a useful feature for part-of-speech or named-entity tagging?,capitalization
Auto Summary,2.2,"For morphologically complex languages like Arabic, we often need to deal with what?",lemmatization
Auto Summary,2.2,"The more corpora we look at, the more word types we find.",larger
Auto Summary,2.2,What is the relationship between word types in the Brown corpus called?,Herdan's Law or Heaps' Law
Auto Summary,2.2,How many entries were in the 1989 edition of the Oxford English Dictionary?,"615,000"
Auto Summary,2.2,What is sufficient for many tasks in English?,wordforms
Auto Summary,2.3,When are NLP algorithms most useful?,when they apply across many languages
Auto Summary,2.3,How many languages does the world have?,7097
Auto Summary,2.3,"When developing computational models for language processing from a corpus, it's important to consider what?","who produced the language, in what context, for what purpose"
Auto Summary,2.3,Text also reflects what of the writer?,demographic characteristics
Auto Summary,2.3,How does a corpus of texts from different historical periods change?,Language changes over time
Auto Summary,2.4.0,"Before almost any natural language processing of a text, the text has to be what?",normalized
Auto Summary,2.4.0,What is the term for segmenting words?,Tokenizing
Auto Summary,2.4.0,How do we go through each of these tasks?,walk through
Auto Summary,2.4.1,"tr, sort, uniq, sort and uniq are commands from what operating system?",Unix
Auto Summary,2.4.1,What is the name of the textfile that contains the complete words of Shakespeare?,sh.txt
Auto Summary,2.4.1,"What is an easy, if somewhat naive version of word?",tokenization
Auto Summary,2.4.1,What can Unix tools be useful for?,building quick word count statistics
Auto Summary,2.4.1,What can we use to tokenize the words by changing every sequence of non-alphabetic characters to a newline?,tr
Auto Summary,2.4.1,What are the most frequent words in Shakespeare?,"articles, pronouns, prepositions"
Auto Summary,2.4.2,What do most NLP applications need to keep?,"punctuation, numbers and punctuation"
Auto Summary,2.4.2,What is a useful piece of information for parsers?,break off punctuation
Auto Summary,2.4.2,What are clitic contractions marked by?,apostrophes
Auto Summary,2.4.2,What is a common tokenization standard?,Penn Treebank tokenization
Auto Summary,2.4.2,What are deterministic algorithms compiled into?,finite state automata
Auto Summary,2.4.2,How many characters long are words on average?,2.4
Auto Summary,2.4.2,What is a single unit of meaning called in Chinese?,a morpheme
Auto Summary,2.4.2,For most Chinese NLP tasks it works better to take what as input?,characters
Auto Summary,2.4.2,What NLP algorithms require algorithms for word segmentation?,Japanese and Thai
Auto Summary,2.4.3,What is another name for a corpus that NLP algorithms learn from?,training corpus
Auto Summary,2.4.3,What can be arbitrary substrings?,Subwords
Auto Summary,2.4.3,What is an important problem in language processing?,unknown words
Auto Summary,2.4.3,What is the name of the algorithm used by Schuster and Nakajima in 2012?,WordPiece
Auto Summary,2.4.3,What does the BPE token learner begin with?,a vocabulary
Auto Summary,2.4.3,What is the new merged symbol added to the vocabulary?,AB
Auto Summary,2.4.3,What is the most frequent pair of adjacent symbols?,e r
Auto Summary,2.4.3,What runs on the test data the merges we have learned from the training data?,The token parser
Auto Summary,2.4.4,What is the task of putting words/tokens in a standard format?,Word normalization
Auto Summary,2.4.4,Mapping everything to lower case means what are represented identically?,Woodchuck and woodchuck
Auto Summary,2.4.4,What is generally not done for sentiment analysis analysis and machine translation?,case folding
Auto Summary,2.4.4,What is the study of the way words are built up from smaller meaning-bearing units called morphemes?,Morphology
Auto Summary,2.4.4,What is the most sophisticated method for lemmatization?,complete morphological parsing
Auto Summary,2.4.4,What is the Porter stemmer algorithm based on?,"series of rewrite rules run in series, as a cascade"
Auto Summary,2.4.5,What is another important step in text processing?,Sentence segmentation
Auto Summary,2.4.5,What is one of the most useful cues for segmenting a text into sentences?,punctuation
Auto Summary,2.4.5,Periods are what type of cues for segmenting a text into sentences?,more ambiguous
Auto Summary,2.4.5,What can be addressed jointly?,sentence tokenization and word tokenization
Auto Summary,2.5.0,What is much of natural language processing concerned with?,measuring how similar two strings are
Auto Summary,2.5.0,What is the minimum edit distance?,minimum number of editing operations needed to transform one string into another
Auto Summary,2.5.0,What is the most important visualization for string distances?,an alignment between the two strings
Auto Summary,2.5.0,What is a correspondence between substrings of the two sequences?,an alignment
Auto Summary,2.5.0,What represents an operation list for converting the top string into the bottom string?,a series of symbols
Auto Summary,2.5.0,What can we assign to each of these operations?,a particular cost or weight
Auto Summary,2.5.1,When was dynamic programming first introduced?,1957
Auto Summary,2.5.1,What is the name for a class of algorithms that apply a table-driven method to solve problems by combining solutions to sub-problems?,Dynamic programming
Auto Summary,2.5.1,Some of the most commonly used algorithms in natural language processing make use of what?,dynamic programming
Auto Summary,2.5.1,Who named the minimum edit distance algorithm?,Wagner and Fischer
Auto Summary,2.5.1,Where is the minimum edit distance algorithm summarized?,Fig.2.17
Auto Summary,2.5.1,What is the version of the minimum edit distance algorithm?,Levenshtein
Auto Summary,2.5.1,What is useful throughout speech and language processing?,Aligning two strings
Auto Summary,2.5.1,What is used to compute the word error rate?,minimum edit distance alignment
Auto Summary,2.5.1,What plays a role in machine translation?,Alignment
Auto Summary,2.5.1,What exercise asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment?,Exercise 2.7
Auto Summary,2.5.1,What does the backpointer from a cell point to?,previous cell (or cells)
Auto Summary,2.5.1,Some cells have what because the minimum extension could have come from multiple previous cells?,multiple backpointers
Auto Summary,3.0,What is essential in any task in which we have to identify words in noisy input?,Predictions
Auto Summary,3.0,Assigning probabilities to sequences of words is essential in what?,machine translation
Auto Summary,3.0,"What writing tools need to find and correct errors in writing like ""Their are two midterms""?",spelling correction or grammatical error correction
Auto Summary,3.1,What is the probability of a word w given some history h?,P(w|h)
Auto Summary,3.1,"With a large enough corpus, such as the web, we can compute these counts and estimate the probability from Eq. what?",2.2
Auto Summary,3.1,What isn't big enough to give us good estimates in most cases?,web
Auto Summary,3.1,We'll represent a sequence of N words as what?,w1...wn or w1:n
Auto Summary,3.1,What is the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past?,Markov models
Auto Summary,3.1,What model is used to predict the conditional probability of the next word?,bigram model
Auto Summary,3.1,The bigram model can be generalized to what?,trigram
Auto Summary,3.1,How does Eq.11 estimate the n-gram probability?,by dividing the observed frequency of a particular sequence by the frequency of a prefix
Auto Summary,3.1,How many times does the word Chinese occur in a corpus of a million words?,400
Auto Summary,3.1,What does the parameter set maximize in MLE?,the likelihood of the training set T
Auto Summary,3.1,"What is the name of the dialogue system that answered questions about a database of restaurants in Berkeley, California?",Berkeley Restaurant Project
Auto Summary,3.1,What shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project?,Figure 3.1
Auto Summary,3.1,What are the majority of the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project?,zero
Auto Summary,3.1,How many words would be more sparse than a random set of?,seven
Auto Summary,3.1,What encode some facts that we think of as strictly syntactic in nature?,Bigram probabilities
Auto Summary,3.1,"Bigram probabilities encode some facts that we think of as strictly syntactic in nature. In practice, it's more common to use what",trigram models
Auto Summary,3.1,How many pseudo-words are used to compute trigram probabilities at the very beginning of a sentence?,two
Auto Summary,3.2.0,What is the best way to evaluate the performance of a language model?,measure how much it improves
Auto Summary,3.2.0,What is the best way to evaluate the performance of a language model?,extrinsic evaluation
Auto Summary,3.2.0,What are the probabilities of an n-gram model trained on?,the training set or test corpus
Auto Summary,3.2.0,How can we measure the quality of an n-gram model?,performance on some unseen data
Auto Summary,3.2.0,What can be taken from a continuous sequence of text inside the corpus?,test data
Auto Summary,3.2.0,Why do we want our test set to be as large as possible?,small test set may be accidentally unrepresentative
Auto Summary,3.2.1,What is the perplexity of a language model on a test set?,the inverse probability of the test set
Auto Summary,3.2.1,What can we use to expand the probability of W?,the chain rule
Auto Summary,3.2.1,What is the number of possible next words that can follow any word?,The branching factor
Auto Summary,3.2.1,What are the digits in English?,"zero, one, two..., nine"
Auto Summary,3.2.1,What is the perplexity of a mini-language?,10
Auto Summary,3.2.1,Perplexity is closely related to the information-theoretic notion of what?,entropy
Auto Summary,3.2.1,"The lower the perplexity, the more information the n-gram gives us about what?",the word sequence
Auto Summary,3.2.1,The perplexity of two language models is only comparable if they use what?,identical vocabularies
Auto Summary,3.3.0,The n-gram model is dependent on what?,training corpus
Auto Summary,3.3.0,"The longer the context on which we train the model, the more coherent what?",sentences
Auto Summary,3.3.0,What random sentences can we generate sentences from?,"bigrams, trigrams, and 4-grams"
Auto Summary,3.3.0,What sentences have no coherent relation between words or any sentence-final punctuation?,unigram sentences
Auto Summary,3.3.0,What sentences have some local word-to-word coherence?,bigram sentences
Auto Summary,3.3.0,What type of sentences are beginning to look a lot like Shakespeare?,4-gram
Auto Summary,3.3.0,"The words ""It cannot be but so"" are directly from what?",King John
Auto Summary,3.3.0,What should a training corpus have?,similar genre
Auto Summary,3.3.0,What is important to get training data in?,appropriate dialect or variety
Auto Summary,3.3.0,"For any n-gram that occurred a sufficient number of times, we might have what?",a good estimate of its probability
Auto Summary,3.3.0,What language is bound to be missing from a training corpus?,English
Auto Summary,3.3.0,What is still not sufficient?,Matching genres and dialects
Auto Summary,3.3.0,Our models may still be subject to the problem of what?,sparsity
Auto Summary,3.3.1,What is zero for words that we haven't seen before?,bigram probability
Auto Summary,3.3.1,What is the problem of words whose bigram probability is zero?,words we simply have never seen before
Auto Summary,3.3.1,What do we know in a language task?,all the words that can occur
Auto Summary,3.3.1,What are OOV words?,out of vocabulary
Auto Summary,3.3.1,What can a language model achieve by choosing a small vocabulary and assigning the unknown word a high probability?,low perplexity
Auto Summary,3.3.1,The exact choice of UNK> model does have an effect on metrics like what?,perplexity
Auto Summary,3.3.1,What should only be compared across language models with the same vocabularies?,perplexities
Auto Summary,3.4.0,What context do words that are in our vocabulary appear in a test set in?,unseen
Auto Summary,3.4.0,What do we do to keep a language model from assigning zero probability to unseen events?,shave off a bit of probability mass
Auto Summary,3.4.0,What is a modification called to keep a language model from assigning zero probability to unseen events?,smoothing or discounting
Auto Summary,3.4.1,What does not perform well enough to be used in modern n-gram models?,Laplace smoothing
Auto Summary,3.4.1,What is the algorithm that introduces many concepts that we see in other smoothing algorithms called?,add-one smoothing
Auto Summary,3.4.1,How many counts does add-one smoothing add to each count?,one
Auto Summary,3.4.1,How can we turn c* into a probability P*?,normalizing by N
Auto Summary,3.4.1,What algorithm has made a very big change to the counts?,Add-one smoothing
Auto Summary,3.4.1,C(want to) changed from what to 238?,609
Auto Summary,3.4.1,What decreases from.66 in the unsmoothed case to.26 in the smoothed case?,P(to|want)
Auto Summary,3.4.2,"Instead of adding I to each count, we add what?",fractional count k
Auto Summary,3.4.2,What is the algorithm that adds fractional count k called?,add-k smoothing
Auto Summary,3.4.2,Add-k smoothing doesn't work well for what?,language modeling
Auto Summary,3.4.3,What can help solve the problem of zero frequency n-grams?,Discounting
Auto Summary,3.4.3,What can we draw on to solve the problem of zero frequency n-grams?,additional source of knowledge
Auto Summary,3.4.3,What can we use to estimate the probability of a particular trigram?,bigram probability
Auto Summary,3.4.3,"If we don't have counts to compute the bigrams probability, what can we look to?",unigrams
Auto Summary,3.4.3,How do we estimate the trigram probability?,"by mixing together the unigram, bigram, and trigram probabilities, each weighted by a lambdas"
Auto Summary,3.4.3,How is each weight computed in a slightly more sophisticated version of linear interpolation?,by conditioning on the context
Auto Summary,3.4.3,What shows the equation for interpolation with context-conditioned weights?,Equation 3.28
Auto Summary,3.4.3,How do we approximate a backoff n-gram model?,by backing off to the (N-1)-gram
Auto Summary,3.4.3,In what model do we rely on a discounted probability P* if we've seen this n-ram before?,Katz backoff
Auto Summary,3.5,What method does Kneser-Ney have roots in?,absolute discounting
Auto Summary,3.5,Why is it necessary to save some probability mass for the smoothing algorithm?,to distribute to unseen n-grams
Auto Summary,3.5,Who used a clever idea to see how to save some probability mass for the smoothing algorithm to distribute to unseen n-grams?,Church and Gale
Auto Summary,3.5,What formalizes this intuition by subtracting a fixed (absolute) discount d from each count?,Absolute discounting
Auto Summary,3.5,What will not affect the very high counts?,a small discount d
Auto Summary,3.5,Kneser-Ney discounting augments absolute discounting with a more sophisticated way to handle what?,lower-order unigram distribution
Auto Summary,3.5,What does the Kneser-Ney intuition base our estimate of P on?,number of different contexts word w has appeared in
Auto Summary,3.5,How many contexts have words appeared in in the past?,more
Auto Summary,3.5,How many contexts does a frequent word (Kong) occur in?,one
Auto Summary,3.5,What is the final equation for Interpolated Kneser-Ney smoothing for bigrams?,3.35
Auto Summary,3.5,What is the best performing version of smoothing called?,modified Kneer Ney
Auto Summary,3.5,"How many different discounts does modified Kneer Ney use for n-grams with counts of 1, 2 and 3 or more?",three
Auto Summary,3.6,What does the Web 1 Trillion 5-gram corpus include?,n-grams
Auto Summary,3.6,What corpora includes n-rams drawn from their book collections?,Google Books Ngrams
Auto Summary,3.6,How many words are in the Google Books Ngrams corpus?,"1,024,908,267,229"
Auto Summary,3.6,What do language model toolkits use to combine probabilities and backoffs in a single value?,sorted arrays
Auto Summary,3.6,What algorithm does not produce a probability distribution?,stupid backoff
Auto Summary,3.6,What does stupid backoff not produce?,probability distribution
Auto Summary,3.6,Where does the stupid backoff end?,unigram
Auto Summary,3.7,What is a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme?,entropy
Auto Summary,3.7,A better n-gram model assigns what to the test data?,higher probability
Auto Summary,3.7,What gives us a lower bound on the number of bits?,X ranges over horses
Auto Summary,3.7,"A code that averages what bit per race can be built with short encodings for more probable horses, and longer encodesings for less",2 bits
Auto Summary,3.7,What is the entropy of the choice of horses?,3 bits
Auto Summary,3.7,"To measure the true entropy of a language, we need to consider what?",sequences of infinite length
Auto Summary,3.7,"What theorem states that if the language is regular in certain ways (to be exact, if it is both stationary and ergo",Shannon-McMillan-Breiman theorem
Auto Summary,3.7,What can be dependent on events that were arbitrarily distant and time dependent?,the probability of upcoming words
Auto Summary,3.7,When is the cross-entropy useful?,when we don't know the actual probability distribution
Auto Summary,3.7,"The cross-entropy allows us to use some m, which is a model of what?",p
Auto Summary,4.0,What lies at the heart of both human and machine intelligence?,Classification
Auto Summary,4.0,What is applied to text categorization?,The Bayes algorithm
Auto Summary,4.0,What is the extraction of sentiment?,sentiment analysis
Auto Summary,4.0,What is the first step in most language processing pipelines?,language id
Auto Summary,4.0,What is another important commercial application?,Spam detection
Auto Summary,4.0,"Related text classification tasks like what are relevant to the digital humanities, social sciences, and forensic linguistics?",authorship attribution
Auto Summary,4.0,"What classifies each occurrence of a word in a sentence as, e.g., a noun or a verb?",A part-of-speech tagger
Auto Summary,4.0,What is the goal of classification?,"to take a single observation, extract some useful features, and classify the observation into one of a set of discrete classes"
Auto Summary,4.0,What is classification essential for?,tasks below the level of the document
Auto Summary,4.0,What are the output classes of supervised classification?,"y1,y2...,yM"
Auto Summary,4.0,What is the output variable for lassification?,c
Auto Summary,4.0,What is the training set of in supervised classification?,N documents
Auto Summary,4.1,What classifier is introduced in this section?,multinomial naive Bayes
Auto Summary,4.1,What is the multinomial naive Bayes classifier?,a Bayesian classifier
Auto Summary,4.1,Where is the intuition of the naive Bayes classifier shown?,Fig. 4.1
Auto Summary,4.1,Where is the intuition of the naive Bayes classifier shown?,Fig. 4.1
Auto Summary,4.1,What does the classifier represent a text document as if it were?,a bag-of-words
Auto Summary,4.1,What type of model is naive Bayes?,generative model
Auto Summary,4.1,What does the naive Bayes classifier state about how a document is generated?,implicit assumption
Auto Summary,4.1,In what chapter will we talk about generative models?,Chapter 5
Auto Summary,4.1,What could we imagine by following this process?,generating artificial documents
Auto Summary,4.1,Why are naive Bayes calculations done in log space?,increase speed
Auto Summary,4.1,How do we apply the naive Bayes classifier to text?,by simply walking an index through every word position in the document
Auto Summary,4.1,What computes the predicted class as a linear function of input features?,Eq.10
Auto Summary,4.2,What probability do we assume a feature is just the existence of a word in the document's bag of words?,P(fi|c
Auto Summary,4.2,"What is P(w, c) computed as?",the fraction of times the word wi appears among all words in all documents of topic c
Auto Summary,4.2,What is the simplest solution introduced in Chapter 3?,add-one smoothing
Auto Summary,4.2,What smoothing algorithm is usually replaced by more sophisticated smoothing algorithms?,Laplace
Auto Summary,4.2,Laplace smoothing is commonly used in what?,naive Bayes text categorization
Auto Summary,4.4,What do small changes in Bayes text classification do?,improve performance
Auto Summary,4.4,What seems to matter more than its frequency?,whether a word occurs or not
Auto Summary,4.4,What can modify a negative word to produce a positive review?,negation
Auto Summary,4.4,What can modify a negative word to produce a positive review?,Negation
Auto Summary,4.4,What might cause us to train accurate Bayes classifiers using all words in the training set to estimate positive and negative sentiment?,insufficient labeled training data
Auto Summary,4.4,What can we derive from sentiment lexicons?,positive and negative word features
Auto Summary,4.4,What are the four popular lexicons?,"General Inquirer, LIWC, the opinion lexicon of Hu and Liu and the MPQA Subjectivity Lexicon"
Auto Summary,4.5,SpamAssassin predefines features like what?,one hundred percent guaranteed
Auto Summary,4.5,What can features in naive Bayes express?,any property of the input text
Auto Summary,4.5,What are the most effective Bayes features?,"character n-grams, or byte n-rams"
Auto Summary,4.5,"Language ID systems are trained on multilingual text, such as what?",Wikipedia
Auto Summary,4.5,What language has a low ratio of text to image area?,HTML
Auto Summary,4.6,What can be viewed as a set of class-specific unigram language models?,naive Bayes Bayes model
Auto Summary,4.6,A naive Bayes Bayes model for each class instantiates what?,language model
Auto Summary,4.6,What do likelihood features assign to each word P(word|c)?,probability
Auto Summary,4.7.0,What is a table for visualizing how an algorithm performs with respect to the human gold labels?,confusion matrix
Auto Summary,4.7.0,What is the goal of spam detection?,"to label every text as being in the spam category (""positive"") or not in the spam category (""negative"")"
Auto Summary,4.7.0,How many false negatives would a simple classifier have?,100
Auto Summary,4.7.0,What is the goal of accuracy?,to discover something that is rare
Auto Summary,4.7.0,What are the two metric we use instead of accuracy?,precision and recall
Auto Summary,4.7.0,What measures the percentage of items actually present in input that were correctly identified by the system?,Recall
Auto Summary,4.7.0,What do precision and recall emphasize?,true positives
Auto Summary,4.7.1,What is already a multi-class classification algorithm?,Bayes algorithm
Auto Summary,4.7.1,What are the three classes for sentiment analysis?,"positive, negative, neutral"
Auto Summary,4.7.1,What is the name of the process that collects decisions for all classes into a single confusion matrix?,microaveraging
Auto Summary,4.8,What is the training and testing procedure for text classification similar to?,language modeling
Auto Summary,4.8,What do we use to train the model?,the training set
Auto Summary,4.8,"Why is there a problem with a fixed training set, devset, and test set?",the test set might not be large enough to be representative
Auto Summary,4.9.0,What do we often need to do in building systems?,compare the performance of two systems
Auto Summary,4.9.0,How can we know if the new system we just built is better or worse than the old one?,better
Auto Summary,4.9.0,Where can we know if the new system we just built is better than the old one?,the literature
Auto Summary,4.9.0,What is the domain of comparing the performance of two systems?,statistical hypothesis testing
Auto Summary,4.9.0,In this section we introduce tests for statistical significance for what?,NLP classifiers
Auto Summary,4.9.0,How do we test statistical significance for NLP classifiers?,by formalizing two hypotheses
Auto Summary,4.9.0,What does the hypothesis H0 suppose?,d(x) is actually negative or zero
Auto Summary,4.9.0,What do we want to know about A's superiority over B?,if A's superiority over B is likely to hold again if we checked another test set x'
Auto Summary,4.9.0,"What is the probability, assuming the null hypothesis H0 is true, of seeing the d(X) that we saw?",p-value
Auto Summary,4.9.0,What are two examples of simple parametric tests in NLP?,t-tests or ANOVAs
Auto Summary,4.9.0,If the d we saw has a probability that is what?,below the threshold
Auto Summary,4.9.0,What type of tests do we artificially create many versions of the experimental setup?,non-parametric tests based on sampling
Auto Summary,4.9.1,What can apply to any metric?,bootstrap test
Auto Summary,4.9.1,What are large numbers of smaller samples with replacement called?,bootstrap samples
Auto Summary,4.9.1,What is the test set for a tiny text classification example?,x of 10 documents
Auto Summary,4.9.1,How many times do we repeatedly select a cell from row x with replacement?,n=10 times
Auto Summary,4.9.1,What is the large num of virtual test sets x?,b
Auto Summary,4.9.1,What can we do statistics on now that we have the b test sets?,how often A has an accidental advantage
Auto Summary,4.9.1,Where is the full algorithm for the bootstrap shown?,Fig.9
Auto Summary,4.9.1,What does the percentage of b bootstrap test sets in which d(x*(i) ) > 2d(x) act as,p-value
Auto Summary,4.10,What is one class of harms caused by a system that perpetuates negative stereotypes about a social group?,demeans
Auto Summary,4.10,Classifiers can lead to both representational harms and what other harm?,censorship
Auto Summary,4.10,What do some widely used toxicity classifiers incorrectly flag as being?,toxic sentences
Auto Summary,4.10,What could lead to the censoring of discourse by or about groups?,false positive errors
Auto Summary,4.10,What can cause model problems?,biases or other problems in the training data
Original Text,2.0,ELIZA could carry on a limited conversation with a user by imitating the responses of what?,Rogerian psychotherapist
Original Text,2.0,What is the output of ELIZA?,"""What would it mean to you if you got X?"""
Original Text,2.0,What program doesn't need to know anything to mimic a Rogerian psychotherapist?,ELIZA
Original Text,2.0,When did Weizenbaum write about ELIZA?,1976
Original Text,2.0,What do modern conversational agents rely on?,more sophisticated understanding of the user's intent
Original Text,2.0,What kind of methods did ELIZA use?,simple pattern-based methods
Original Text,2.0,What is the most important tool for describing text patterns?,regular expression
Original Text,2.0,What can be used to specify strings we might want to extract from a document?,Regular expressions
Original Text,2.0,What is a set of tasks collectively called?,text normalization
Original Text,2.0,"What means converting text to a more convenient, standard form?",Normalizing
Original Text,2.0,What is the task of normalizing text?,tokenization
Original Text,2.0,English words are often separated from each other by what?,whitespace
Original Text,2.0,What are sometimes treated as large words despite the fact that they contain spaces?,New York and rock 'n' roll
Original Text,2.0,What is a hashtag that we'll need to tokenize for processing tweets?,#nlproc
Original Text,2.0,What language doesn't have spaces between words?,Japanese
Original Text,2.0,What is the task of determining that two words have the same root?,lemmatization
Original Text,2.0,What is the common lemma of the words sing?,sing
Original Text,2.0,"What is the common lemma of the words sang, sung, and sings?",sing
Original Text,2.0,What is essential for processing morphologically complex languages like Arabic?,Lemmatization
Original Text,2.0,What refers to a simpler version of lemmatization in which we mainly strip suffixes from the end of the word?,Stemming
Original Text,2.0,Text normalization breaks up a text into individual what?,sentences
Original Text,2.0,What will we need to do with words and other strings?,compare
Original Text,2.0,What metric measures how similar two strings are based on the number of edits?,edit distance
Original Text,2.0,What is an algorithm with applications throughout language processing?,Edit distance
Original Text,2.1.0,What language is used for specifying text search strings?,regular expression
Original Text,2.1.0,What are some of the tools that use the regular expression?,Unix tools grep or Emacs
Original Text,2.1.0,What is a regular expression?,algebraic notation
Original Text,2.1.0,Regular expressions are particularly useful for searching in texts when we have a pattern to search for and what to search through?,corpus of texts
Original Text,2.1.0,"What will search through the corpus, returning all texts that match the pattern?",A regular expression search function
Original Text,2.1.0,What types of corpus can a regular expression search function search through?,a single document or a collection
Original Text,2.1.0,What Unix command-line tool takes a regular expression and returns every line of the input document that matches the expression?,grep
Original Text,2.1.0,What can a regular expression search return if there are more than one match?,every match on a line
Original Text,2.1.0,What do we usually underline in the following examples?,the exact part of the pattern that matches the regular expression
Original Text,2.1.0,What are regular expressions delimited by?,slashes
Original Text,2.1.0,Regular expressions come in many what?,variants
Original Text,2.1.0,What will we be describing?,extended regular expressions
Original Text,2.1.0,What is a handy way to test out your regular expressions?,online regular expression tester
Original Text,2.1.1,What is the simplest kind of regular expression?,a sequence of simple characters
Original Text,2.1.1,"To search for woodchuck, we type what?",/woodchuck/
Original Text,2.1.1,What expression matches any string containing the substring Buttercup?,/Buttercup/
Original Text,2.1.1,What is an example of a sequence of characters in a regular expression?,/urgl/
Original Text,2.1.1,Regular expressions are what?,case sensitive
Original Text,2.1.1,What will the pattern /woodchucks/ not match?,the string Woodchucks
Original Text,2.1.1,What can be used to solve this problem?,square braces
Original Text,2.1.1,What specifies a disjunction of characters to match?,The string of characters inside the braces
Original Text,2.1.1,What shows that the pattern /[wW]/ matches patterns containing either w or W?,Fig. 2.2
Original Text,2.1.1,What does the regular expression /[1234567890]/ specify?,specifies any single digit
Original Text,2.1.1,What classes of characters are important building blocks in expressions?,digits or letters
Original Text,2.1.1,What can be used with the dash (-) to specify any one character in a range?,brackets
Original Text,2.1.1,What does the pattern /[2-5]/ specify?,"any one of the characters 2, 3, 4, or 5"
Original Text,2.1.1,The pattern /[b-g]/ specifies one of which characters?,"b, c, d, e, f, or g"
Original Text,2.1.1,What can be used to specify what a single character cannot be?,square braces
Original Text,2.1.1,What can be used to specify what a single character cannot be?,square braces
Original Text,2.1.1,What is the first symbol after the open square brace?,caret
Original Text,2.1.1,What is the first symbol after the open square brace?,a
Original Text,2.1.1,What is the first symbol after the open square brace?,caret
Original Text,2.1.1,What does the caret stand for if it occurs anywhere else?,a caret
Original Text,2.1.1,What is an optional s in woodchuck and woodchucks?,optional elements
Original Text,2.1.1,What do square brackets not allow us to say?,s or nothing
Original Text,2.1.1,What does the question mark /?/ mean?,the preceding character or nothing
Original Text,2.1.1,"What signifies ""zero or one instances of the previous character""?",question mark
Original Text,2.1.1,What does the question mark mean?,zero or one instances of the previous character
Original Text,2.1.1,What is the question mark a way of specifying?,how many of something that we want
Original Text,2.1.1,What does the language of certain sheep look like?,baa!
Original Text,2.1.1,What does the language of certain sheep look like?,baaaa!
Original Text,2.1.1,What does the language of certain sheep look like?,baa! baaa! baaaa!
Original Text,2.1.1,What is the name of the string in the language of certain sheep?,b
Original Text,2.1.1,"The set of operators that allows us to say things like ""some number of as"" are based on the asterisk or *, commonly called what",Kleene *
Original Text,2.1.1,What does the Kleene star mean?,zero or more occurrences of the immediately previous character or regular expression
Original Text,2.1.1,"What means ""any string of zero or more as""?",/a*/
Original Text,2.1.1,What will /a*/ match?,match a or aaaaa
Original Text,2.1.1,What is the regular expression for matching one or more a's?,/aa*/
Original Text,2.1.1,What can also be repeated?,More complex patterns
Original Text,2.1.1,What does /ab*/ mean?,zero or more right square braces
Original Text,2.1.1,What will this match?,strings like aaaa or ababab or aabb
Original Text,2.1.1,What is the regular expression for a single digit?,/[0-9]/
Original Text,2.1.1,What is an integer?,a string of digits
Original Text,2.1.1,What is the regular expression for a single digit?,/[0-9]*/
Original Text,2.1.1,What is it like to have to write the regular expression for digits twice?,annoying
Original Text,2.1.1,"What means ""one or more occurrences of the immediately preceding character or regular expression""?",Kleene +
Original Text,2.1.1,The expression /[0-9]+/ is the normal way to specify what?,a sequence of digits
Original Text,2.1.1,What are the two ways to specify the sheep language?,/baaa*!/ or /baa+!/
Original Text,2.1.1,What is a wildcard expression that matches any single character except a carriage return?,the period
Original Text,2.1.1,The wildcard is often used together with the Kleene star to mean what?,any string of characters
Original Text,2.1.1,The wildcard is often used together with the Kleene star to mean what?,any string of characters
Original Text,2.1.1,What is an example of a word that appears twice in a line?,aardvark
Original Text,2.1.1,What is the regular expression used to specify this?,/aardvark
Original Text,2.1.1,What are special characters that anchor regular expressions to particular places in a string?,Anchors
Original Text,2.1.1,What are special characters that anchor regular expressions to particular places in a string?,Anchors
Original Text,2.1.1,The caret  matches what?,the start of a line
Original Text,2.1.1,What are the three uses of the caret?,"to match the start of a line, to indicate a negation inside of square brackets, and just to mean a caret"
Original Text,2.1.1,What allows grep or Python to know which function a given caret is supposed to have?,contexts
Original Text,2.1.1,What matches the end of a line?,The dollar sign $
Original Text,2.1.1,What does /.$/ match a line that contains only the phrase The dog?,The dog
Original Text,2.1.1,"What do we have to use in order to use the. to mean ""period"" and not the wildcard?",backslash
Original Text,2.1.1,What do we want the backslash to mean?,period
Original Text,2.1.1,What does /btheb/ match?,the but not the word other
Original Text,2.1.1,"What is a ""word"" for the purposes of a regular expression defined as?","any sequence of digits, underscores, or letters"
Original Text,2.1.1,What string does /b99b/ match?,99 in
Original Text,2.1.1,What does /b99b/ match?,99 in $99
Original Text,2.1.2,What are we particularly interested in when searching for texts about pets?,cats and dogs
Original Text,2.1.2,What would we want to search for in a text about pets?,string cat or the string dog
Original Text,2.1.2,"What can't we use to search for ""cat or dog""?",square brackets
Original Text,2.1.2,What is the pipe symbol?,disjunction operator
Original Text,2.1.2,What pattern matches either the string cat or the string dog?,/cat|dog/
Original Text,2.1.2,What operator does the pattern /cat|dog/ match in the midst of a larger sequence?,disjunction operator
Original Text,2.1.2,What do I want to search for for my cousin David?,pet fish
Original Text,2.1.2,What can I specify when searching for information about pet fish for my cousin David?,guppy and guppies
Original Text,2.1.2,What would match only the strings guppy and ies?,guppy|ies
Original Text,2.1.2,What takes precedence over the disjunction operator?,sequences like guppy
Original Text,2.1.2,What do we need to use to make the disjunction operator apply only to a specific pattern?,parenthesis operators
Original Text,2.1.2,Enclosing a pattern in parentheses makes it act like what for the purposes of neighboring operators?,single character
Original Text,2.1.2,What is the pattern /gupp?,y
Original Text,2.1.2,What operator applies by default only to a single character?,Kleene*
Original Text,2.1.2,What do we want to match with the Kleene* operator?,repeated instances of a string
Original Text,2.1.2,What is the name of the column labels on a line that we want to match repeated instances of a string?,Column 1 Column 2 Column 3
Original Text,2.1.2,What will the expression /Column [0-9] */ match?,a single column followed by any number of spaces
Original Text,2.1.2,The star here applies only to what?,the space that precedes it
Original Text,2.1.2,What expression could be written in parentheses to match the word Column?,/ (Column [0-9]+ *)*/
Original Text,2.1.2,What formalizes the idea that one operator may take precedence over another?,operator precedence hierarchy
Original Text,2.1.2,What is the order of RE operator precedence?,highest precedence to lowest precedence
Original Text,2.1.2,What has a higher precedence than sequences?,counters
Original Text,2.1.2,What has a higher precedence than disjunction?,sequences
Original Text,2.1.2,What can be ambiguous in another way?,Patterns
Original Text,2.1.2,"What expression could match nothing, or just the first letter o, on, one, or once?",/ [a-z]*/
Original Text,2.1.2,What does /[a-z]*/ match?,zero or more letters
Original Text,2.1.2,What do regular expressions say patterns are?,patterns are greedy
Original Text,2.1.2,What qualifier is used to enforce non-greedy matching?,?
Original Text,2.1.2,What is another way to enforce non-greedy matching?,qualifier
Original Text,2.1.2,What is the operator *??,Kleene star
Original Text,2.1.2,What is the operator *??,Kleene star
Original Text,2.1.2,What is the operator +??,Kleene plus
Original Text,2.1.2,What is the operator +??,Kleene plus
Original Text,2.1.3,What English article would we want to find cases of?,the
Original Text,2.1.3,What is a simple pattern for a RE to find cases of the English article the?,/the/
Original Text,2.1.3,What is a problem with the /the/ pattern?,miss the word
Original Text,2.1.3,What is the pattern that will miss the word when it begins a sentence?,/[tT]he/
Original Text,2.1.3,What will we still return texts with?,embedded in other words
Original Text,2.1.3,What does /b/ not treat as word boundaries?,underscores and numbers
Original Text,2.1.3,What do we want?,instances in which there are no alphabetic letters on either side of the the
Original Text,2.1.3,What is the problem with the pattern?,it won't find the word the when it begins a line
Original Text,2.1.3,What implies that there must be some single (although non-alphabetic) character before the the?,the regular expression
Original Text,2.1.3,What are two things we can avoid by specifying that before the the we require?,beginning-of-line or a non-alphabetic character
Original Text,2.1.3,What are strings that we incorrectly matched like other or there?,false positives
Original Text,2.1.3,What two kinds of errors come up again and again in implementing?,speech and language processing systems
Original Text,2.1.3,What is one antagonistic effort to reduce the overall error rate for an application?,increasing precision
Original Text,2.1.3,When will we come back to precision and recall with more precise definitions?,Chapter 4
Original Text,2.1.4,What shows some aliases for common ranges?,Figure 2.8
Original Text,2.1.4,Where can we use explicit numbers as counters?,curly brackets
Original Text,2.1.4,What will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots followed by z)?,/a
Original Text,2.1.4,What can also be specified?,A range of numbers
Original Text,2.1.4,"/n,m/ specifies from what occurrences of the previous char or expression?",n to m
Original Text,2.1.4,What are summarized in Fig. 2.9?,REs
Original Text,2.1.4,Where are REs for counting summarized?,2.9
Original Text,2.1.4,What are referred to by special notation based on the backslash?,special characters
Original Text,2.1.4,Special characters are referred to by special notation based on what?,backslash
Original Text,2.1.4,"To refer to characters that are special themselves, precede them with what?",a backslash
Original Text,2.1.5,Let's try out a more significant example of the power of what?,REs
Original Text,2.1.5,What do we want to help a user buy a computer on the Web?,build an application
Original Text,2.1.5,What would a user want to buy a computer for?,$1000
Original Text,2.1.5,What is the price of a Mac?,$999.99
Original Text,2.1.5,What will we work out in the rest of this section?,simple regular expressions
Original Text,2.1.5,What do we need to complete our regular expression for?,prices
Original Text,2.1.5,What is the regular expression for a dollar sign followed by a string of digits?,/$[0-9]+/
Original Text,2.1.5,What function does the $ character have a different function from?,end-of-line function
Original Text,2.1.5,Most regular expression parsers are smart enough to realize that $ here doesn't mean what?,end-of-line
Original Text,2.1.5,Who might figure out the function of $ from the context?,regex parsers
Original Text,2.1.5,What do we now need to deal with?,fractions of dollars
Original Text,2.1.5,What will we add after the decimal point and two digits?,a decimal point and two digits
Original Text,2.1.5,What is the only price that this pattern allows?,$199.99
Original Text,2.1.5,What do we need to do to make the cents optional?,make the cents optional
Original Text,2.1.5,What would be far too expensive?,$199999.99
Original Text,2.1.5,What would be far too expensive?,$199999.99
Original Text,2.1.5,What do we need to do to limit prices like $199999.99?,limit the dollars
Original Text,2.1.5,What do we need to allow for optional fractions?,disk space
Original Text,2.1.5,How much disk space will we need to allow for optional fractions?,5.5 GB
Original Text,2.1.5,What does / */ mean?,zero or more spaces
Original Text,2.1.5,What does b/ mean?,*(GB|[Gg]igabytes?)
Original Text,2.1.5,What is left as an exercise for the reader?,more than 500 GB
Original Text,2.1.6,What is an important use of regular expressions?,substitutions
Original Text,2.1.6,What substitution operator allows a string characterized by a regular expression to be replaced by another string?,s/colour/color
Original Text,2.1.6,What do we want to put around all integers in a text?,angle brackets
Original Text,2.1.6,What do we want to add to the first pattern?,brackets
Original Text,2.1.6,What is the number operator used in the second pattern?,1
Original Text,2.1.6,What can the parenthesis and number operators specify?,a certain string or expression must occur twice in the text
Original Text,2.1.6,What is the pattern that we want to constrain the two X's to be the same string?,"the Xer they were, the Xer they will be"
Original Text,2.1.6,How do we constrain the two X's to be the same string?,by surrounding the first X with the parenthesis operator
Original Text,2.1.6,"What is the Xer they were, the Xer they will be?",1er they will be
Original Text,2.1.6,What will the 1 be replaced by?,whatever string matched the first item in parentheses
Original Text,2.1.6,What will the first item in parentheses match?,the bigger they were
Original Text,2.1.6,What is the use of parentheses to store a pattern in memory called?,capture group
Original Text,2.1.6,Where is the resulting match stored when a capture group is used?,numbered register
Original Text,2.1.6,What does 2 mean if you match two sets of parentheses?,whatever matched the second capture group
Original Text,2.1.6,What will the 1er we 2/ match?,faster they ran
Original Text,2.1.6,What is the third capture group stored in?,3
Original Text,2.1.6,What is used to group terms for specifying the order in which operators should apply?,Parentheses
Original Text,2.1.6,"What do we sometimes want to use for grouping, but don't want to capture the resulting pattern in a register?",parentheses
Original Text,2.1.6,How is a non-capturing group specified?,putting the commands
Original Text,2.1.6,Where is a non-capturing group specified?,after the open paren
Original Text,2.1.6,What is a non-capturing group specified by putting the commands?,pattern
Original Text,2.1.6,What is a non-capturing group?,people|cats
Original Text,2.1.6,What are very useful in implementing simple chatbots like ELIZA?,Substitutions and capture groups
Original Text,2.1.6,What does ELIZA simulate?,Rogerian psychologist
Original Text,2.1.6,What does ELIZA have a series or cascade of?,regular expression substitutions
Original Text,2.1.6,What are input lines in ELIZA?,first uppercased
Original Text,2.1.6,What do the first substitutions do?,"change all instances of MY to YOUR, and I'M to YOU ARE"
Original Text,2.1.6,What happens to the next set of substitutions in ELIZA?,matches and replaces other patterns in the input
Original Text,2.1.6,"In ELIZA, substitutions are assigned what?",rank
Original Text,2.1.6,Creating patterns is the topic of what exercise?,Exercise 2.3
Original Text,2.1.7,What do we do when we need to predict the future?,look ahead in the text
Original Text,2.1.7,What syntax do lookahead assertions make use of?,(?
Original Text,2.1.7,What do lookahead assertions use the (? syntax for?,non-capture groups
Original Text,2.1.7,When is the operator (?= pattern) true?,if pattern occurs
Original Text,2.1.7,"What happens if the operator (?= pattern) is true if pattern occurs, but is zero-width?",the match pointer doesn't advance
Original Text,2.1.7,What operator returns true if a pattern does not match?,pattern
Original Text,2.1.7,What operator returns true if a pattern does not match?,pattern
Original Text,2.1.7,What is commonly used when we are parsing some complex pattern but want to rule out a special case?,Negative lookahead
Original Text,2.1.7,What word would you want to match at the beginning of a line?,Volcano
Original Text,2.1.7,"What do we use when we want to match, at the beginning of a line, any single word that doesn't start with ""Volcano",negative lookahead
Original Text,2.1.7,"What is a word that doesn't start with ""Volcano""?",!Volcano
Original Text,2.2,"Before we talk about processing words, we need to decide what?",what counts as a word
Original Text,2.2,What is a computer-readable collection of text or speech called?,plural corpora
Original Text,2.2,How many written English texts are in the Brown corpus?,500
Original Text,2.2,When was the Brown corpus assembled?,1963-64
Original Text,2.2,How many words are in the following Brown sentence?,How many words are in the following Brown sentence
Original Text,2.2,What did Brown encounter when he stepped out into the hall?,water brother
Original Text,2.2,How many words does the Brown sentence have if we don't count punctuation marks as words?,13
Original Text,2.2,What is a comma called?,period
Original Text,2.2,"Punctuation is critical for finding boundaries of things (commas, periods, colons) and identifying some aspects of meaning (question",comma
Original Text,2.2,What is critical for finding boundaries of things?,Punctuation
Original Text,2.2,Punctuation marks are sometimes treated as if they were separate words for what tasks?,part-of-speech tagging or parsing or speech synthesis
Original Text,2.2,What corpus of American English telephone conversations between strangers was collected in the early 1990s?,Switchboard
Original Text,2.2,What does the Switchboard corpus of spoken language not have?,punctuation
Original Text,2.2,What is the main part of an utterance?,business data processing
Original Text,2.2,How many kinds of disfluencies does an utterance have?,two
Original Text,2.2,The broken-off word main- is called what?,a fragment
Original Text,2.2,What are fillers called?,filled pauses
Original Text,2.2,Fillers or filled pauses are considered to be what?,words
Original Text,2.2,What determines whether fillers or filled pauses are words?,the application
Original Text,2.2,"If we are building a speech transcription system, we might want to strip out what?",disfluencies
Original Text,2.2,What do we sometimes keep around?,disfluencies
Original Text,2.2,What disfluencies are helpful in speech recognition in predicting the upcoming word?,uh or um
Original Text,2.2,What can be a cue to speaker identification?,different disfluencies
Original Text,2.2,Who showed that uh and um have different meanings?,Clark and Fox Tree
Original Text,2.2,What are uh and um like?,capitalized tokens
Original Text,2.2,What are uh and um like?,capitalized tokens
Original Text,2.2,What are capitalized tokens and uncapitalized tokens lumped together in?,speech recognition
Original Text,2.2,What word has the same lemma cat but are different wordforms?,cats
Original Text,2.2,What do cats and cat have the same?,lemma cat
Original Text,2.2,"What is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense",A lemma
Original Text,2.2,What is the word-form?,full inflected or derived form
Original Text,2.2,"For morphologically complex languages like Arabic, we often need to deal with what?",lemmatization
Original Text,2.2,What is sufficient for many tasks in English?,wordforms
Original Text,2.2,How many words are there in English?,How many words
Original Text,2.2,How many ways of talking about words are there in English?,two
Original Text,2.2,What are tokens?,the total number N of running words
Original Text,2.2,Where did the Browns picnic?,the pool
Original Text,2.2,"When we speak about the number of words in the language, we are generally referring to what?",word types
Original Text,2.2,What shows the rough numbers of types and tokens computed from some popular English corpora?,2.11
Original Text,2.2,What shows the rough numbers of types and tokens computed from some popular English corpora?,2.11
Original Text,2.2,What is the relationship between the number of types |V | and number of tokens N called?,Herdan's Law
Original Text,2.2,Where is Herdan's Law shown?,Eq. 2.1
Original Text,2.2,What are positive constants in Herdan's Law?,k and B
Original Text,2.2,The value of B depends on what?,corpus size and the genre
Original Text,2.2,What goes up significantly faster than the square root of its length in words?,vocabulary size
Original Text,2.2,What is another measure of the number of words in English?,lemmas
Original Text,2.2,Dictionaries can help in giving what?,lemma counts
Original Text,2.2,How many entries were in the 1989 edition of the Oxford English Dictionary?,"615,000"
Original Text,2.3,What doesn't appear out of nowhere?,Words
Original Text,2.3,What is a specific dialect of?,language
Original Text,2.3,When are NLP algorithms most useful?,when they apply across many languages
Original Text,2.3,How many languages does the world have at the time of this writing?,7097
Original Text,2.3,What language do NLP algorithms tend to be tested on?,English
Original Text,2.3,What are some of the official languages of large industrialized nations?,"Chinese, Spanish, Japanese, German"
Original Text,2.3,What do most languages have?,multiple varieties
Original Text,2.3,What is the name for the many variations of language used by millions of people in African American communities?,African American Language
Original Text,2.3,What social network might use features often used by speakers of African American Language?,Twitter
Original Text,2.3,In what year did Blodgett and others study word segmentation?,2015
Original Text,2.3,What is it called when a speaker or writer uses multiple languages in a single communicative act?,code switching
Original Text,2.3,What is enormously common across the world?,Code switching
Original Text,2.3,What is another dimension of variation?,genre
Original Text,2.3,What types of texts do NLP algorithms have to process?,"newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious texts"
Original Text,2.3,What are some examples of spoken genres?,"telephone conversations, business meetings, police body-worn cameras, medical interviews, or transcripts of television shows or movies"
Original Text,2.3,What are some examples of work situations that NLP algorithms must process?,"doctors' notes, legal text, or parliamentary or congressional proceedings"
Original Text,2.3,What can all influence the linguistic properties of the text we are processing?,"their age, gender, race, socioeconomic class"
Original Text,2.3,What is important to consider when developing computational models for language processing from a corpus?,time
Original Text,2.3,How does a corpus of texts from different historical periods change?,Language changes over time
Original Text,2.3,"When developing computational models for language processing from a corpus, it's important to consider what?","who produced the language, in what context, for what purpose"
Original Text,2.3,What is the best way for the creator to build a datasheet for each corpus?,data statement
Original Text,2.3,What is the context of a corpus?,When and in what situation was the text written/spoken
Original Text,2.3,What type of situation was the text written/spoken?,task
Original Text,2.3,What is an example of a language used in a corpus?,monologue vs. dialogue
Original Text,2.3,What language was the corpus in?,What language
Original Text,2.3,What are the demographics of the authors of a text?,age or gender
Original Text,2.3,What is the collection process of a corpus?,How big is the data
Original Text,2.3,What was the data collected with?,consent
Original Text,2.3,What is the collection process of a corpus?,"How was the data pre-processed, and what metadata is available"
Original Text,2.3,What are the annotators?,demographics
Original Text,2.3,What type of restrictions are there in relation to the distribution of a corpus?,intellectual property
Original Text,2.4.0,"Before almost any natural language processing of a text, the text has to be what?",normalized
Original Text,2.4.0,What is the term for segmenting words?,Tokenizing
Original Text,2.4.0,How do we go through each of these tasks?,walk through
Original Text,2.4.1,What was the inspiration for the UNIX command-line?,Church
Original Text,2.4.1,What Unix command is used to change particular characters in input?,tr
Original Text,2.4.1,What collapses and counts adjacent identical lines?,uniq
Original Text,2.4.1,What is the name of the textfile that contains the 'complete words' of Shakespeare?,sh.txt
Original Text,2.4.1,What type of characters can be changed to a newline with tr?,alphabetic
Original Text,2.4.1,How many words per line can we sort?,one
Original Text,2.4.1,The -n option to sort means to sort what rather than alphabetically?,numerically
Original Text,2.4.1,What can be very handy in building quick word count statistics for any corpus?,Unix tools
Original Text,2.4.2,What is the task of segmenting running text into words called?,tokenization
Original Text,2.4.2,What did the Unix command sequence remove?,all the numbers and punctuation
Original Text,2.4.2,What is a useful piece of information for parsers?,commas
Original Text,2.4.2,What do most NLP applications want to keep in their tokenization?,punctuation that occurs word internally
Original Text,2.4.2,What is the price of AT&T?,45.55
Original Text,2.4.2,What is an example of an email address?,someone@cs.colorado.edu
Original Text,2.4.2,"Commas are used inside numbers in English, every three digits: what?","555,500.50"
Original Text,2.4.2,What do many continental European languages use to mark the decimal point?,comma
Original Text,2.4.2,What can a tokenizer be used to expand clitic contractions that are marked by?,apostrophes
Original Text,2.4.2,What is a part of a word that can't stand on its own?,A clitic
Original Text,2.4.2,What are examples of clitic contractions in French?,articles and pronouns
Original Text,2.4.2,What is required to tokenize multiword expressions like New York or rock 'n' roll as a single token?,multiword expression dictionary
Original Text,2.4.2,Tokenization is intimately tied to what?,named entity recognition
Original Text,2.4.2,What is the most commonly used tokenization standard?,Penn Treebank tokenization standard
Original Text,2.4.2,What does the Penn Treebank tokenization standard do?,separates out clitics
Original Text,2.4.2,Why is tokenization needed to be very fast?,tokenization needs to be run before any other language processing
Original Text,2.4.2,What are deterministic algorithms compiled into?,finite state automata
Original Text,2.4.2,What shows an example of a basic regular expression that can be used to tokenize with the nltk.regexp tokenize,Fig. 2.12
Original Text,2.4.2,What language is the nltk.regexp tokenize function of?,Python
Original Text,2.4.2,When did Bird and colleagues publish their work on the Python-based Natural Language Toolkit?,2009
Original Text,2.4.2,What needs to be tokenized differently when used as a genitive marker?,apostrophe
Original Text,2.4.2,What languages do not use spaces to mark potential word-boundaries?,"Chinese, Japanese, and Thai"
Original Text,2.4.2,What are characters called in Chinese?,hanzi
Original Text,2.4.2,What is a unit of meaning called?,a morpheme
Original Text,2.4.2,How many characters long are words on average?,2.4
Original Text,2.4.2,What is deciding what counts as a word in Chinese?,complex
Original Text,2.4.2,"What is a sentence that could be treated as 3 words ('Chinese Treebank' segmentation): ""YaoMing ","""Yao Ming reaches the finals"""
Original Text,2.4.2,"Who argued that the sentence ""Yao Ming reaches the finals"" could be treated as 3 words?",Chen et al.
Original Text,2.4.2,"How many words could be treated as ""YaoMing reaches the finals""?",3
Original Text,2.4.2,What is 'Yao Ming reaches overall finals' segmentation?,Peking University
Original Text,2.4.2,What are the basic elements of a sentence in Chinese?,characters
Original Text,2.4.2,For most Chinese NLP tasks it turns out to work better to take what as input?,characters
Original Text,2.4.2,For what languages is a character too small a unit?,Japanese and Thai
Original Text,2.4.2,What are algorithms for word segmentation useful for Chinese in the rare situations where word rather than words are required?,character boundaries
Original Text,2.4.2,How are neural sequence models trained?,supervised machine learning
Original Text,2.4.3,What option is there to tokenizing text?,third
Original Text,2.4.3,"Instead of words, what can we use our data to tell us what the tokens should be?",characters
Original Text,2.4.3,What is an important problem in language processing?,unknown words
Original Text,2.4.3,What is a corpus that NLP algorithms learn from?,training corpus
Original Text,2.4.3,What words do NLP algorithms learn from a training corpus?,"low, new, newer, but not lower"
Original Text,2.4.3,What are sets of tokens that include tokens smaller than words called?,subwords
Original Text,2.4.3,What can subwords be?,arbitrary substrings
Original Text,2.4.3,What is the smallest meaning-bearing unit of a language?,morpheme
Original Text,2.4.3,What is an example of a frequently occurring morpheme?,-er
Original Text,2.4.3,What word can be represented by a sequence of known subword units?,lower
Original Text,2.4.3,What are the two parts of most tokenization schemes?,"a token learner, and a token seg- menter"
Original Text,2.4.3,What does the token learner induce?,a set of tokens
Original Text,2.4.3,What takes a raw test sentence and segments it into the tokens in the vocabulary?,token segmenter
Original Text,2.4.3,What is one of the three algorithms used by the token learner?,byte-pair encoding
Original Text,2.4.3,What is the simplest of the three algorithms?,byte-pair encoding or BPE algorithm
Original Text,2.4.3,What is the simplest of the three algorithms?,2.13
Original Text,2.4.3,What does the BPE token learner begin with?,a vocabulary that is just the set of all individual characters
Original Text,2.4.3,What is the new merged symbol added to the vocabulary?,AB
Original Text,2.4.3,How many merges have been done creating k novel tokens?,k
Original Text,2.4.3,The resulting vocabulary consists of the original set of characters plus how many new symbols?,k
Original Text,2.4.3,Where is the BPE algorithm usually run?,inside words
Original Text,2.4.3,What is the starting vocabulary of the BPE algorithm?,11
Original Text,2.4.3,What is the most frequent pair of adjacent symbols?,e r
Original Text,2.4.3,What is the most frequent pair of adjacent symbols?,er
Original Text,2.4.3,What is the token parser used to do?,tokenize a test sentence
Original Text,2.4.3,What just runs on the test data the merges we have learned from the training data?,The token parser
Original Text,2.4.3,Why does the token parser run on the test data?,the frequencies in the test data don't play a role
Original Text,2.4.3,What do we segment each test sentence word into?,characters
Original Text,2.4.3,What is the first rule of the BPE algorithm?,replace every instance of er in the test corpus with r
Original Text,2.4.3,What would be tokenized as a full word if the test corpus contained the word?,n e w e r
Original Text,2.4.3,What new word would be merged into the two tokens low er?,l o w e r
Original Text,2.4.3,How many merges are there in real algorithms?,thousands
Original Text,2.4.3,What happens when BPE is run with thousands of merges on a very large input corpus?,most words will be represented as full symbols
Original Text,2.4.4,What is the task of putting words/tokens in a standard format?,Word normalization
Original Text,2.4.4,What is lost in the normalization process?,spelling information
Original Text,2.4.4,What are two types of normalization that we might want to see information from documents whether they mention the US or the USA?,information retrieval or information extraction
Original Text,2.4.4,What is another kind of normalization?,Case folding
Original Text,2.4.4,Mapping everything to lower case means that what two words are represented identically?,Woodchuck and woodchuck
Original Text,2.4.4,What are some tasks that require case folding?,"sentiment analysis and other text classification tasks, information extraction, and machine translation"
Original Text,2.4.4,What can outweigh the advantage in generalization that case folding would have provided for other words?,US the country and us the pronoun
Original Text,2.4.4,For many natural language processing situations we also want what of a word to behave similarly?,two morphologically different forms
Original Text,2.4.4,"What language has different endings in the phrases Moscow, of Moscow, to Moscow, and so on?",Russian
Original Text,2.4.4,What is the task of determining that two words have the same root?,Lemmatization
Original Text,2.4.4,What words have the shared lemma be?,"am, are, and is"
Original Text,2.4.4,"What Russian word has different endings than am, are, and is?",Moscow
Original Text,2.4.4,"What is the lemmatized form of a sentence like ""He is reading detective stories""?",He be read detective story
Original Text,2.4.4,What is the process of determining that two words have the same root?,lemmatization
Original Text,2.4.4,What is one of the most sophisticated methods for lemmatization?,complete morphological parsing
Original Text,2.4.4,What is the study of the way words are built up from smaller meaning-bearing units called morphemes?,Morphology
Original Text,2.4.4,"What is the central morpheme of the word, supplying the main meaning, and affixes?",stems
Original Text,2.4.4,What is the morpheme of the word/ox?,morpheme fox
Original Text,2.4.4,How can lemmatization algorithms be?,complex
Original Text,2.4.4,What is a simpler but cruder method called stemming?,chopping off word-final affixes
Original Text,2.4.4,What is a naive version of morphological analysis called?,stemming
Original Text,2.4.4,When was the Porter stemmer first used?,1980
Original Text,2.4.4,What produces the following stemmed output?,The Porter stemmer
Original Text,2.4.4,What is the Porter stemmer based on?,series of rewrite rules run in series
Original Text,2.4.4,In what languages can code be found on Martin Porter's homepage?,"Java, Python, etc."
Original Text,2.4.4,Who wrote the original paper on the Porter stemmer?,Martin Porter
Original Text,2.4.4,What can be useful in cases where we need to collapse across different variants of the same lemma?,Simple stemmers
Original Text,2.4.4,Simple stemmers tend to commit errors of what?,over- and under-generalizing
Original Text,2.4.5,What is another important step in text processing?,Sentence segmentation
Original Text,2.4.5,What is one of the most useful cues for segmenting a text into sentences?,punctuation
Original Text,2.4.5,What are relatively unambiguous markers of sentence boundaries?,Question marks and exclamation points
Original Text,2.4.5,What is more ambiguous than question marks and exclamation points?,Periods
Original Text,2.4.5,What character is ambiguous between a sentence boundary marker and a marker of abbreviations like Mr. or Inc?,period character
Original Text,2.4.5,What are two abbreviations that are ambiguous between a sentence boundary marker and a marker of abbreviations?,Mr. or Inc
Original Text,2.4.5,What marked both an abbreviation and a sentence boundary marker?,the final period of Inc.
Original Text,2.4.5,What can be addressed jointly?,tokenization
Original Text,2.4.5,What is part of a word or a sentence-boundary marker?,a period
Original Text,2.4.5,In what year was the final sentence splitter created?,2006
Original Text,2.4.5,In what toolkit is sentence splitting rule-based?,Stanford CoreNLP toolkit
Original Text,2.4.5,What happens when a sentence-ending punctuation is not already grouped with other characters into a token?,optionally followed by additional final quotes or brackets
Original Text,2.5.0,What is much of natural language processing concerned with?,measuring how similar two strings are
Original Text,2.5.0,What is an example of an erroneous string?,graffe
Original Text,2.5.0,What is a similar word to a graffe?,graffe
Original Text,2.5.0,What word differs by only one letter from graffe?,giraffe
Original Text,2.5.0,What is the task of deciding whether two strings refer to the same entity?,coreference
Original Text,2.5.0,How many words do the two strings differ by?,one
Original Text,2.5.0,What gives us a way to quantify both of these intuitions about string similarity?,Edit distance
Original Text,2.5.0,What is the minimum edit distance between two strings?,minimum number of editing operations
Original Text,2.5.0,What is the gap between intention and execution?,5
Original Text,2.5.0,What is the most important visualization for string distances?,alignment between the two strings
Original Text,2.5.0,What is the number of sequences in which an alignment is a correspondence between substrings of the two sequences?,2.14
Original Text,2.5.0,What is a correspondence between substrings of two sequences?,an alignment
Original Text,2.5.0,What do we say I aligns with?,empty string
Original Text,2.5.0,What represents an operation list for converting the top string into the bottom string?,a series of symbols
Original Text,2.5.0,What can we assign to each of these operations?,a particular cost or weight
Original Text,2.5.0,What is the simplest weighting factor in which each of the three operations has a cost of 1?,Levenshtein
Original Text,2.5.0,What is the Levenshtein distance between intention and execution?,5.
Original Text,2.5.0,What did Levenshtein propose in his metric that each insertion or deletion has a cost of 1 and what else?,substitutions are not allowed
Original Text,2.5.0,What is the Levenshtein distance between intention and execution equivalent to?,allowing substitution
Original Text,2.5.0,What is the Levenshtein distance between intention and execution?,8.
Original Text,2.5.1,How do we find the minimum edit distance?,minimum edit distance
Original Text,2.5.1,How do we find the minimum edit distance?,a search task
Original Text,2.5.1,What is the space of all possible edits?,enormous
Original Text,2.5.1,How do we find the minimum edit distance?,recomputing all those paths
Original Text,2.5.1,What is the name for a class of algorithms that apply a table-driven method to solve problems by combining solutions to sub-problems?,dynamic programming
Original Text,2.5.1,When was dynamic programming first introduced?,1957
Original Text,2.5.1,What is the name of the algorithm used for parsing?,CKY algorithm
Original Text,2.5.1,How can a large problem be solved?,by properly combining the solutions to various sub-problems
Original Text,2.5.1,The shortest path of transformed words represents the minimum edit distance between the strings intention and execution shown in what?,Fig. 2.16
Original Text,2.5.1,What is the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16?,2.16
Original Text,2.5.1,What is a string in the shortest path of transformed words that represents the minimum edit distance between intention and execution shown in Fig. 2.16?,exention
Original Text,2.5.1,"If exention is in what operation list, the optimal sequence must also include the optimal path from intention to exention?",optimal
Original Text,2.5.1,What would happen if there were a shorter path from intention to exention?,shorter overall path
Original Text,2.5.1,Who named the minimum edit distance algorithm?,Wagner and Fischer
Original Text,2.5.1,What is the first step in the minimum edit distance algorithm?,minimum edit distance between two strings
Original Text,2.5.1,What is the edit distance between X 1..i and Y 1..j?,"D|i, j"
Original Text,2.5.1,What is the edit distance between X and Y?,"D(n, m)"
Original Text,2.5.1,"What will we use to compute D n, m) bottom up?",dynamic programming
Original Text,2.5.1,What does going from i characters to 0 require?,i deletes
Original Text,2.5.1,What does a target substring of length j require to go from 0 characters to y characters?,j inserts
Original Text,2.5.1,"What do we compute for small i, j?","D i, j"
Original Text,2.5.1,What value is computed by taking the minimum of the three possible paths through the matrix which arrive there?,"i, j"
Original Text,2.5.1,"What is the computation for D i, j?",2.8
Original Text,2.5.1,Where is the minimum edit distance algorithm summarized?,Fig. 2.17
Original Text,2.5.1,In what figure is the minimum edit distance algorithm summarized?,2.17
Original Text,2.5.1,What shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq.,2.18
Original Text,2.5.1,What is the minimum edit distance?,2.8
Original Text,2.5.1,What is knowing the minimum edit distance useful for?,spelling error corrections
Original Text,2.5.1,What can the edit distance algorithm provide with a small change?,the minimum cost alignment between two strings
Original Text,2.5.1,What is useful throughout speech and language processing?,Aligning two strings
Original Text,2.5.1,"In speech recognition, what is the minimum edit distance alignment used for?",to compute the word error rate
Original Text,2.5.1,Alignment plays a role in what?,machine translation
Original Text,2.5.1,How can we extend the edit distance algorithm to produce an alignment?,visualizing an alignment
Original Text,2.5.1,What shows the path through the edit distance matrix?,Figure 2.19
Original Text,2.5.1,What does each boldfaced cell represent?,an alignment of a pair of letters in the two strings
Original Text,2.5.1,"If two boldfaced cells occur in the same row, what will occur in going from the source to the target?",insertion
Original Text,2.5.1,What shows the intuition of how to compute this alignment path?,Figure 2.19
Original Text,2.5.1,How many steps does the computation proceed in?,two steps
Original Text,2.5.1,What is the first step of the minimum edit distance algorithm?,to store backpointers in each cell
Original Text,2.5.1,The backpointer from a cell points to what?,previous cell (or cells)
Original Text,2.5.1,Where are the backpointers from a cell shown?,Fig. 2.19
Original Text,2.5.1,Where are the backpointers from a cell shown in Fig. 2.19?,2.19
Original Text,2.5.1,Some cells have what because the minimum extension could have come from multiple previous cells?,multiple backpointers
Original Text,2.5.1,What is performed in the second step?,a backtrace
Original Text,2.5.1,Where do we start in a backtrace?,last cell
Original Text,2.5.1,What is each complete path between the final cell and the initial cell?,minimum distance alignment
Original Text,2.5.1,What exercise asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment?,Exercise 2.7
Original Text,2.5.1,What did we use in our example?,Levenshtein distance
Original Text,2.5.1,What does the algorithm in Fig. 2.17 allow on the operations?,arbitrary weights
Original Text,2.5.1,What is more likely to happen between letters that are next to each other on the keyboard?,substitutions
Original Text,2.5.1,What is a probabilistic extension of minimum edit distance?,Viterbi algorithm
Original Text,2.5.1,"What does Viterbi compute instead of the ""minimum edit distance"" between two strings?",maximum probability alignment
Original Text,2.5.1,In what chapter will we discuss the Viterbi algorithm?,Chapter 8
Original Text,3.0,"What is difficult, especially about the future?",Predicting
Original Text,3.0,What is an example of something that seems much easier to predict?,the next few words
Original Text,3.0,"What is a very likely word to be in, or possibly over, but probably not?",refrigerator or the
Original Text,3.0,How will we formalize this intuition?,introducing models
Original Text,3.0,Models that assign a probability to each possible next word will also serve to assign a probability to what?,entire sentence
Original Text,3.0,What could a model predict that the following sequence has a much higher probability of appearing in a text?,probability
Original Text,3.0,"What are essential in any task in which we have to identify words in noisy, ambiguous input?",Probabilities
Original Text,3.0,What is a more probable sequence than a bassoon dish?,bassoon dish
Original Text,3.0,What writing tools are needed to find and correct errors in writing?,spelling correction or grammatical error correction
Original Text,3.0,"What phrase will be much more probable than ""Their are""?","""There are"""
Original Text,3.0,Assigning probabilities to sequences of words is also essential in what?,machine translation
Original Text,3.0,What language is a source sentence in?,Chinese
Original Text,3.0,What part of a Chinese source sentence did the translator introduce reporters to?,main contents
Original Text,3.0,What could suggest that briefed reporters on is a more probable English phrase than briefed to reporters?,A probabilistic model of word sequences
Original Text,3.0,Probabilities are important for what types of systems?,augmentative and alternative communication systems
Original Text,3.0,Who et al. 2017 said that probabilities are important for augmentative and alternative communication systems?,Kane
Original Text,3.0,In what year did Kane et al. publish a paper on the importance of probabilities in augmentative and alternative communication systems?,2017
Original Text,3.0,What can people use AAC devices to select words from a menu to be spoken by the system?,use eye gaze or other specific movements
Original Text,3.0,What can be used to suggest likely words for the menu?,Word prediction
Original Text,3.0,What are models that assign probabilities to sequences of words called?,language models or LMs
Original Text,3.0,What is the simplest model that assigns probabilities to sentences and sequences of words?,n-gram
Original Text,3.0,What is a sequence of n words?,n
Original Text,3.0,What model will we use to estimate the probability of the last word of an n-gram given the previous words?,n-gram
Original Text,3.0,"In terminological ambiguity, what term is used to mean the word sequence itself or the predictive model that assigns it a probability?",n-gram
Original Text,3.0,What model assigns the probability of the last word of an n-gram?,predictive model
Original Text,3.0,What is the name of the language model introduced in Chapter 9?,RNN LMs
Original Text,3.1,What is the probability of a word w given some history h?,P(w|h)
Original Text,3.1,What is one way to estimate the probability of a word w given some history h?,relative frequency counts
Original Text,3.1,How many times was the history h followed by the word w?,how many times
Original Text,3.1,What is a large enough corpus to compute the probability of a word w given some history h?,the web
Original Text,3.1,What is the probability of a word w given some history h?,3.2
Original Text,3.1,What should you do to estimate the probability of a word w given some history h?,compute this estimate for yourself
Original Text,3.1,What isn't big enough to give us good estimates in most cases?,web
Original Text,3.1,Why is the webn't big enough to give us good estimates in most cases?,language is creative
Original Text,3.1,What example sentence used to have counts of zero?,"""Walden Pond's water is so transparent that the"""
Original Text,3.1,"What is the probability of an entire sequence of words like ""Walden Pond's water is so transparent""?",its water is so transparent
Original Text,3.1,"What would we have to get the count of if we wanted to know the probability of an entire sequence of words like ""its water is so transparent?""",its water is so transparent
Original Text,3.1,"How much to estimate the probability of an entire sequence of words like ""its water is so transparent""?",rather a lot
Original Text,3.1,What is the first step in estimating the probability of a word given a history h?,formalizing of notation
Original Text,3.1,"To represent the probability of a particular random variable X, taking on the value ""the"", or P(Xi=""the""), we will",P(the)
Original Text,3.1,What will we represent a sequence of N words as?,w1...wn or w1:n
Original Text,3.1,What is one thing we can do to compute probability of entire sequences?,decompose
Original Text,3.1,What shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words?,The chain rule
Original Text,3.1,What suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities?,Equation 3.4
Original Text,3.1,What doesn't seem to help us estimate the probability of a word given a long sequence of preceding words?,using the chain rule
Original Text,3.1,We don't know any way to compute what?,the exact probability of a word given a long sequence of preceding words
Original Text,3.1,Why can't we just estimate by counting the number of times every word occurs following every long string?,language is creative
Original Text,3.1,How can we approximate the probability of a word given its entire history?,by just the last few words
Original Text,3.1,What model approximates the probability of a word given all the previous words?,bigram model
Original Text,3.1,What is the probability P(the|Walden Pond's water is so transparent that) approximated with?,the probability P(the|that)
Original Text,3.1,What is the approximation of the conditional probability of the next word?,P(wn|w1:n-1)
Original Text,3.1,What is the assumption that the probability of a word depends only on the previous word called?,Markov assumption
Original Text,3.1,What are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past?,Markov models
Original Text,3.1,What model looks two words into the past?,trigram
Original Text,3.1,What is the general equation for the conditional probability of the next word in a sequence?,(3.8)
Original Text,3.1,What equation is used to compute the probability of a complete word sequence?,Eq. 3.7
Original Text,3.1,What is the substitution of Eq. 3.4 for the probability of a complete word sequence?,3.7
Original Text,3.1,What is the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4?,3.9
Original Text,3.1,What is another name for bigram?,n-gram
Original Text,3.1,What is an intuitive way to estimate probabilities called?,maximum likelihood estimation
Original Text,3.1,What are the MLE estimates for the parameters of an n-gram model?,0 and 1
Original Text,3.1,What is the count of a bigram to compute a particular bigram probability of a word y given a previous word x?,C(xy)
Original Text,3.1,How many sentences are in the mini-corpus?,three
Original Text,3.1,What symbol is needed at the beginning of a sentence to give us the context of the first word?,s>
Original Text,3.1,What do we need to augment each sentence with a special symbol at the beginning of the sentence to give us the bigram context of the first word?,special end-symbol
Original Text,3.1,What is a special end-symbol?,s>
Original Text,3.1,What is the general case of MLE?,n-gram
Original Text,3.1,What equation estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix?,Equation 3.12
Original Text,3.1,What equation estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix?,Equation 3.12
Original Text,3.1,What is the ratio that estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a pre,relative frequency
Original Text,3.1,What is an example of maximum likelihood estimation?,MLE
Original Text,3.1,"In MLE, the resulting parameter set maximizes what?",the likelihood of the training set T
Original Text,3.1,How many times does the word Chinese occur in a corpus of a million words?,400
Original Text,3.1,How many times does the word Chinese occur in a corpus of a million words?,400
Original Text,3.1,What is not the best estimate of the probability of Chinese occurring in all situations?,.0004
Original Text,3.1,How many times in a million-word corpus is Chinese likely to occur?,400
Original Text,3.1,Where do we present ways to modify the MLE estimates to get better probability estimates?,Section 3.4
Original Text,3.1,How many words are in the example above?,14
Original Text,3.1,"What is the name of the dialogue system that answered questions about a database of restaurants in Berkeley, California?",Berkeley Restaurant Project
Original Text,3.1,What shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project?,Figure 3.1
Original Text,3.1,What are the majority of the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project?,zero
Original Text,3.1,How many words were selected from a random set?,seven
Original Text,3.1,What shows the bigram probabilities after normalization?,Figure 3.2
Original Text,3.1,What is the row of cells in Fig. 3.1 divided by?,unigram
Original Text,3.1,How can we compute the probability of sentences like I want English food or I want Chinese food?,multiplying the appropriate bigram probabilities together
Original Text,3.1,"What exercise is used to compute the probability of ""i want chinese food""?",Exercise 3.2
Original Text,3.1,What kind of phenomena are captured in bigram statistics?,linguistic
Original Text,3.1,What is usually a noun or an adjective?,eat
Original Text,3.1,What is the high probability of sentences beginning with the words I?,personal assistant task
Original Text,3.1,What is the higher probability that people are looking for?,Chinese versus English food
Original Text,3.1,What condition on the previous two words rather than the previous word?,trigram models
Original Text,3.1,"For larger n-grams, we'll need to assume what to the left and right of the sentence end?",extra contexts
Original Text,3.1,What do we use to compute trigram probabilities at the very beginning of the sentence?,two pseudo-words
Original Text,3.1,What do we always represent and compute language model probabilities in log format as?,log probabilities
Original Text,3.1,What are probabilities by definition?,less than or equal to 1
Original Text,3.1,Multiplying enough n-grams together would result in what?,numerical underflow
Original Text,3.1,What do we use instead of raw probabilities to get numbers that are not as small?,log probabilities
Original Text,3.1,What is equivalent to multiplying in linear space?,Adding in log space
Original Text,3.1,What is the exp of a bigram statistic?,logprob
Original Text,3.2.0,What is the best way to evaluate the performance of a language model?,measure how much the application improves
Original Text,3.2.0,What is the best way to evaluate the performance of a language model?,extrinsic evaluation
Original Text,3.2.0,What is the only way to know if a particular improvement in a component is really going to help the task at hand?,Extrinsic evaluation
Original Text,3.2.0,"What can we compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate",speech recognition
Original Text,3.2.0,Is running big NLP systems end-to-end expensive or expensive?,expensive
Original Text,3.2.0,What can be used to quickly evaluate potential improvements in a language model?,metric
Original Text,3.2.0,What does an intrinsic evaluation metric measure?,quality of a model independent of any application
Original Text,3.2.0,What do we need for an intrinsic evaluation of a language model?,a test set
Original Text,3.2.0,What are the probabilities of an n-gram model trained on?,the training set or training corpus
Original Text,3.2.0,What is the unseen data used to measure the quality of an n-gram model?,test set or test corpus
Original Text,3.2.0,Why do we call test sets held out corpora?,hold them out from the training data
Original Text,3.2.0,What do we divide the corpus of text into?,training and test sets
Original Text,3.2.0,What does it mean to compare two different n-gram models?,fit the test set
Original Text,3.2.0,What is a better model?,whichever model assigns a higher probability to the test set
Original Text,3.2.0,What is the better model given two probabilistic models?,the one that has a tighter fit to the test data
Original Text,3.2.0,What is our evaluation metric based on?,test set probability
Original Text,3.2.0,What is a sentence that we are trying to compute the probability of?,test
Original Text,3.2.0,"If a test sentence is part of the training corpus, we will mistakenly assign it what?",artificially high probability
Original Text,3.2.0,"When a test sentence is part of the training corpus, we mistakenly assign it an artificially high probability when it occurs in the test set",training on the test set
Original Text,3.2.0,What introduces a bias that makes the probabilities all look too high?,Training on the test set
Original Text,3.2.0,What do we do when we use a particular test set so often that we need a fresh test set that is truly unseen?,implicitly tune to its characteristics
Original Text,3.2.0,What do we need when we use a particular test set so often that we implicitly tune to its characteristics?,a fresh test set that is truly unseen
Original Text,3.2.0,What is the initial test set called when we need a fresh test set that is truly unseen?,development test set
Original Text,3.2.0,What do we divide our data into?,"training, development, and test sets"
Original Text,3.2.0,How much training data do we want in a test set?,as much training data as possible
Original Text,3.2.0,What gives us enough statistical power to measure a statistically significant difference between two potential models?,smallest test set
Original Text,3.2.0,"In practice, we often divide our data into what percentage of training, 10% development, and 10% test?",80%
Original Text,3.2.0,What can be taken from some continuous sequence of text inside the corpus?,test data
Original Text,3.2.1,What is a variant of raw probability used for evaluating language models?,perplexity
Original Text,3.2.1,What is the perplexity of a language model on a test set?,inverse probability of the test set
Original Text,3.2.1,What is the perplexity of a test set?,w1w2...wN
Original Text,3.2.1,What can we use to expand the probability of W?,the chain rule
Original Text,3.2.1,What language model is used to compute the perplexity of W?,bigram
Original Text,3.2.1,"Why is the higher the conditional probability of the word sequence, the lower the perplexity?",the inverse
Original Text,3.2.1,What is the conditional probability of the word sequence?,higher
Original Text,3.2.1,What is equivalent to maximizing the test set probability according to the language model?,minimizing perplexity
Original Text,3.2.1,What is the entire sequence of words in some test set?,Eq. 3.15 or Eq. 3.16
Original Text,3.2.1,What do we generally use for word sequence in Eq. 3.15 or Eq. 3.16?,the entire sequence of words in some test set
Original Text,3.2.1,What do we generally use for word sequence in Eq. 3.15 or Eq. 3.16?,the entire sequence of words in some test set
Original Text,3.2.1,What do we need to include in the perplexity calculation?,begin- and end-sentence markers
Original Text,3.2.1,What is the number of possible next words that can follow any word?,The branching factor
Original Text,3.2.1,What is the probability that each of the 10 digits occurs with equal probability?,P 0.1
Original Text,3.2.1,What is the perplexity of the mini-language?,10
Original Text,3.2.1,What is the test string of digits of length N?,assume that in the training set all the digits occurred with equal probability
Original Text,3.2.1,What is the perplexity of the test string of digits of length N?,Eq. 3.15
Original Text,3.2.1,What is the branching factor of a language?,3.15
Original Text,3.2.1,What is the perplexity of the test string of digits of length N?,the number zero is really frequent and occurs far more often than other numbers
Original Text,3.2.1,How many times does 0 occur in the training set?,91
Original Text,3.2.1,Why is the perplexity of the test set to be lower?,most of the time the next number will be zero
Original Text,3.2.1,What is the probability that the next number will be zero?,has a high probability
Original Text,3.2.1,What is the branching factor of a language?,10
Original Text,3.2.1,What is the exact calculation of the branching factor of a language?,exercise 12
Original Text,3.2.1,Perplexity is closely related to the information-theoretic notion of what?,entropy
Original Text,3.2.1,What can be used to compare different n-gram models?,perplexity
Original Text,3.2.1,"How many words did we train unigram, bigram, and trigram grammars on?",38 million
Original Text,3.2.1,How many words were used in the test set?,1.5 million
Original Text,3.2.1,How many words were used in the test set?,1.5 million
Original Text,3.2.1,The table below shows the perplexity of what word WSJ test set according to each of these grammars?,1.5 million
Original Text,3.2.1,What determines the lower perplexity?,the more information the n-gram gives us about the word sequence
Original Text,3.2.1,Perplexity is related inversely to what?,likelihood of the test sequence according to the model
Original Text,3.2.1,What is required in computing perplexities?,n-gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set
Original Text,3.2.1,What can cause the perplexity to be artificially low?,Any kind of knowledge of the test set
Original Text,3.2.1,How is the perplexity of two language models comparable?,if they use identical vocabularies
Original Text,3.2.1,What does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation?,(intrinsic) improvement in perplexity
Original Text,3.2.1,Perplexity is commonly used as what?,quick check on an algorithm
Original Text,3.2.1,What should a model's improvement in perplexity always be confirmed by before concluding the evaluation of the model?,end-to-end evaluation of a real task
Original Text,3.3.0,What is the n-gram model dependent on?,training corpus
Original Text,3.3.0,What often encode specific facts about a given training corpus?,the probabilities
Original Text,3.3.0,When was Shannon born?,1951
Original Text,3.3.0,For what case is it simplest to visualize how the n-gram model works?,unigram
Original Text,3.3.0,What is the probability space between all the words of the English language?,0 and 1
Original Text,3.3.0,What is the probability space between the words of the English language?,0 and 1
Original Text,3.3.0,What do we continue choosing and generating words until we randomly generate the sentence-final token?,random numbers
Original Text,3.3.0,What is the first word of a bigram?,w
Original Text,3.3.0,What is the second part of a bigram that starts with s?,w
Original Text,3.3.0,"What shows the random sentences generated from unigram, bigram, trigram, and 4-gram models trained on Shakespeare's works?",Fig. 3.3
Original Text,3.3.0,"What shows the random sentences generated from unigram, bigram, trigram, and 4-gram models trained on Shakespeare's works?",Fig. 3.3
Original Text,3.3.0,How long is the context on which we train the model?,longer
Original Text,3.3.0,What type of sentences have no coherent relation between words or any sentence-final punctuation?,unigram
Original Text,3.3.0,What sentences have some local word-to-word coherence?,bigram
Original Text,3.3.0,The tri-gram and what other sentence are beginning to look a lot like Shakespeare?,4-gram
Original Text,3.3.0,What do the 4-gram sentences look a little too much like?,Shakespeare
Original Text,3.3.0,"Who wrote the words ""It cannot be but so""?",King John
Original Text,3.3.0,"Why do the words ""It cannot be but so"" look like Shakespeare?",his oeuvre is not very large
Original Text,3.3.0,How many possible bigrams are there?,"844,000,000"
Original Text,3.3.0,"For many 4-grams, how many possible continuations are there?",one
Original Text,3.3.0,What newspaper does WSJ stand for?,Wall Street Journal
Original Text,3.3.0,What language are both Shakespeare and the Wall Street Journal?,English
Original Text,3.3.0,"What shows the sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ?",Fig. 3.4
Original Text,3.3.0,"What is the size of the sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ?",3.4
Original Text,3.3.0,"What is the name of the person who wrote the words ""It cannot be but so"" in Fig. 3.4?",pseudo-Shakespeare
Original Text,3.3.0,"What is the number of n-grams that both model ""English-like sentences""?",3.3
Original Text,3.3.0,What do both Shakespeare and WSJ model?,English-like sentences
Original Text,3.3.0,What are likely to be pretty useless as predictors if the training sets and the test sets are as different as Shakespeare and WSJ?,Statistical models
Original Text,3.3.0,What type of models should we build to deal with this problem?,n-gram models
Original Text,3.3.0,What type of genre should a training corpus have?,similar genre
Original Text,3.3.0,What do we need a training corpus of?,legal documents
Original Text,3.3.0,What do we need to build a language model for a question-answering system?,a training corpus of questions
Original Text,3.3.0,What is it important to get training data in when processing social media posts or spoken transcripts?,dialect or variety
Original Text,3.3.0,What is the name for the many variations of language used in African American communities?,African American Language
Original Text,3.3.0,What is a spelling for a word that marks immediate future tense that doesn't occur in other varieties?,den for then
Original Text,3.3.0,What language has markedly different vocabulary and n-gram patterns from American English?,Nigerian English
Original Text,3.3.0,What is beta get ur wiv twitter?,IT placement
Original Text,3.3.0,What is still not sufficient?,Matching genres and dialects
Original Text,3.3.0,Our models may still be subject to the problem of what?,sparsity
Original Text,3.3.0,What is a good estimate of the probability of a sufficient number of times?,n-gram
Original Text,3.3.0,What language is bound to be missing from any corpus?,English
Original Text,3.3.0,What do we have many cases of that should have some non-zero probability?,"""zero probability n-grams"""
Original Text,3.3.0,What is the bigram that follows the bigram in the WSJ Treebank corpus?,denied the
Original Text,3.3.0,What are two phrases in the WSJ Treebank corpus that our model will incorrectly estimate that the P(offer|denied the),denied the offer or denied the loan
Original Text,3.3.0,What is the P(offer|denied the)?,0
Original Text,3.3.0,What are things that don't ever occur in the training set but do occur in the test set?,zeros
Original Text,3.3.0,What means we are underestimating the probability of all sorts of words that might occur?,their presence
Original Text,3.3.0,"If the probability of any word in the test set is 0, what is the entire probability of the test set?","if the probability of any word in the test set is 0, the entire probability of the test set is 0."
Original Text,3.3.0,What is perplexity based on?,inverse probability of the test set
Original Text,3.3.0,Why can't we compute perplexity if some words have zero probability?,we can't divide by 0
Original Text,3.3.1,What is the problem of words whose probability is zero?,bigram probability
Original Text,3.3.1,What is the problem of words whose bigram probability is zero?,words we simply have never seen before
Original Text,3.3.1,Why can't we have a language task in which this can't happen?,because we know all the words that can occur
Original Text,3.3.1,"In a closed vocabulary system, the test set can only contain words from this lexicon, and there will be no what?",unknown words
Original Text,3.3.1,What is a domain where we have a pronunciation dictionary or a phrase table that are fixed in advance?,machine translation
Original Text,3.3.1,What are OOV words?,out of vocabulary
Original Text,3.3.1,What is the percentage of OOV words that appear in the test set called?,OOV rate
Original Text,3.3.1,What is the pseudo-word used in an open vocabulary system?,UNK>
Original Text,3.3.1,How many ways are there to train the probabilities of the unknown word model UNK>?,two
Original Text,3.3.1,What is an example of a vocabulary size V in advance?,"50,000"
Original Text,3.3.1,What is UNK> treated like in a language model?,a regular word
Original Text,3.3.1,What metrics does the exact choice of UNK> model have an effect on?,perplexity
Original Text,3.3.1,How can a language model achieve low perplexity?,by choosing a small vocabulary and assigning the unknown word a high probability
Original Text,3.3.1,What should only be compared across language models with the same vocabularies?,perplexities
Original Text,3.4.0,What is an example of an unseen context for words that appear in a test set?,they appear after a word they never appeared after in training
Original Text,3.4.0,What do we do to keep a language model from assigning zero probability to unseen events?,shave off a bit of probability mass
Original Text,3.4.0,What is shave off a bit of probability mass from some more frequent events and give it to the events we've never seen?,smoothing or discounting
Original Text,3.4.0,What type of smoothing is introduced?,Kneser-Ney
Original Text,3.4.1,What is the simplest way to do smoothing?,add one
Original Text,3.4.1,All the counts that used to be zero will now have a count of what?,1
Original Text,3.4.1,What is the simplest way to do smoothing?,Laplace smoothing
Original Text,3.4.1,What does Laplace smoothing apply to?,unigram probabilities
Original Text,3.4.1,What is Laplace smoothing's alternate name?,add-one smoothing
Original Text,3.4.1,What do we need to adjust the denominator to take into account?,the extra Y observations
Original Text,3.4.1,What happens to our P values if we don't do?,increase the denominator
Original Text,3.4.1,How does a smoothing algorithm affect the numerator?,by defining an adjusted count c*
Original Text,3.4.1,What is the adjusted count c* easier to compare with?,MLE counts
Original Text,3.4.1,What is lowering some non-zero counts in order to get the probability mass that will be assigned to the zero counts?,discounting
Original Text,3.4.1,What is the ratio of the discounted counts to the original counts?,relative discount d
Original Text,3.4.1,What do we smooth now that we have the intuition for the unigram case?,Berkeley Restaurant Project bigrams
Original Text,3.4.1,What shows the add-one smoothed counts for the bigrams in Fig. 3.1?,Figure 3.5
Original Text,3.4.1,What shows the add-one smoothed counts for the bigrams in Fig. 3.1?,Figure 3.5
Original Text,3.4.1,What shows the add-one smoothed probabilities for the bigrams in Fig. 3.2?,Figure 3.6
Original Text,3.4.1,Figure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. what?,3.2
Original Text,3.4.1,How are normal bigram probabilities computed?,by normalizing each row of counts by the unigram count
Original Text,3.4.1,What is the number of total word types in the vocabulary?,V
Original Text,3.4.1,What is the number of times a smoothing algorithm has changed the original counts?,3.6
Original Text,3.4.1,How can we see how much a smoothing algorithm has changed the original counts?,reconstruct the count matrix
Original Text,3.4.1,How can adjusted counts be computed?,Eq. 3.24
Original Text,3.4.1,How much a smoothing algorithm has changed the original counts?,3.24
Original Text,3.4.1,What shows the reconstructed counts?,Figure 3.7
Original Text,3.4.1,What smoothing algorithm has made a very big change to the counts?,add-one
Original Text,3.4.1,What changed from 609 to 238?,C(want to)
Original Text,3.4.1,P(to|want) decreases from what in the unsmoothed case to.26 in the smoothed case?,.66
Original Text,3.4.1,What is the discount for the bigram want to?,.39
Original Text,3.4.1,Why does the sharp change in counts and probabilities occur?,because too much probability mass is moved to all the zeros
Original Text,3.4.2,What is one alternative to add-one smoothing?,move a bit less of the probability mass
Original Text,3.4.2,What does add-k smoothing add instead of adding I to each count?,fractional count k
Original Text,3.4.2,What is the fractional count k?,.01
Original Text,3.4.2,What is the fractional count k?,.01
Original Text,3.4.2,What is a fractional count k called?,add-k smoothing
Original Text,3.4.2,What does add-k smoothing require?,a method for choosing k
Original Text,3.4.2,Add-k smoothing doesn't work well for what?,language modeling
Original Text,3.4.3,The discounting we have been discussing so far can help solve the problem of what?,zero frequency n-grams
Original Text,3.4.3,What can we draw on to solve the problem of zero frequency n-grams?,additional source of knowledge
Original Text,3.4.3,What can we estimate the probability of a trigram if we have no examples of it?,bigram probability
Original Text,3.4.3,"If we don't have counts to compute the bigram probability, what can we look to?",unigram probability
Original Text,3.4.3,What does using less context help to generalize more for?,contexts that the model hasn't learned much about
Original Text,3.4.3,How many ways are there to use this n-gram hierarchy?,two
Original Text,3.4.3,What does backoff use if the evidence is sufficient?,trigram
Original Text,3.4.3,What do we back off to if we have zero evidence for a higher-order n-gram?,lower-order n-gram
Original Text,3.4.3,In what method do we always mix the probability estimates from all the n-gram estimators?,interpolation
Original Text,3.4.3,How do we combine different order n-grams?,linearly interpolating all the models
Original Text,3.4.3,How do we estimate the trigram probability?,"by mixing together the unigram, bigram, and trigram probabilities"
Original Text,3.4.3,How is each weight computed in a slightly more sophisticated version of linear interpolation?,by conditioning on the context
Original Text,3.4.3,"If we have particularly accurate counts for a particular bigram, we assume that what will be more trustworthy?",the counts of the trigrams based on this bigram
Original Text,3.4.3,What shows the equation for interpolation with context-conditioned weights?,Equation 3.28
Original Text,3.4.3,What are the simple interpolation and conditional interpolation lambdas learned from?,held-out corpus
Original Text,3.4.3,What is an additional training corpus that we use to set hyperparameters like these lambda values?,A held-out corpus
Original Text,3.4.3,What do we fix in a held-out corpus?,n-gram probabilities
Original Text,3.4.3,"When plugged into Eq. 3.28, what gives us the highest probability of the held-out set?",3.26
Original Text,3.4.3,How many ways are there to find the optimal set of lambdas?,various
Original Text,3.4.3,What is an iterative learning algorithm that converges on locally optimal lambdas?,EM algorithm
Original Text,3.4.3,How do we approximate a n-gram if it has zero counts?,by backing off to the (N-1)-gram
Original Text,3.4.3,When do we continue backing off to the (N-1)-gram?,until we reach a history that has some counts
Original Text,3.4.3,"In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save",lower order n-grams
Original Text,3.4.3,What smoothing method is used when the higher-order n-grams aren't discounted?,add-one
Original Text,3.4.3,What is the name of the function that distributes the probability mass to the lower order n-grams?,n
Original Text,3.4.3,What is a backoff with discounting called?,Katz backoff
Original Text,3.4.3,What is a backoff with discounting called?,Katz backoff
Original Text,3.4.3,What do we recursively back off to for the shorter-history (N-1)-gram?,Katz probability
Original Text,3.4.3,What smoothing method is often combined with Katz backoff?,Good-Turing
Original Text,3.4.3,What backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the P* and alpha values?,Good-Turing
Original Text,3.5,What is one of the most commonly used and best performing n-gram smoothing methods?,interpolated Kneser-Ney algorithm
Original Text,3.5,What is the Kneser-Ney method called?,absolute discounting
Original Text,3.5,Why is discounting the counts for frequent n-grams necessary?,to save some probability mass
Original Text,3.5,Who wrote a clever idea for the Kneser-Ney algorithm?,Church and Gale
Original Text,3.5,What does an n-gram have?,count 4
Original Text,3.5,How do we discount an n-gram that has count 4?,by some amount
Original Text,3.5,How much should we discount an n-gram that has count 4?,how much
Original Text,3.5,Church and Gale's clever idea was to look at a held-out corpus and see what the count is for all those bigrams,count 4
Original Text,3.5,Church and Gale computed a bigram grammar from how many words of AP newswire?,22 million
Original Text,3.5,How many times did a bigram occur in the next 22 million words?,3.23
Original Text,3.5,What shows the counts for bigrams with c from 0 to 9?,Fig. 3.8
Original Text,3.5,Church and Gale's Fig. 3.8 shows the counts for bigrams with what count?,c from 0 to 9
Original Text,3.5,What figure shows that all the other bigram counts in the held-out set could be estimated pretty well by just subtracting 0.75 from the count in,Fig. 3.8
Original Text,3.5,How many bigram counts could be estimated pretty well by subtracting from the count in the training set?,0.75
Original Text,3.5,What formalizes this intuition by subtracting a fixed (absolute) discount d from each count?,Absolute discounting
Original Text,3.5,What will not affect the very high counts?,a small discount d
Original Text,3.5,What will a small discount d mainly modify?,smaller counts
Original Text,3.5,Fig. 3.8 suggests that in practice this discount is actually a good one for bigrams with what counts?,counts 2 through 9
Original Text,3.5,What is the second term in the equation for interpolated absolute discounting applied to bigrams?,unigram with an interpolation weight lambda
Original Text,3.5,What is the discount value for bigrams with counts of 1?,0.5
Original Text,3.5,What augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution?,Kneser-Ney discounting
Original Text,3.5,What is the job of Kneser-Ney discounting?,predicting the next word
Original Text,3.5,"What word is more likely to follow the word ""I can't see without my reading""?",glasses
Original Text,3.5,"What is a very frequent word in the word ""glasses""?",Hong Kong
Original Text,3.5,What will assign Kong a higher probability than glasses?,A standard unigram model
Original Text,3.5,What phrase is Kong mainly frequent in?,Hong Kong
Original Text,3.5,What word has a much wider distribution than Kong?,glasses
Original Text,3.5,"What answers the question ""How likely is w?""",P(w)
Original Text,3.5,How likely is w to appear as a new unseen context?,novel
Original Text,3.5,How likely is w to appear as a new unseen context?,novel
Original Text,3.5,What is the Kneser-Ney intuition based on?,the number of bigram types it completes
Original Text,3.5,When was every bigram type a novel continuation?,the first time it was seen
Original Text,3.5,We hypothesize that words that have appeared in what number of contexts in the past are more likely to appear in some new context as well?,more
Original Text,3.5,What do we normalize to turn this count into a probability?,the total number of word bigram types
Original Text,3.5,What is normalized by the number of words preceding all words?,the number of word types seen to precede w
Original Text,3.5,How many contexts does a frequent word (Kong) occur in?,one
Original Text,3.5,What is the final equation for Interpolated Kneser-Ney smoothing for bigrams?,3.35
Original Text,3.5,What is the lambda?,normalizing constant
Original Text,3.5,What is the first term of the normalized discount?,d
Original Text,3.5,What is the second term in the equation for Kneser-Ney smoothing for bigrams?,number of word types that can follow wi-1
Original Text,3.5,"What is the highest-order n-gram being interpolated if we are interpolating trigram, bigram, and unigram",trigram
Original Text,3.5,What is the number of unique single word contexts?,The continuation count
Original Text,3.5,What parameter is the empty string in the uniform distribution?,epsilon
Original Text,3.5,What word is included as a regular vocabulary entry with count zero?,UNK>
Original Text,3.5,Who wrote the best performing version of Kneser-Ney smoothing?,Chen and Goodman
Original Text,3.5,What are the three different discounts used in modified Kneser-Ney smoothing?,"d1, d2 and d3+"
Original Text,3.5,Who published a study on modified Kneser-Ney smoothing in 2013?,Heafield et al.
Original Text,3.5,In what year did Heafield et al. publish their work on modified Kneser-Ney smoothing?,2013
Original Text,3.6,What can be used to build large language models?,enormous collections
Original Text,3.6,How many words are in the Web 1 Trillion 5-gram corpus?,"1,024,908,267,229"
Original Text,3.6,How many tokens of n-grams are in Google Books Ngrams corpora?,800 billion
Original Text,3.6,How many words are in the COCA corpus of American English?,1 billion
Original Text,3.6,What is a balanced corpora?,COCA
Original Text,3.6,What considerations are important when building language models that use large sets of n-grams?,Efficiency
Original Text,3.6,How is each word represented in memory?,a 64-bit hash number
Original Text,3.6,How are probabilities quantized?,4-8 bits
Original Text,3.6,How can n-grams be shrunk?,pruning
Original Text,3.6,What is a technique used to build approximate language models?,Bloom filters
Original Text,3.6,When did Church and Osborne use Bloom filters?,2007
Original Text,3.6,What efficient language model toolkit uses sorted arrays?,KenLM
Original Text,3.6,What do efficient language model toolkits use?,sorted arrays
Original Text,3.6,What does KenLM use to efficiently build the probability tables in a minimal number of passes through a large corpus?,merge sorts
Original Text,3.6,What is used to build web-scale language models?,Kneser-Ney smoothing
Original Text,3.6,What may be sufficient with very large language models?,a much simpler algorithm
Original Text,3.6,What algorithm gives up the idea of trying to make the language model a true probability distribution?,stupid backoff
Original Text,3.6,What gives up the idea of trying to make the language model a true probability distribution?,Stupid backoff
Original Text,3.6,What does stupid backoff not do to the higher-order probabilities?,discounting
Original Text,3.6,"If a higher-order probability has a zero count, we simply backoff to a lower-order probability, weighed by a fixed",n-gram
Original Text,3.6,What does stupid backoff not produce?,probability distribution
Original Text,3.6,Where does the backoff terminate?,unigram
Original Text,3.6,What value worked well for lambda?,0.4
Original Text,3.6,What language model did Brants et al. find that a value of 0.4 worked well for?,lambda
Original Text,3.7,What was introduced in Section 3.2.1 as a way to evaluate n-gram models on a test set?,perplexity
Original Text,3.7,What does a better n-gram model assign to the test data?,higher probability
Original Text,3.7,What does the perplexity measure originate from?,information-theoretic concept of cross-entropy
Original Text,3.7,What is the relationship between perplexity and the information-theoretic concept of cross-entropy?,entropy
Original Text,3.7,What is a measure of information?,Entropy
Original Text,3.7,What is the entropy of the random variable X called?,p(x)
Original Text,3.7,"In principle, the log can be computed in what?",any base
Original Text,3.7,What is the resulting value of entropy measured in if we use log base 2?,bits
Original Text,3.7,What is an intuitive way to think about entropy?,lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme
Original Text,3.7,Who wrote a standard information theory textbook in 1991?,Cover and Thomas
Original Text,3.7,Why would we want to send a short message to the bookie to tell him which of the eight horses to bet on?,it is too far to go all the way to Yonkers Racetrack
Original Text,3.7,What is the binary representation of the horse's number?,888
Original Text,3.7,How many bits is each horse coded with?,3 bits
Original Text,3.7,What is the question of entropy?,Can we do better
Original Text,3.7,What does the spread represent?,the prior probability of each horse
Original Text,3.7,What random variable gives us a lower bound on the number of bits?,X
Original Text,3.7,"A code that averages what bit per race can be built with short encodings for more probable horses, and longer encodings for less",2 bits
Original Text,3.7,What are the remaining horses in a code?,10
Original Text,3.7,What if the horses are what?,equally likely
Original Text,3.7,How many bits did each horse take to code?,3
Original Text,3.7,What is the same as the entropy of a binary code?,entropy
Original Text,3.7,What is the probability of each horse in an equal-length binary code?,1/8
Original Text,3.7,What is the entropy of the choice of horses?,3 bits
Original Text,3.7,Until now we have been computing the entropy of what?,single variable
Original Text,3.7,What is most of what we will use entropy for?,sequences
Original Text,3.7,For what is the entropy of some sequence of words used?,a grammar
Original Text,3.7,What is one way to compute the entropy of some sequence of words?,variable that ranges over sequences of words
Original Text,3.7,What is the entropy rate?,per-word entropy
Original Text,3.7,"To measure the true entropy of a language, we need to consider sequences of what length?",infinite length
Original Text,3.7,"What theorem states that if the language is regular in certain ways (to be exact, if it is both stationary and ergo",Shannon-McMillan-Breiman theorem
Original Text,3.7,What is the Shannon-McMillan-Breiman theorem?,a single sequence that is long enough
Original Text,3.7,What is the intuition of the Shannon-McMillan-Breiman theorem?,Shannon-McMillan-Breiman theorem
Original Text,3.7,What is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index,A stochastic process
Original Text,3.7,What is the same for words at time t as the probability distribution at time t+1?,probability distribution
Original Text,3.7,What models are stationary?,Markov models
Original Text,3.7,What is dependent only on Pi-1 in a bigram?,Pi
Original Text,3.7,"If we shift our time index by x, what is still dependent on Pi+x-1?",Pi+x
Original Text,3.7,"What is not stationary, since the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent?",natural language
Original Text,3.7,What do our statistical models give to the correct distributions and entropies of natural language?,approximation
Original Text,3.7,How can we compute the entropy of some stochastic process?,by taking a very long sample of the output and computing its average log probability
Original Text,3.7,What does the Shannon-McMillan-Breiman theorem introduce?,cross-entropy
Original Text,3.7,When is the cross-entropy useful?,when we don't know the actual probability distribution p
Original Text,3.7,What is a model of p?,m
Original Text,3.7,How is the cross-entropy of m on p defined?,"by drawing sequences according to the probability distribution p, but summing the log of their probabilities according to m"
Original Text,3.7,What is an upper bound on the entropy H(p)?,"H(p,m)"
Original Text,3.7,What can we use a simplified model m to help estimate the true entropy of?,a sequence of symbols drawn according to probability y
Original Text,3.7,"The more accurate m is, the closer what is the cross-entropy H(p, m) to the true entropy",closer
Original Text,3.7,What is a measure of how accurate a model is?,"H(p, m) and H(p)"
Original Text,3.7,What are the two models with the lowest cross-entropy?,m1 and m2
Original Text,3.7,The cross-entropy can never be lower than what?,true entropy
Original Text,3.7,What is the relation between cross-entropy and cross-entropy?,perplexity
Original Text,3.7,"In Eq. 3, what is the relation between perplexity and cross-entropy?",3.49
Original Text,3.7,The length of the observed word sequence goes to what?,infinity
Original Text,3.7,What will we need to estimate the cross-entropy?,an approximation
Original Text,3.7,The perplexity of a model P on a sequence of words W is now formally defined as what?,exp
Original Text,4.0,What lies at the heart of both human and machine intelligence?,Classification
Original Text,4.0,What is one example of assigning a category to an input?,assigning grades
Original Text,4.0,"Who imagined classifying animals into suckling pigs, mermaids, mermaids and stray dogs?",Jorge Luis Borges
Original Text,4.0,What type of tasks involve classification?,language processing tasks
Original Text,4.0,What is text categorization?,assigning a label or category to an entire text or document
Original Text,4.0,What is the extraction of positive or negative orientation that a writer expresses toward some object?,sentiment analysis
Original Text,4.0,What does an editorial or political text express sentiment toward?,a candidate or political action
Original Text,4.0,What type of cues do the words of a review provide?,excellent
Original Text,4.0,What types of reviews provide cues for sentiment analysis?,positive and negative
Original Text,4.0,What are two examples of cues from positive and negative reviews of movies and restaurants?,awful and ridiculously
Original Text,4.0,What is another important commercial application?,Spam detection
Original Text,4.0,What can be used to perform the binary classification task of assigning an email to one of the two classes spam or not-spam?,Many lexical and other features
Original Text,4.0,What phrases might you be suspicious of an email containing?,"""online pharmaceutical"" or ""WITHOUT ANY COST"" or ""Dear Winner"""
Original Text,4.0,What is another thing we might want to know about a text?,the language it's written in
Original Text,4.0,Texts on what can be in any number of languages?,social media
Original Text,4.0,What is the first step in most language processing pipelines?,language id
Original Text,4.0,What is determining a text's author?,authorship attribution
Original Text,4.0,What is one of the oldest tasks in text classification?,assigning a library subject category or topic label
Original Text,4.0,What is an important component of information retrieval?,epidemiology
Original Text,4.0,What does MeSH stand for?,Medical Subject Headings
Original Text,4.0,When was the naive Bayes algorithm invented?,1961
Original Text,4.0,Classification is essential for what?,tasks below the level of the document
Original Text,4.0,What is deciding if a character should be a word boundary?,word tokenization
Original Text,4.0,Even language modeling can be viewed as what?,classification
Original Text,4.0,What is the goal of classification?,"to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes"
Original Text,4.0,What is one method for classifying text?,handwritten rules
Original Text,4.0,What constitutes a state-of-the-art system?,handwritten rule-based classifiers
Original Text,4.0,Why are handwritten rules fragile?,humans aren't necessarily good at coming up with the rules
Original Text,4.0,How are most cases of classification in language processing done?,supervised machine learning
Original Text,4.0,"What is the term for a data set of input observations, each associated with some correct output?",supervised learning
Original Text,4.0,What is the goal of supervised learning?,learn how to map from a new observation to a correct output
Original Text,4.0,"What type of classification is to take input x and a fixed set of output classes Y=y1,y2,...,yM and return",supervised
Original Text,4.0,What class does supervised classification use instead of y?,c
Original Text,4.0,What is the training set of in supervised classification?,N documents
Original Text,4.0,What tells us the probability of the observation being in the class?,probabilistic classifier
Original Text,4.0,What can be useful when combining systems?,making discrete decisions early on
Original Text,4.0,What is used to build classifiers?,machine learning algorithms
Original Text,4.0,What machine learning algorithm is introduced in this chapter?,naive Bayes
Original Text,4.0,How many ways of doing classification do naive Bayes and logistic regression exemplify?,two
Original Text,4.0,What type of classifiers build a model of how a class could generate some input data?,Generative classifiers
Original Text,4.0,What do Generative classifiers return given an observation?,the class most likely to have generated the observation
Original Text,4.0,What type of classifier learns what features from the input are most useful to discriminate between the different possible classes?,Discriminative classifiers
Original Text,4.0,What type of classifiers still have a role in supervised classification?,generative classifiers
Original Text,4.1,What is the multinomial Bayes classifier called?,naive
Original Text,4.1,Where is the intuition of the naive Bayes classifier shown?,Fig. 4.1
Original Text,4.1,What is the intuition of the naive Bayes classifier shown in Fig. 4.1?,4.1
Original Text,4.1,What does the classifier represent a text document as if it were?,a bag-of-words
Original Text,4.1,What is one example of a naive Bayes classifier?,I love this movie
Original Text,4.1,What type of classifier is naive Bayes?,probabilistic
Original Text,4.1,Who first applied Bayesian inference to text classification?,Mosteller and Wallace
Original Text,4.1,What is the intuition of Bayesian classification?,to transform Eq. 4.1
Original Text,4.1,What does Bayes' rule transform Eq. 4.1 into?,other probabilities
Original Text,4.1,How many other probabilities can a conditional probability P(x|y) be broken down into?,three
Original Text,4.1,What can we do by dropping the denominator P(d)?,simplify
Original Text,4.1,How can we simplify Eq. 4.3?,dropping the denominator
Original Text,4.1,How can we simplify Eq. 4.3 by dropping the denominator P(d)?,computing Eq. 4.3
Original Text,4.1,What is the denominator for Eq. 4.3?,4.3
Original Text,4.1,How does P(d) change for each class?,P(d) doesn't change for each class
Original Text,4.1,What can we choose to simplify Eq. 4.3 by dropping the denominator P(d)?,the class that maximizes this simpler formula
Original Text,4.1,What is Naive Bayes called?,generative model
Original Text,4.1,What could we imagine generating by following the generative model?,artificial documents
Original Text,4.1,In what chapter will we discuss the intuition of generative models?,Chapter 5
Original Text,4.1,What are the two probabilities that determine the most probable class c given some document d?,the prior probability of the class P(c) and the likelihood of the document P(d|c)
Original Text,4.1,"Without some simplifying assumptions, estimating the probability of every possible combination of features would require what?",huge numbers of parameters and impossibly large training sets
Original Text,4.1,What makes two simplifying assumptions?,Naive Bayes classifiers
Original Text,4.1,What is the first simplifying assumption of naive Bayes classifiers?,the bag of words assumption
Original Text,4.1,What do we assume the features only encode?,encode word identity and not position
Original Text,4.1,What is the naive Bayes assumption?,conditional independence assumption
Original Text,4.1,How do we apply the naive Bayes classifier to text?,by simply walking an index through every word position in the document
Original Text,4.1,Why are naive Bayes calculations done in log space?,increase speed
Original Text,4.1,Where do naive Bayes calculations take place?,log space
Original Text,4.1,What is the name of the naive Bayes calculation that computes the predicted class as a linear function of input features?,4.10
Original Text,4.1,What are classifiers that use a linear combination of inputs to make a classification decision called?,linear classifiers
Original Text,4.2,What probabilities can we learn?,P(c) and P(fi|c)
Original Text,4.2,What are the probabilities P(c) and P(fi|c)?,maximum likelihood estimate
Original Text,4.2,What do we use to estimate the maximum likelihood?,frequencies in the data
Original Text,4.2,What is the probability P(fi|c)?,"P(w, c)"
Original Text,4.2,What is used to give a maximum likelihood estimate of the probability?,the frequency of wi
Original Text,4.2,What is a problem with the maximum likelihood estimate?,maximum likelihood training
Original Text,4.2,"What is the problem with estimating the probability of the word ""fantastic"" given class positive?","suppose there are no training documents that both contain the word ""fantastic"" and are classified as positive"
Original Text,4.2,"How does the word ""fantastic"" happen in a negative class?",sarcastically
Original Text,4.2,"What class does the word ""fantastic"" occur in?",negative
Original Text,4.2,"What is the probability for the word ""fantastic"" in a negative class?",zero
Original Text,4.2,What naively multiplies all the feature likelihoods together?,naive Bayes
Original Text,4.2,What is Laplace smoothing?,add-one
Original Text,4.2,What is usually replaced by more sophisticated smoothing algorithms in language modeling?,Laplace smoothing
Original Text,4.2,What does the vocabulary V consist of?,all the word types in all classes
Original Text,4.2,Why are words not in our vocabulary at all?,they did not occur in any training document in any class
Original Text,4.2,What is the solution for unknown words that are not in our vocabulary at all because they did not occur in any training document in any class?,ignore them
Original Text,4.2,"What are very frequent words like ""the"" and ""a""?",stop words
Original Text,4.2,How can some systems completely ignore stop words?,"by sorting the vocabulary by frequency in the training set, and defining the top 10-100 vocabulary entries as stop words"
Original Text,4.2,What is removed from both training and test documents as if they had never occurred?,stop words
Original Text,4.2,"In most text classification applications, what is more common than using a stop word list?",make use of the entire vocabulary and not use a stop word list
Original Text,4.4,Standard naive Bayes text classification can work well for what?,sentiment analysis
Original Text,4.4,What seems to matter more than its frequency?,whether a word occurs or not
Original Text,4.4,How many words are clipped in each document?,1
Original Text,4.4,What is the variant of naive Bayes called?,binary multinomial naive Bayes
Original Text,4.4,What does binary multinomial naive Bayes use?,Eq. 4.10
Original Text,4.4,What does binary multinomial naive Bayes do for each document?,remove all duplicate words
Original Text,4.4,What shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary?,Fig. 4.3
Original Text,4.4,What shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary?,Fig. 4.3
Original Text,4.4,Why is the example worked without add-1 smoothing?,make the differences clearer
Original Text,4.4,The word great has a count of how many even for Binary NB?,2
Original Text,4.4,What is another important addition commonly made when doing text classification for sentiment?,negation
Original Text,4.4,"What is the meaning of ""I really like this movie""?",positive
Original Text,4.4,"What is the difference between ""I really like this movie"" and ""I didn't like this movie""?",negative
Original Text,4.4,What completely alters the inferences we draw from the predicate like?,negation
Original Text,4.4,Negation can modify what word to produce a positive review?,negative
Original Text,4.4,What prefix is prepended to every word after a token of logical negation?,NOT_
Original Text,4.4,"What phrase becomes ""didn't NOT_like NOT_this NOT_movie, but I""?","""didn't like this movie, but I"""
Original Text,4.4,"Newly formed words like NOT_like, NOT_recommend will occur more often in what type of documents?",negative
Original Text,4.4,When will we return to the use of parsing to deal more accurately with the scope relationship between negation words and the predicates they modify?,Chapter 16
Original Text,4.4,What do naive Bayes classifiers estimate?,positive and negative sentiment
Original Text,4.4,What are lists of words that are pre-annotated with positive or negative sentiment?,sentiment lexicons
Original Text,4.4,Who created the MPQA Subjectivity Lexicon?,"Wilson et al., 2005"
Original Text,4.4,How many words does the MPQA subjectivity lexicon have?,6885
Original Text,4.4,What is a common way to use lexicons in a naive Bayes classifier?,add a feature that is counted whenever a word from that lexicon occurs
Original Text,4.4,What is a feature that is counted whenever a word from a lexicon occurs?,this word occurs in the positive lexicon
Original Text,4.4,What is a feature that is counted whenever a word from a lexicon occurs?,this word occurs in the negative lexicon
Original Text,4.4,How many features won't work in a naive Bayes classifier?,two
Original Text,4.4,"When training data is not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better.",sparse
Original Text,4.4,In what chapter will we discuss the use of lexicons?,Chapter 20
Original Text,4.5,What does not require that our classifier use all the words in the training data as features?,naive Bayes
Original Text,4.5,What can features in naive Bayes express?,any property of the input text
Original Text,4.5,What is the task of deciding if a particular piece of email is an example of spam?,spam detection
Original Text,4.5,What is a common solution to using all the words as features?,predefine likely sets of words or phrases as features
Original Text,4.5,What feature does SpamAssassin predefine?,one hundred percent guaranteed
Original Text,4.5,What is a non-linguistic feature of naive Bayes?,the path that the email took to arrive
Original Text,4.5,What is a naive Bayes feature that pretends everything is a string of raw bytes?,byte n-grams
Original Text,4.5,What can model statistics about the beginning or ending of words?,byte n-grams
Original Text,4.5,What is a widely used naive Bayes system?,langid.py
Original Text,4.5,Language ID systems are trained on what type of text?,multilingual
Original Text,4.5,What are two countries with large Anglophone populations?,Nigeria or India
Original Text,4.6,What can naive Bayes classifiers use?,"dictionaries, URLs, email addresses, network features, phrases"
Original Text,4.6,What does naive Bayes have an important similarity to?,language modeling
Original Text,4.6,A naive Bayes model can be viewed as a set of class-specific what?,unigram language models
Original Text,4.6,What do the likelihood features from the naive Bayes model assign to each word P(word|c)?,probability
Original Text,4.7.0,What type of detection tasks are used to evaluate text classification?,binary
Original Text,4.7.0,What is the definition of a spam category?,positive
Original Text,4.7.0,What is another name for spam detection?,negative
Original Text,4.7.0,What do we need to know for each item?,whether our system called it spam or not
Original Text,4.7.0,What do we need to know?,whether the email is actually spam or not
Original Text,4.7.0,What do we refer to as the gold labels?,human-defined labels
Original Text,4.7.0,What are the human labels for each document that we are trying to match?,gold labels
Original Text,4.7.0,What is the name of the company that you're the CEO of?,Delicious Pie
Original Text,4.7.0,What is the positive class?,tweets about Delicious Pie
Original Text,4.7.0,What do we need in order to know how well our spam detector is doing?,metric
Original Text,4.7.0,What is a table for visualizing how an algorithm performs with respect to the human gold labels?,confusion matrix
Original Text,4.7.0,What is a table for visualizing how an algorithm performs with respect to the human gold labels?,confusion matrix
Original Text,4.7.0,What is a table for visualizing how an algorithm performs with respect to the human gold labels?,confusion matrix
Original Text,4.7.0,What are documents that are indeed spam that our system correctly said were spam?,true positives
Original Text,4.7.0,What are documents that are indeed spam but our system incorrectly labeled as non-spam?,False negatives
Original Text,4.7.0,To the bottom right of the table is the equation for what percentage of all the observations (for the spam or pie examples that means all emails or tweets,accuracy
Original Text,4.7.0,What do we generally don't use accuracy for?,text classification tasks
Original Text,4.7.0,What is a large majority of email?,spam
Original Text,4.7.0,How many tweets are discussing their love or hatred for our pie?,100
Original Text,4.7.0,What would a simple classifier classify every tweet as?,not about pie
Original Text,4.7.0,What is the accuracy of a simple classifier?,99.99%
Original Text,4.7.0,What is the level of accuracy in a classifier?,accuracy
Original Text,4.7.0,How many false negatives would a classifier have?,"999,900"
Original Text,4.7.0,What would a 'no pie' classifier be useless for?,it wouldn't find a single one of the customer comments
Original Text,4.7.0,What is a common situation in the world where accuracy is not a good metric?,rare
Original Text,4.7.0,What are the two other metrics shown in Fig. 4.4?,precision and recall
Original Text,4.7.0,What are the two other metrics shown in Fig. 4.4?,precision and recall
Original Text,4.7.0,"What measures the percentage of items that the system detected (i.e., the system labeled as positive) that are in fact positive?",Precision
Original Text,4.7.0,What measures the percentage of items actually present in input that were correctly identified by the system?,Recall
Original Text,4.7.0,What classifier has a terrible recall of 0?,nothing is pie
Original Text,4.7.0,"What is the accuracy of the ""nothing is pie"" classifier?",99.99%
Original Text,4.7.0,"What is a problem with the ""nothing is pie"" classifier?",relevant tweets
Original Text,4.7.0,What do precision and recall emphasize?,true positives
Original Text,4.7.0,There are many ways to define a single metric that incorporates aspects of what?,precision and recall
Original Text,4.7.0,What is the simplest way to define a single metric that incorporates aspects of precision and recall?,F-measure
Original Text,4.7.0,What parameter differentially weights the importance of recall and precision?,The B parameter
Original Text,4.7.0,What value favors recall?,B>1
Original Text,4.7.0,What is the most commonly used metric when precision and recall are equally balanced?,FB=1 or just F1
Original Text,4.7.0,What does the F-measure come from?,a weighted harmonic mean of precision and recall
Original Text,4.7.0,What is the reciprocal of the arithmetic mean of reciprocals?,The harmonic mean of a set of numbers
Original Text,4.7.0,Why is the harmonic mean used?,because it is a conservative metric
Original Text,4.7.0,What does the harmonic mean weigh more heavily than the arithmetic mean?,lower of the two numbers
Original Text,4.7.1,"Up to now, we have been describing text classification tasks with how many classes?",two
Original Text,4.7.1,What do lots of classification tasks in language processing have?,more than two classes
Original Text,4.7.1,What are the three classes for sentiment analysis?,"positive, negative, neutral"
Original Text,4.7.1,What is already a multi-class classification algorithm?,naive Bayes algorithm
Original Text,4.7.1,What definitions will we need to modify to make the naive Bayes algorithm a multi-class classification algorithm?,precision and recall
Original Text,4.7.1,What is the example confusion matrix for in Fig. 4.5?,"hypothetical 3-way ""one-of"" email categorization decision"
Original Text,4.7.1,"What are some examples of ""one-of"" email categorization decisions?","urgent, normal, spam"
Original Text,4.7.1,What did the system mistakenly label as urgent?,one spam document
Original Text,4.7.1,How can we derive a single metric that tells us how well the system is doing?,combine these values in two ways
Original Text,4.7.1,"What computes the performance for each class, and then averages over classes?",macroaveraging
Original Text,4.7.1,In what method do we collect the decisions for all classes into a single confusion matrix?,microaveraging
Original Text,4.7.1,What figure shows the confusion matrix for each class separately?,4.6
Original Text,4.7.1,What figure shows the confusion matrix for each class separately?,4.6
Original Text,4.7.1,What class dominates a microaverage?,more frequent class
Original Text,4.7.1,What better reflects the statistics of the smaller classes?,macroaverage
Original Text,4.8,What is another name for the development test set?,devset
Original Text,4.8,What is the purpose of the test set?,to report its performance
Original Text,4.8,What does the use of a devset avoid?,overfitting
Original Text,4.8,Wouldn't it be better if we could use all our data for training and still use all our data for test?,use all our data for training and still use all our data for test
Original Text,4.8,How can we use all our data for training and still use all our data for test?,by cross-validation
Original Text,4.8,What does cross-validation repeat with?,a different randomly selected training set and test set
Original Text,4.8,How many times do we do cross-validation?,10
Original Text,4.8,What is cross-validation called?,10-fold cross-validation
Original Text,4.8,What is the problem with cross-validation?,blind
Original Text,4.8,What is important in designing NLP systems?,looking at the corpus
Original Text,4.8,What is it common to do in training and testing?,"create a fixed training set and test set, then do 10-fold cross-validation inside the training set"
Original Text,4.8,What type of cross-validation is done inside a fixed training set and test set?,10-fold cross-validation
Original Text,4.9.0,"In building systems, we often need to compare the performance of how many systems?",two
Original Text,4.9.0,How can we know if the new system we just built is better than the old one?,new system we just built is better than our old one
Original Text,4.9.0,How can we know if the new system we just built is better or worse than the other system described in the literature?,better
Original Text,4.9.0,What is the domain of comparing the performance of two systems?,statistical hypothesis testing
Original Text,4.9.0,Who published a study on statistical significance for NLP classifiers in 2012?,Berg-Kirkpatrick et al.
Original Text,4.9.0,When was Berg-Kirkpatrick et al. published?,2012
Original Text,4.9.0,What do we compare the performance of classifiers A and B on?,metric M
Original Text,4.9.0,What is the F1 score of our logistic regression sentiment classifier A?,higher
Original Text,4.9.0,What is the score that system A gets on test set x?,"M(A,x)"
Original Text,4.9.0,What means that our logistic regression classifier has a higher F1 than our naive Bayes classifier on X?,"d(x) > 0,"
Original Text,4.9.0,What is the effect size?,d(x)
Original Text,4.9.0,What is the F1 score of A higher than that of Bs?,.04
Original Text,4.9.0,Can we be certain that A is better than B?,We cannot
Original Text,4.9.0,Can we be certain that A is better than B?,We cannot
Original Text,4.9.0,Why do we want to know if A's superiority over B is likely to hold again if we checked another test set x'?,A might just be accidentally better than B
Original Text,4.9.0,How do we test the null hypothesis?,by formalizing two hypotheses
Original Text,4.9.0,What is the hypothesis H0 called?,the null hypothesis
Original Text,4.9.0,What hypothesis does the null hypothesis support?,H1
Original Text,4.9.0,How do we support the null hypothesis?,creating a random variable X ranging over all test sets
Original Text,4.9.0,What is the probability that we would see if the null hypothesis is true?,d(x)
Original Text,4.9.0,What is the probability that we would see d(x) assuming the null hypothesis is true?,p-value
Original Text,4.9.0,What is the probability that we would see d(x)?,assuming A is not better than B
Original Text,4.9.0,What is A's F1?,.9
Original Text,4.9.0,What might be less surprising to us if the null hypothesis is true and A is not really better than B?,if d(x) is very small
Original Text,4.9.0,A very small p-value means that the difference we observed is what under the null hypothesis?,very unlikely
Original Text,4.9.0,What counts as a p-value if the null hypothesis is true?,very small
Original Text,4.9.0,What value means that if the p-value (the probability of observing the d we saw assuming H0 is true) is less,.01
Original Text,4.9.0,"If a result is statistically significant, what is the null hypothesis?",if the d we saw has a probability that is below the threshold
Original Text,4.9.0,What is the probability of observing the d we saw assuming H0 is true?,p-value
Original Text,4.9.0,What are two examples of simple parametric tests in NLP?,t-tests or ANOVAs
Original Text,4.9.0,Parametric tests make assumptions about the distributions of the test statistic that don't generally hold in our cases.,normality
Original Text,4.9.0,What are non-parametric tests based on?,sampling
Original Text,4.9.0,"If we had lots of different test sets, what would we measure all the d(x') for all the x'?",x'
Original Text,4.9.0,What do we get when we measure all the d(x') for all the x'?,a distribution
Original Text,4.9.0,What percentage of deltas are smaller than the delta we observed?,99% or more
Original Text,4.9.0,What is the probability of seeing a d(x) as big as the one we saw?,p-value(x)
Original Text,4.9.0,What is the most common non-parametric test used in NLP?,approximate randomization
Original Text,4.9.0,What is the most common non-parametric test used in NLP?,bootstrap test
Original Text,4.9.0,What is the most common non-parametric test used in NLP?,bootstrap
Original Text,4.9.0,What are those in which we compare two sets of observations that are aligned?,Paired tests
Original Text,4.9.0,When do Paired tests happen?,when we are comparing the performance of two systems on the same test set
Original Text,4.9.1,What can apply to any metric?,bootstrap test
Original Text,4.9.1,What is the term for large numbers of smaller samples with replacement from an original larger sample?,bootstrap samples
Original Text,4.9.1,How can we create many virtual test sets from an observed test set?,by repeatedly sampling from it
Original Text,4.9.1,The bootstrap test only makes the assumption that the sample is what?,representative of the population
Original Text,4.9.1,What is the test set for a tiny text classification example?,x of 10 documents
Original Text,4.9.1,What row of Fig. 4.8 shows the results of two classifiers?,first row
Original Text,4.9.1,The first row of Fig. 1 shows the results of two classifiers (A and B) on a test set x of 10 documents,4.8
Original Text,4.9.1,Which two classifiers get the correct class on the first document?,A and B
Original Text,4.9.1,"If we assume for simplicity that our metric is accuracy, what is d(z)?",.20
Original Text,4.9.1,What is a large num of virtual test sets x(i)?,b
Original Text,4.9.1,What shows a couple examples of virtual test sets?,Fig. 4.8
Original Text,4.9.1,What figure shows a couple examples of virtual test sets?,4.8
Original Text,4.9.1,How many times do we repeatedly select a cell from row x with replacement?,n=10 times
Original Text,4.9.1,What would we create if we randomly selected the second cell of the x row?,first cell of the first virtual test set x(1)
Original Text,4.9.1,What can we do statistics on now that we have the b test sets?,how often A has an accidental advantage
Original Text,4.9.1,Who published a version of statistics on how often A has an accidental advantage?,Berg-Kirkpatrick et al.
Original Text,4.9.1,In what year did Berg-Kirkpatrick et al. publish a version of how often A has an accidental advantage?,(2012)
Original Text,4.9.1,What does H0 assume?,A isn't better than B
Original Text,4.9.1,What would we compute to measure exactly how surprising is our observed d(x)?,p-value
Original Text,4.9.1,Why is the expected value of d(X) over many test sets not true?,assuming A isn't better than B
Original Text,4.9.1,What is the bias of the original test set x in favor of A?,.20
Original Text,4.9.1,What is computed by counting over many test sets how often d(x(i)) exceeds the expected value of d(x),p-value
Original Text,4.9.1,"What is the threshold of 10,000 test sets x(i)?",.01
Original Text,4.9.1,Where is the full algorithm for the bootstrap shown?,Fig. 4.9
Original Text,4.9.1,The full algorithm for the bootstrap is shown in Fig. what?,4.9
Original Text,4.9.1,What is the num of samples b?,b
Original Text,4.9.1,What does the percentage of b bootstrap test sets in which d(x*(i)) > 2d(x) act as?,one-sided empirical p-value
Original Text,4.10,What is important to avoid that may result from classifiers?,harms
Original Text,4.10,What class of harms are caused by a system that demeans a social group?,representational harms
Original Text,4.10,What are representational harms caused by a system that perpetuates negative stereotypes about a social group?,demeans
Original Text,4.10,Who examined the performance of 200 sentiment analysis systems on pairs of sentences that were identical except for a common African American first name?,Kiritchenko and Mohammad
Original Text,4.10,Where did Caliskan and Mohammad examine the performance of 200 sentiment analysis systems on pairs of sentences that were identical except for a common African American,Chapter 6
Original Text,4.10,Who found that most systems assigned lower sentiment and more negative emotion to sentences with African American names?,"Popp et a1., 2003"
Original Text,4.10,"Classifiers can lead to representational harms and other harms, such as what?",censorship
Original Text,4.10,"What is the important text classification task of detecting hate speech, abuse, harassment, or other kinds of toxic language?",toxicity detection
Original Text,4.10,What classifiers can cause harms?,toxicity classifiers
Original Text,4.10,Who published a study in which toxicity classifiers incorrectly flag sentences that are non-toxic but simply mention minority identities?,Davidson et al.
Original Text,4.10,In what year did Davidson and his colleagues publish their findings about toxicity classifiers?,2019
Original Text,4.10,What could lead to the censoring of discourse by or about minority identities?,false positive errors
Original Text,4.10,What can cause model problems?,biases or other problems in the training data
Original Text,4.10,What can cause model problems?,the labels
Original Text,4.10,What is an important area of research?,mitigation
Original Text,4.10,What is important when introducing a NLP model?,study these kinds of factors and make them clear
Original Text,4.10,"What document documents a machine learning model with information like: training algorithms and parameters, motivation, and preprocessing, evaluation data sources, motivation, and",model card
